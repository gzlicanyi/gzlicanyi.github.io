<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[看完上手——awk]]></title>
    <url>%2F2020%2F01%2F01%2F%E7%9C%8B%E5%AE%8C%E4%B8%8A%E6%89%8B%E2%80%94%E2%80%94awk%2F</url>
    <content type="text"><![CDATA[测试文件：test.txt 内如如下： 12341,2,3,4,56,7,8,9,1011,12,13,14,1516,17,18,19,20 输入方式12345# 通过管道cat test.txt | awk '&#123;print $0&#125;'# 文件放最后awk '&#123;print $0&#125;' test.txt 分割字段12345678# $0表示整条记录，$1表示分割后的第一个字段awk -F',' '&#123;print $1&#125;' test.txt# 输出161116 过滤记录 正则过滤 /regular/ 12345awk '/9/&#123;print $0&#125;' test.txt# 输出6,7,8,9,1016,17,18,19,20 条件过滤 1234awk -F, '&#123;if($1=="11") print $0&#125;' test.txt# 输出11,12,13,14,15 常用变量 FS 字段分隔符 Field separator NF 这条记录的总字段数 NR 第几条记录 12345678# 输出的字符串，通过双引号引住awk -F, '&#123;print "这个是第"NR"行，这条记录共有"NF"个字段"&#125;' test.txt# 输出这个是第1行，这条记录共有5个字段这个是第2行，这条记录共有5个字段这个是第3行，这条记录共有5个字段这个是第4行，这条记录共有5个字段 循环1234567891011121314151617181920212223awk 'BEGIN &#123;FS=","&#125; &#123;for(i=1; i&lt;=NF;i++)&#123;print $i&#125;&#125;' test.txt# 输出1234567891011121314151617181920 计算平均数1234awk -F, '&#123;sum+=$1&#125; END &#123;print "Average = ", sum/NR&#125;' test.txt# 输出Average = 8.5 去重test文件增加几条记录 12345678910cat test.txt# 输出1,2,3,4,56,7,8,9,101,2,3,4,511,12,13,14,1516,17,18,19,2011,12,13,14,1511,12,13,14,15 去重输出 12345678# 如果数组a不存在这一行，则打印awk '&#123; if (!a[$0]++) &#123; print $0; &#125; &#125;' test.txt#输出1,2,3,4,56,7,8,9,1011,12,13,14,1516,17,18,19,20 去重，并输出对应记录的重复数 1234567awk '&#123;a[$0]++&#125;END&#123;for(w in a)&#123;print w "出现次数为" a[w]&#125;&#125;' test.txt# 输出6,7,8,9,10出现次数为116,17,18,19,20出现次数为111,12,13,14,15出现次数为31,2,3,4,5出现次数为2]]></content>
      <categories>
        <category>看完上手</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>shell</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java api系列之HashMap]]></title>
    <url>%2F2019%2F12%2F29%2Fjava-api%E7%B3%BB%E5%88%97%E4%B9%8BHashMap%2F</url>
    <content type="text"><![CDATA[java api系列目录java api系列 简述首先看一下官方文档是怎么介绍HashMap的： Hash table based implementation of the Map interface. This implementation provides all of the optional map operations, and permits null values and the null key. (The HashMap class is roughly equivalent to Hashtable, except that it is unsynchronized and permits nulls.) This class makes no guarantees as to the order of the map; in particular, it does not guarantee that the order will remain constant over time. 简单来说，就是： 基于Map接口来实现，支持所有的map操作 Key和Value都支持null值 不保证有序，无论是插入顺序，还是Key的排序等 不保证固定顺序，随着容量的增大或减少，顺序会发生变化 举个例子就很清楚了： 12345678Map map = new HashMap();map.put("java", 0);map.put("python", 1);map.put("go", 2);map.put("php", 3);map.put("c++", 4);map.put(null, 5);map.put("noValue", null); 通过IDEA断点查看里面的存储结构是这样的，可以看到插入顺序是打乱了的： 影响映射分布的两个因子第一个是hash表的capacity（容量）： The capacity is the number of buckets in the hash table, and the initial capacity is simply the capacity at the time the hash table is created. 第二个是load factor（负载因子）： The load factor is a measure of how full the hash table is allowed to get before its capacity is automatically increased. 当hash表存储的数据量超过capacity 乘以 load factor时，hash表会扩容，容量会翻倍，部分数据会重新映射到扩容后的地方。所以初始容量和负载因子是影响HashMap性能及分布的两个重要因素。 1static final float DEFAULT_LOAD_FACTOR = 0.75f; HashMap默认的负载因子是0.75，这个是比较折中的数字，平衡空间和时间的开销。负载因子如果太大，会造成hash冲突严重，影响查询性能；如果太小，会容易触及扩容，浪费空间。 有效预估HashMap存储的实际数据量，来设定初始容量，可以有效避免经常扩容导致的性能开销。 put方法详解123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263public V put(K key, V value) &#123; return putVal(hash(key), key, value, false, true);&#125;final V putVal(int hash, K key, V value, boolean onlyIfAbsent, boolean evict) &#123; Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; p; int n, i; //table不存在，则创建table if ((tab = table) == null || (n = tab.length) == 0) n = (tab = resize()).length; //通过hash计算偏移量位置，如果这个位置没有节点，则自己新建一个节点 if ((p = tab[i = (n - 1) &amp; hash]) == null) tab[i] = newNode(hash, key, value, null); else &#123; //e为需要覆盖的节点 Node&lt;K,V&gt; e; K k; //如果已经存在的节点的hash值和值相等（分别对应的是类的hashCode方法和equals方法） //则认为是覆盖操作，先记录为e，后面会将新的value覆盖旧的value if (p.hash == hash &amp;&amp; ((k = p.key) == key || (key != null &amp;&amp; key.equals(k)))) e = p; //如果节点是树，则以树的方式添加 else if (p instanceof TreeNode) e = ((TreeNode&lt;K,V&gt;)p).putTreeVal(this, tab, hash, key, value); else &#123; //节点是链表 //通过遍历链表的方式，如果整个链表所有节点没有和新的节点重复，则将新的节点放置尾端 //如果因为放多了一个新的节点，导致链表过长，则将链表转换成红黑树 //如果其中有一个节点和新的节点重复，则记录并跳出循环 for (int binCount = 0; ; ++binCount) &#123; //将新的节点放置链表尾端，此时e为null if ((e = p.next) == null) &#123; p.next = newNode(hash, key, value, null); //链表过长，为了提高查询性能，将链表转换成树 if (binCount &gt;= TREEIFY_THRESHOLD - 1) // -1 for 1st treeifyBin(tab, hash); break; &#125; //链表中，其中一个节点的hash值和值相等 if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) break; p = e; &#125; &#125; //如果e不为null，说明是新值覆盖旧值 if (e != null) &#123; // existing mapping for key V oldValue = e.value; //判断写入条件，并写入 if (!onlyIfAbsent || oldValue == null) e.value = value; afterNodeAccess(e); //直接返回，没有改变table的大小 return oldValue; &#125; &#125; ++modCount; //改变了table的大小，判断是否触发扩容 if (++size &gt; threshold) resize(); afterNodeInsertion(evict); return null;&#125; 总结起来就是： 通过hash计算偏移量获得节点位置 如果位置没有旧节点，则新建节点存放 如果旧节点的key等于新节点的key，则用新的value覆盖旧的value 如果旧节点是树节点，则以树的形式添加新节点 如果是一个链表，则以链表的信息添加新的节点，重复则覆盖，不然的放置尾端；链表过长，则转换成树 如果table大小发生改变，并且触发扩容，则扩容 hash函数详解看完put方法，大致对HashMap整个结构有了个大概的了解，那么其中一个比较重要的步骤，通过hash值计算table偏移量，这个决定了新旧值碰撞概率，以及扩容时，节点移动时的效率。 下面是API实现的hash方法： 1234567891011121314151617181920212223/** * Computes key.hashCode() and spreads (XORs) higher bits of hash * to lower. Because the table uses power-of-two masking, sets of * hashes that vary only in bits above the current mask will * always collide. (Among known examples are sets of Float keys * holding consecutive whole numbers in small tables.) So we * apply a transform that spreads the impact of higher bits * downward. There is a tradeoff between speed, utility, and * quality of bit-spreading. Because many common sets of hashes * are already reasonably distributed (so don't benefit from * spreading), and because we use trees to handle large sets of * collisions in bins, we just XOR some shifted bits in the * cheapest possible way to reduce systematic lossage, as well as * to incorporate impact of the highest bits that would otherwise * never be used in index calculations because of table bounds. */static final int hash(Object key) &#123; int h; //兼容key为null的情况 //hashCode是integer类型，共32位 //保留高16位，优化低16位，在低16位中，将高16位与低16位做异或操作 return (key == null) ? 0 : (h = key.hashCode()) ^ (h &gt;&gt;&gt; 16);&#125; 为啥要将高16位和低16位做异或操作，其实再put方法里面，我们看到计算偏移量的计算是这样的：(n - 1) &amp; hash，n的大小一般不会很大，超过16位(65535)；所以高低位异或的目的，是让n小时，高16位也能参与hash计算，增加散列的随机性。 get方法详解有了put方法里面讲解的节点存储结构，及hash的计算，那么就很容易理解get方法了 1234567891011121314151617181920212223242526272829public V get(Object key) &#123; Node&lt;K,V&gt; e; //兼容value是null的情况 return (e = getNode(hash(key), key)) == null ? null : e.value;&#125;final Node&lt;K,V&gt; getNode(int hash, Object key) &#123; Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; first, e; int n; K k; if ((tab = table) != null &amp;&amp; (n = tab.length) &gt; 0 &amp;&amp; //计算偏移量 (first = tab[(n - 1) &amp; hash]) != null) &#123; //检查第一个节点是否满足 if (first.hash == hash &amp;&amp; // always check first node ((k = first.key) == key || (key != null &amp;&amp; key.equals(k)))) return first; if ((e = first.next) != null) &#123; //以树节点的方式获取 if (first instanceof TreeNode) return ((TreeNode&lt;K,V&gt;)first).getTreeNode(hash, key); //以链表的方式，遍历获取 do &#123; if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) return e; &#125; while ((e = e.next) != null); &#125; &#125; return null;&#125; resize方法详解有了前面的铺垫，那么怎么设计扩容，才能使节点移动最少呢？ 官方是这么说的： 12345Initializes or doubles table size. If null, allocates inaccord with initial capacity target held in field threshold.Otherwise, because we are using power-of-two expansion, theelements from each bin must either stay at same index, or movewith a power of two offset in the new table. 在扩容时，将table的大小乘以2，为什么是乘以2而不是3？ 在计算偏移量：(n - 1) &amp; hash时，n是比较关键的参数，如果n一开始是16，那么n-1的二进制是：1111，那么偏移量计算就是与hash的低4位&amp;操作；如果n = n * 2 为32，n-1的二进制是：11111，那么偏移量计算就是与hash的低5位&amp;操作。 这样的话，原来的节点将发生什么变化呢？ 如果原来存在这样两个节点，hash值分别是A：101011和B：111011，在n=16时，这两个节点是发生碰撞的(以链表或树的形式共存)，偏移量都是1011；扩容后，A的偏移量是01011即1011保持不变，B的偏移量是11011发生节点转移。 看到这里，就明白了这样的扩容设计，保持了一半左右的节点不需要移动，而大部分碰撞节点，将迁移到新的位置，使存储和查询更加高效。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687final Node&lt;K,V&gt;[] resize() &#123; Node&lt;K,V&gt;[] oldTab = table; int oldCap = (oldTab == null) ? 0 : oldTab.length; int oldThr = threshold; int newCap, newThr = 0; if (oldCap &gt; 0) &#123; //table的大小达到最大值，就不在扩容了 if (oldCap &gt;= MAXIMUM_CAPACITY) &#123; threshold = Integer.MAX_VALUE; return oldTab; &#125; //没达到最大值，则大小翻倍，触发阈值也翻倍 else if ((newCap = oldCap &lt;&lt; 1) &lt; MAXIMUM_CAPACITY &amp;&amp; oldCap &gt;= DEFAULT_INITIAL_CAPACITY) newThr = oldThr &lt;&lt; 1; // double threshold &#125; else if (oldThr &gt; 0) // initial capacity was placed in threshold newCap = oldThr; else &#123; // zero initial threshold signifies using defaults //table大小和触发阈值都小于等于0，则说明此时做的是初始化操作 newCap = DEFAULT_INITIAL_CAPACITY; newThr = (int)(DEFAULT_LOAD_FACTOR * DEFAULT_INITIAL_CAPACITY); &#125; //如果新的触发阈值等于0，则重新计算 if (newThr == 0) &#123; float ft = (float)newCap * loadFactor; newThr = (newCap &lt; MAXIMUM_CAPACITY &amp;&amp; ft &lt; (float)MAXIMUM_CAPACITY ? (int)ft : Integer.MAX_VALUE); &#125; threshold = newThr; @SuppressWarnings(&#123;"rawtypes","unchecked"&#125;) Node&lt;K,V&gt;[] newTab = (Node&lt;K,V&gt;[])new Node[newCap]; table = newTab; if (oldTab != null) &#123; //遍历table for (int j = 0; j &lt; oldCap; ++j) &#123; Node&lt;K,V&gt; e; if ((e = oldTab[j]) != null) &#123; oldTab[j] = null; //如果这个是单节点，则直接转移到新节点 if (e.next == null) newTab[e.hash &amp; (newCap - 1)] = e; //如果这个节点是树节点，按树节点的方式处理 else if (e instanceof TreeNode) ((TreeNode&lt;K,V&gt;)e).split(this, newTab, j, oldCap); else &#123; // preserve order Node&lt;K,V&gt; loHead = null, loTail = null; Node&lt;K,V&gt; hiHead = null, hiTail = null; Node&lt;K,V&gt; next; //遍历链表 do &#123; next = e.next; //节点位置不变，举上面的例子： //A的hash为101011，oldcup为16(10000) //那么结果就是0，所以A节点归到loHead if ((e.hash &amp; oldCap) == 0) &#123; if (loTail == null) loHead = e; else loTail.next = e; loTail = e; &#125; //反过来，就是节点位置改变了的，归到hiHead else &#123; if (hiTail == null) hiHead = e; else hiTail.next = e; hiTail = e; &#125; &#125; while ((e = next) != null); //将重构后的链表loHead覆盖到原来的位置 if (loTail != null) &#123; loTail.next = null; newTab[j] = loHead; &#125; //移动后的新链表hiHead放置到新的位置 if (hiTail != null) &#123; hiTail.next = null; newTab[j + oldCap] = hiHead; &#125; &#125; &#125; &#125; &#125; return newTab;&#125; 整篇下来，基本就可以了解到HashMap的实现原理了，介于篇幅有限，文章中埋了个坑，树节点没怎么讲，想要了解更多，就持续关注 java api系列 ，并点赞留言告知。]]></content>
      <categories>
        <category>java api系列</category>
      </categories>
      <tags>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java 16进制转换10进制]]></title>
    <url>%2F2019%2F12%2F01%2Fjava-16%E8%BF%9B%E5%88%B6%E8%BD%AC%E6%8D%A210%E8%BF%9B%E5%88%B6%2F</url>
    <content type="text"><![CDATA[通常，在java里面，将16进制字符转换成10进制数字，可以这么写： 1234String s = "b81f7";long r = Long.parseLong(s, 16);System.out.println(r);//输出 754167 但是s如果是比较长的数，如：0000000000000bae09a7a393a8acded75aa67e46cb81f7acaa5ad94f9eacd103那么就会报如下错误： 123Exception in thread "main" java.lang.NumberFormatException: For input string: "0000000000000bae09a7a393a8acded75aa67e46cb81f7acaa5ad94f9eacd103" at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65) at java.lang.Long.parseLong(Long.java:592) 原因是转换后的10进制数，超出了Long的最大值。 所以我们的正确做法应该是使用BigInteger: 1234String s = "0000000000000bae09a7a393a8acded75aa67e46cb81f7acaa5ad94f9eacd103";BigInteger r = new BigInteger(s, 16);System.out.println(r);//输出 18768770924787611988401014025437350240749174288167713091932419]]></content>
      <categories>
        <category>问题总结</category>
      </categories>
      <tags>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mysql字符串与数字比较问题]]></title>
    <url>%2F2019%2F09%2F08%2Fmysql%E5%AD%97%E7%AC%A6%E4%B8%B2%E4%B8%8E%E6%95%B0%E5%AD%97%E6%AF%94%E8%BE%83%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[由于某些原因，项目中使用到雪花id（固定18位的数字）作为mysql表的PRIMARY KEY，且id类型为varchar。 使用id做偏移量分页时，踩到了一些类型的坑，如下： 123456mysql&gt; select '465543153571602432' &lt;= 465543153571602431;+-------------------------------------------+| '465543153571602432' = 465543153571602431 |+-------------------------------------------+| 1 |+-------------------------------------------+ 如果程序中传入的id是long类型的话，这样是相同的，原因是mysql会将字符串自动转换为数字类型，但是精度丢失导致了’465543153571602432’ 等于 465543153571602431。 正确的应该是传入字符串类型： 123456mysql&gt; select '465543153571602432' &lt;= '465543153571602431';+----------------------------------------------+| '465543153571602432' &lt;= '465543153571602431' |+----------------------------------------------+| 0 |+----------------------------------------------+ 如果传入的参数都是雪花id，那么上面的方案应该就可以解决问题了，但是如果传入的是5，就会存在下面的问题： 123456mysql&gt; select '465543153571602432' &lt;= '5';+-----------------------------+| '465543153571602432' &lt;= '5' |+-----------------------------+| 1 |+-----------------------------+ mysql字符串比较时，不管你的字符串长度的，按字符顺序比较下去，第一个相同，就比较第二个等，所以这就不符合我的需求了。 所以需要将字符类型转换为数字类型来比较： 123456mysql&gt; select CONVERT('465543153571602432',SIGNED) &lt;= 5;+-------------------------------------------+| CONVERT('465543153571602432',SIGNED) &lt;= 5 |+-------------------------------------------+| 0 |+-------------------------------------------+ 那么是不是第二种方案也可以满足第一种方案的情况： 123456mysql&gt; select CONVERT('465543153571602432',SIGNED) &lt;= 465543153571602431;+------------------------------------------------------------+| CONVERT('465543153571602432',SIGNED) &lt;= 465543153571602431 |+------------------------------------------------------------+| 0 |+------------------------------------------------------------+ 答案是可以满足，但是性能不允许，因为第二种方案走不了索引。这个id的字段是varchar，建立的是字符串索引，转化为数字比较，是走不了字符串索引的。 所以需要做一下折中，如果是雪花id，则走第一种方案，否则走第二种方案： 12345678910111213/** * @param offset * @return left是key，right是value */ private ImmutablePair&lt;String, Object&gt; fixIdSql(String offset) &#123; //判断是否雪花id if (StringUtils.length(offset) == 18) &#123; return ImmutablePair.of("id", offset); &#125; else &#123; return ImmutablePair.of("CONVERT(id,SIGNED)", Long.parseLong(offset)); &#125; &#125; 这样就满足大部分雪花id查询的性能，也能处理一些偶尔非雪花id的需求。]]></content>
      <categories>
        <category>笔记</category>
      </categories>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[webflux的delay原理详解]]></title>
    <url>%2F2019%2F07%2F28%2Fwebflux%E7%9A%84delay%E5%8E%9F%E7%90%86%E8%AF%A6%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[反应式编程一开始是从前端和客户端开始兴起，现在大有蔓延到后端的趋势，Spring5推出的webflux就是反应式编程的产物。 webflux对比于springMVC，性能高出很多，网上已经有很多的测评，不再在过多说明。 上图看出对比于同步，异步所用的线程是比较少的，不过有个前提是，程序逻辑中有阻塞（如io阻塞等），且这种阻塞是可以异步化的。 为了满足这个前提，反应式编程框架就必须将这些阻塞变成异步化，如新出的WebClient工具就是将http请求io异步化。 delay方法就是用来代替sleep方法的，下面来讲解一下delay方法是怎么将延时异步化的。 源码解读 通过查看Mono&lt;Long&gt; delay(Duration duration)方法源码，它会构造一个MonoDelay类，并通过传入全局公用的调度器Schedulers.parallel()来调度里面的异步任务。 1234567public static Mono&lt;Long&gt; delay(Duration duration) &#123; return delay(duration, Schedulers.parallel());&#125;public static Mono&lt;Long&gt; delay(Duration duration, Scheduler timer) &#123; return onAssembly(new MonoDelay(duration.toMillis(), TimeUnit.MILLISECONDS, timer));&#125; 查看MonoDelay类的订阅方法subscribe： 1234567891011121314151617public void subscribe(CoreSubscriber&lt;? super Long&gt; actual) &#123; MonoDelayRunnable r = new MonoDelayRunnable(actual); actual.onSubscribe(r); try &#123; //重点在于下面的 timedScheduler.schedule(r, delay, unit) //通过timedScheduler来调度延时任务，而不是当前线程阻塞等待 r.setCancel(timedScheduler.schedule(r, delay, unit)); &#125; catch (RejectedExecutionException ree) &#123; if(r.cancel != OperatorDisposables.DISPOSED) &#123; actual.onError(Operators.onRejectedExecution(ree, r, null, null, actual.currentContext())); &#125; &#125;&#125; 查看ParallelScheduler的delay方法： 1234public Disposable schedule(Runnable task, long delay, TimeUnit unit) &#123; //pick方法会获取一个ScheduledExecutorService线程执行器给到Schedulers使用 return Schedulers.directSchedule(pick(), task, delay, unit);&#125; 查看directSchedule方法： 123456789101112131415161718192021static Disposable directSchedule(ScheduledExecutorService exec, Runnable task, long delay, TimeUnit unit) &#123; //包装任务 SchedulerTask sr = new SchedulerTask(task); Future&lt;?&gt; f; if (delay &lt;= 0L) &#123; f = exec.submit((Callable&lt;?&gt;) sr); &#125; else &#123; //延时调度 //ScheduledExecutorService是java自带的并发调度接口， //通过一条线程轮询延时队列来避免所有线程阻塞 f = exec.schedule((Callable&lt;?&gt;) sr, delay, unit); &#125; //设置结果 sr.setFuture(f); return sr;&#125; 自此就可以知道为什么delay方法没有阻塞线程，因为它的延时处理都交给了ScheduledExecutorService执行器处理，调用delay方法的主线程就直接返回了，等到延时时间过后，ScheduledExecutorService就会从线程池就获取一个线程来处理延时后的任务逻辑。整个流程就类似于上面图片中的右图。 通过反应式编程范式，将所有阻塞都修改为类似于delay之于sleep的形式，就能大幅度提升服务性能了。]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Mac效率神器Alfred]]></title>
    <url>%2F2019%2F06%2F09%2FMac%E6%95%88%E7%8E%87%E7%A5%9E%E5%99%A8Alfred%2F</url>
    <content type="text"><![CDATA[曾经，为了转化时间戳，我写了个Python脚本来提高效率，后来感觉不好编辑，又写了时间戳在线转化工具，感觉还行，后续就写了一系列在线工具来提高工作效率。 翻译就使用Chrome的有道或谷歌翻译插件，谷歌身份验证器使用的也是Chrome插件，所以工作严重依赖Chrome，屏幕经常在idea和Chrome之间来回切换。 曾经以为这就是理想的工作环境了，直到遇到了Alfred。 Alfred是和spotlight同类型但更强大的效率软件。以前会使用spotlight来切换APP、搜索文件等，也仅限于此，而且搜索的结果其实也比较杂乱吧，所以使用场景比较有限。Alfred则强大很多，可以通过自带或自定义的前缀命令，一步到位做很多事情，比较简单的如：open down打开下载文件夹。 首先说明Alfred是半免费产品，免费的功能是自带的Features，收费的是Workflows，Features的功能已经比spotlight的要强大了，而Workflows则提供更强大的自定义功能。所以轻度使用的话，免费的就足够了，如果要使用Workflows，推荐去官网支持正版，囊中羞涩但想试用的请自行搜索破解版。 简单使用常规设置唤起快捷键用惯了spotlight的快捷键唤起，要使用Alfred替换它，需要做的是： 去掉spotlight的唤起快捷键： 在Alfred设置中设置快捷键： 去掉shift键预览对于使用shift键来做中英切换的人来说，Alfred的shift键预览简直恶心，不多说，在这里去掉： 常规搜索默认如果Alfred不清楚你要搜索什么时，它会给你谷歌、亚马逊和维基百科三个搜索，对于我个人来说，谷歌搜索就够了，用惯百度的也可以自己设置： 如果我搜索hello world，确认就会跳去浏览器的谷歌搜索： 也会记录常去的站点，以便一搜即去： 文件搜索新建一个名为alfred_open_test.txt且内容为alfred_in_test的文件。 搜索文件名 搜索文件内容 App搜索打开 常用功能计算器平常计算一个数，就需要打开计算器软件才能计算，Alfred可以免去这一步，直接在搜索框输入计算数据： 历史复制记录输入框输入cli: 回车即可出现历史复制过的记录： 个人不喜欢一按回车就将复制内容粘贴到活跃输入上，可以在这里去掉： shell操作在命令可以直接输入shell命令，并回车执行： 默认会去Terminal执行，不过可以修改为iTerm2： 在这里修改： 那段内容是iterm2官网提供的内容： 123456789101112131415161718192021222324252627282930313233343536373839404142434445-- This is v0.7 of the custom script for AlfredApp for iTerm 3.1.1+-- created by Sinan Eldem www.sinaneldem.com.tron alfred_script(q) if application "iTerm2" is running or application "iTerm" is running then run script " on run &#123;q&#125; tell application \"iTerm\" activate try select first window set onlywindow to true on error create window with default profile select first window set onlywindow to true end try tell the first window if onlywindow is false then create tab with default profile end if tell current session to write text q end tell end tell end run " with parameters &#123;q&#125; else run script " on run &#123;q&#125; tell application \"iTerm\" activate try select first window on error create window with default profile select first window end try tell the first window tell current session to write text q end tell end tell end run " with parameters &#123;q&#125; end ifend alfred_script 至此，简单的、免费的使用，基本上就这些功能，如果觉得够用就没必要往下看了，下面的是要收(po)费(jie)的。 重度使用通过Workflows的编排，可以将一件复杂多步骤的事情一步完成。 可以通过自己编排，也可以网上搜索相关的Workflows导入，如个人搜集到的有： 翻译工具有道翻译 谷歌翻译 数据转换时间戳转换 数字进制转换 hash生成 编码解码 随机字符生成 工作工具idea窗口切换Alfred默认只能在APP之间切换，但是工作时通常会打开多个idea窗口，这时就可以用到idea的workflows，输入前缀idea，后面输入项目名，回车即可去到对应项目窗口： 谷歌身份验证器安全原因，登录服务器都是需要谷歌验证码的，这时使用Alfred就很方便，输入gauth直接获取： ip地址查找 日期查看 如果文章有帮助，请点赞转发，有更好的Workflows推荐，欢迎评论留言。]]></content>
      <categories>
        <category>笔记</category>
      </categories>
      <tags>
        <tag>mac</tag>
        <tag>Alfred</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[sha512在线加密]]></title>
    <url>%2F2019%2F04%2F27%2Fsha512%E5%9C%A8%E7%BA%BF%E5%8A%A0%E5%AF%86%2F</url>
    <content type="text"><![CDATA[其他加密签名工具更多的实用小玩意请查看在线工具 var hexcase = 0; /* hex output format. 0 - lowercase; 1 - uppercase */var b64pad = ""; /* base-64 pad character. "=" for strict RFC compliance *//* * These are the functions you'll usually want to call * They take string arguments and return either hex or base-64 encoded strings */function hex_sha512(s) { return rstr2hex(rstr_sha512(str2rstr_utf8(s))); }function b64_sha512(s) { return rstr2b64(rstr_sha512(str2rstr_utf8(s))); }function any_sha512(s, e) { return rstr2any(rstr_sha512(str2rstr_utf8(s)), e);}function hex_hmac_sha512(k, d) { return rstr2hex(rstr_hmac_sha512(str2rstr_utf8(k), str2rstr_utf8(d))); }function b64_hmac_sha512(k, d) { return rstr2b64(rstr_hmac_sha512(str2rstr_utf8(k), str2rstr_utf8(d))); }function any_hmac_sha512(k, d, e) { return rstr2any(rstr_hmac_sha512(str2rstr_utf8(k), str2rstr_utf8(d)), e);}/* * Perform a simple self-test to see if the VM is working */function sha512_vm_test(){ return hex_sha512("abc").toLowerCase() == "ddaf35a193617abacc417349ae20413112e6fa4e89a97ea20a9eeee64b55d39a" + "2192992a274fc1a836ba3c23a3feebbd454d4423643ce80e2a9ac94fa54ca49f";}/* * Calculate the SHA-512 of a raw string */function rstr_sha512(s){ return binb2rstr(binb_sha512(rstr2binb(s), s.length * 8));}/* * Calculate the HMAC-SHA-512 of a key and some data (raw strings) */function rstr_hmac_sha512(key, data){ var bkey = rstr2binb(key); if(bkey.length > 32) bkey = binb_sha512(bkey, key.length * 8); var ipad = Array(32), opad = Array(32); for(var i = 0; i < 32; i++) { ipad[i] = bkey[i] ^ 0x36363636; opad[i] = bkey[i] ^ 0x5C5C5C5C; } var hash = binb_sha512(ipad.concat(rstr2binb(data)), 1024 + data.length * 8); return binb2rstr(binb_sha512(opad.concat(hash), 1024 + 512));}/* * Convert a raw string to a hex string */function rstr2hex(input){ try { hexcase } catch(e) { hexcase=0; } var hex_tab = hexcase ? "0123456789ABCDEF" : "0123456789abcdef"; var output = ""; var x; for(var i = 0; i < input.length; i++) { x = input.charCodeAt(i); output += hex_tab.charAt((x >>> 4) & 0x0F) + hex_tab.charAt( x & 0x0F); } return output;}/* * Convert a raw string to a base-64 string */function rstr2b64(input){ try { b64pad } catch(e) { b64pad=''; } var tab = "ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789+/"; var output = ""; var len = input.length; for(var i = 0; i < len; i += 3) { var triplet = (input.charCodeAt(i) < 16) | (i + 1 < len ? input.charCodeAt(i+1) < 8 : 0) | (i + 2 < len ? input.charCodeAt(i+2) : 0); for(var j = 0; j < 4; j++) { if(i * 8 + j * 6 > input.length * 8) output += b64pad; else output += tab.charAt((triplet >>> 6*(3-j)) & 0x3F); } } return output;}/* * Convert a raw string to an arbitrary string encoding */function rstr2any(input, encoding){ var divisor = encoding.length; var i, j, q, x, quotient; /* Convert to an array of 16-bit big-endian values, forming the dividend */ var dividend = Array(Math.ceil(input.length / 2)); for(i = 0; i < dividend.length; i++) { dividend[i] = (input.charCodeAt(i * 2) < 8) | input.charCodeAt(i * 2 + 1); } /* * Repeatedly perform a long division. The binary array forms the dividend, * the length of the encoding is the divisor. Once computed, the quotient * forms the dividend for the next step. All remainders are stored for later * use. */ var full_length = Math.ceil(input.length * 8 / (Math.log(encoding.length) / Math.log(2))); var remainders = Array(full_length); for(j = 0; j < full_length; j++) { quotient = Array(); x = 0; for(i = 0; i < dividend.length; i++) { x = (x < 16) + dividend[i]; q = Math.floor(x / divisor); x -= q * divisor; if(quotient.length > 0 || q > 0) quotient[quotient.length] = q; } remainders[j] = x; dividend = quotient; } /* Convert the remainders to the output string */ var output = ""; for(i = remainders.length - 1; i >= 0; i--) output += encoding.charAt(remainders[i]); return output;}/* * Encode a string as utf-8. * For efficiency, this assumes the input is valid utf-16. */function str2rstr_utf8(input){ var output = ""; var i = -1; var x, y; while(++i < input.length) { /* Decode utf-16 surrogate pairs */ x = input.charCodeAt(i); y = i + 1 < input.length ? input.charCodeAt(i + 1) : 0; if(0xD800 > 12) & 0x3F), 0x80 | ((x >>> 6 ) & 0x3F), 0x80 | ( x & 0x3F)); } return output;}/* * Encode a string as utf-16 */function str2rstr_utf16le(input){ var output = ""; for(var i = 0; i < input.length; i++) output += String.fromCharCode( input.charCodeAt(i) & 0xFF, (input.charCodeAt(i) >>> 8) & 0xFF); return output;}function str2rstr_utf16be(input){ var output = ""; for(var i = 0; i < input.length; i++) output += String.fromCharCode((input.charCodeAt(i) >>> 8) & 0xFF, input.charCodeAt(i) & 0xFF); return output;}/* * Convert a raw string to an array of big-endian words * Characters >255 have their high-byte silently ignored. */function rstr2binb(input){ var output = Array(input.length >> 2); for(var i = 0; i < output.length; i++) output[i] = 0; for(var i = 0; i < input.length * 8; i += 8) output[i>>5] |= (input.charCodeAt(i / 8) & 0xFF) < (24 - i % 32); return output;}/* * Convert an array of big-endian words to a string */function binb2rstr(input){ var output = ""; for(var i = 0; i < input.length * 32; i += 8) output += String.fromCharCode((input[i>>5] >>> (24 - i % 32)) & 0xFF); return output;}/* * Calculate the SHA-512 of an array of big-endian dwords, and a bit length */var sha512_k;function binb_sha512(x, len){ if(sha512_k == undefined) { sha512_k = new Array(new int64(0x428a2f98, -685199838), new int64(0x71374491, 0x23ef65cd),new int64(-1245643825, -330482897), new int64(-373957723, -2121671748),new int64(0x3956c25b, -213338824), new int64(0x59f111f1, -1241133031),new int64(-1841331548, -1357295717), new int64(-1424204075, -630357736),new int64(-670586216, -1560083902), new int64(0x12835b01, 0x45706fbe),new int64(0x243185be, 0x4ee4b28c), new int64(0x550c7dc3, -704662302),new int64(0x72be5d74, -226784913), new int64(-2132889090, 0x3b1696b1),new int64(-1680079193, 0x25c71235), new int64(-1046744716, -815192428),new int64(-459576895, -1628353838), new int64(-272742522, 0x384f25e3),new int64(0xfc19dc6, -1953704523), new int64(0x240ca1cc, 0x77ac9c65),new int64(0x2de92c6f, 0x592b0275), new int64(0x4a7484aa, 0x6ea6e483),new int64(0x5cb0a9dc, -1119749164), new int64(0x76f988da, -2096016459),new int64(-1740746414, -295247957), new int64(-1473132947, 0x2db43210),new int64(-1341970488, -1728372417), new int64(-1084653625, -1091629340),new int64(-958395405, 0x3da88fc2), new int64(-710438585, -1828018395),new int64(0x6ca6351, -536640913), new int64(0x14292967, 0xa0e6e70),new int64(0x27b70a85, 0x46d22ffc), new int64(0x2e1b2138, 0x5c26c926),new int64(0x4d2c6dfc, 0x5ac42aed), new int64(0x53380d13, -1651133473),new int64(0x650a7354, -1951439906), new int64(0x766a0abb, 0x3c77b2a8),new int64(-2117940946, 0x47edaee6), new int64(-1838011259, 0x1482353b),new int64(-1564481375, 0x4cf10364), new int64(-1474664885, -1136513023),new int64(-1035236496, -789014639), new int64(-949202525, 0x654be30),new int64(-778901479, -688958952), new int64(-694614492, 0x5565a910),new int64(-200395387, 0x5771202a), new int64(0x106aa070, 0x32bbd1b8),new int64(0x19a4c116, -1194143544), new int64(0x1e376c08, 0x5141ab53),new int64(0x2748774c, -544281703), new int64(0x34b0bcb5, -509917016),new int64(0x391c0cb3, -976659869), new int64(0x4ed8aa4a, -482243893),new int64(0x5b9cca4f, 0x7763e373), new int64(0x682e6ff3, -692930397),new int64(0x748f82ee, 0x5defb2fc), new int64(0x78a5636f, 0x43172f60),new int64(-2067236844, -1578062990), new int64(-1933114872, 0x1a6439ec),new int64(-1866530822, 0x23631e28), new int64(-1538233109, -561857047),new int64(-1090935817, -1295615723), new int64(-965641998, -479046869),new int64(-903397682, -366583396), new int64(-779700025, 0x21c0c207),new int64(-354779690, -840897762), new int64(-176337025, -294727304),new int64(0x6f067aa, 0x72176fba), new int64(0xa637dc5, -1563912026),new int64(0x113f9804, -1090974290), new int64(0x1b710b35, 0x131c471b),new int64(0x28db77f5, 0x23047d84), new int64(0x32caab7b, 0x40c72493),new int64(0x3c9ebe0a, 0x15c9bebc), new int64(0x431d67c4, -1676669620),new int64(0x4cc5d4be, -885112138), new int64(0x597f299c, -60457430),new int64(0x5fcb6fab, 0x3ad6faec), new int64(0x6c44198c, 0x4a475817)); } var H = new Array(new int64(0x6a09e667, -205731576),new int64(-1150833019, -2067093701),new int64(0x3c6ef372, -23791573),new int64(-1521486534, 0x5f1d36f1),new int64(0x510e527f, -1377402159),new int64(-1694144372, 0x2b3e6c1f),new int64(0x1f83d9ab, -79577749),new int64(0x5be0cd19, 0x137e2179)); var T1 = new int64(0, 0), T2 = new int64(0, 0), a = new int64(0,0), b = new int64(0,0), c = new int64(0,0), d = new int64(0,0), e = new int64(0,0), f = new int64(0,0), g = new int64(0,0), h = new int64(0,0), s0 = new int64(0, 0), s1 = new int64(0, 0), Ch = new int64(0, 0), Maj = new int64(0, 0), r1 = new int64(0, 0), r2 = new int64(0, 0), r3 = new int64(0, 0); var j, i; var W = new Array(80); for(i=0; i 5] |= 0x80 < (24 - (len & 0x1f)); x[((len + 128 >> 10)< 5) + 31] = len; for(i = 0; i shift) | (x.h < (32-shift)); dst.h = (x.h >>> shift);}function int64add(dst, x, y){ var w0 = (x.l & 0xffff) + (y.l & 0xffff); var w1 = (x.l >>> 16) + (y.l >>> 16) + (w0 >>> 16); var w2 = (x.h & 0xffff) + (y.h & 0xffff) + (w1 >>> 16); var w3 = (x.h >>> 16) + (y.h >>> 16) + (w2 >>> 16); dst.l = (w0 & 0xffff) | (w1 < 16); dst.h = (w2 & 0xffff) | (w3 < 16);}function int64add4(dst, a, b, c, d){ var w0 = (a.l & 0xffff) + (b.l & 0xffff) + (c.l & 0xffff) + (d.l & 0xffff); var w1 = (a.l >>> 16) + (b.l >>> 16) + (c.l >>> 16) + (d.l >>> 16) + (w0 >>> 16); var w2 = (a.h & 0xffff) + (b.h & 0xffff) + (c.h & 0xffff) + (d.h & 0xffff) + (w1 >>> 16); var w3 = (a.h >>> 16) + (b.h >>> 16) + (c.h >>> 16) + (d.h >>> 16) + (w2 >>> 16); dst.l = (w0 & 0xffff) | (w1 < 16); dst.h = (w2 & 0xffff) | (w3 < 16);}function int64add5(dst, a, b, c, d, e){ var w0 = (a.l & 0xffff) + (b.l & 0xffff) + (c.l & 0xffff) + (d.l & 0xffff) + (e.l & 0xffff); var w1 = (a.l >>> 16) + (b.l >>> 16) + (c.l >>> 16) + (d.l >>> 16) + (e.l >>> 16) + (w0 >>> 16); var w2 = (a.h & 0xffff) + (b.h & 0xffff) + (c.h & 0xffff) + (d.h & 0xffff) + (e.h & 0xffff) + (w1 >>> 16); var w3 = (a.h >>> 16) + (b.h >>> 16) + (c.h >>> 16) + (d.h >>> 16) + (e.h >>> 16) + (w2 >>> 16); dst.l = (w0 & 0xffff) | (w1 < 16); dst.h = (w2 & 0xffff) | (w3 < 16);}function encode(){ var content = document.getElementById("codeContent"); var outcontent = document.getElementById("outcodeContent"); outcontent.value=hex_sha512(content.value);}function cl(){ var content = document.getElementById("codeContent"); var outcontent = document.getElementById("outcodeContent"); content.value=""; outcontent.value="";}]]></content>
      <categories>
        <category>在线工具</category>
      </categories>
      <tags>
        <tag>编码解码</tag>
        <tag>在线签名</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[sha256在线加密]]></title>
    <url>%2F2019%2F04%2F27%2Fsha256%E5%9C%A8%E7%BA%BF%E5%8A%A0%E5%AF%86%2F</url>
    <content type="text"><![CDATA[其他加密签名工具更多的实用小玩意请查看在线工具 var hexcase = 0; /* hex output format. 0 - lowercase; 1 - uppercase */var b64pad = ""; /* base-64 pad character. "=" for strict RFC compliance *//* * These are the functions you'll usually want to call * They take string arguments and return either hex or base-64 encoded strings */function hex_sha256(s) { return rstr2hex(rstr_sha256(str2rstr_utf8(s))); }function b64_sha256(s) { return rstr2b64(rstr_sha256(str2rstr_utf8(s))); }function any_sha256(s, e) { return rstr2any(rstr_sha256(str2rstr_utf8(s)), e); }function hex_hmac_sha256(k, d) { return rstr2hex(rstr_hmac_sha256(str2rstr_utf8(k), str2rstr_utf8(d))); }function b64_hmac_sha256(k, d) { return rstr2b64(rstr_hmac_sha256(str2rstr_utf8(k), str2rstr_utf8(d))); }function any_hmac_sha256(k, d, e) { return rstr2any(rstr_hmac_sha256(str2rstr_utf8(k), str2rstr_utf8(d)), e); }/* * Perform a simple self-test to see if the VM is working */function sha256_vm_test(){ return hex_sha256("abc").toLowerCase() == "ba7816bf8f01cfea414140de5dae2223b00361a396177a9cb410ff61f20015ad";}/* * Calculate the sha256 of a raw string */function rstr_sha256(s){ return binb2rstr(binb_sha256(rstr2binb(s), s.length * 8));}/* * Calculate the HMAC-sha256 of a key and some data (raw strings) */function rstr_hmac_sha256(key, data){ var bkey = rstr2binb(key); if(bkey.length > 16) bkey = binb_sha256(bkey, key.length * 8); var ipad = Array(16), opad = Array(16); for(var i = 0; i < 16; i++) { ipad[i] = bkey[i] ^ 0x36363636; opad[i] = bkey[i] ^ 0x5C5C5C5C; } var hash = binb_sha256(ipad.concat(rstr2binb(data)), 512 + data.length * 8); return binb2rstr(binb_sha256(opad.concat(hash), 512 + 256));}/* * Convert a raw string to a hex string */function rstr2hex(input){ try { hexcase } catch(e) { hexcase=0; } var hex_tab = hexcase ? "0123456789ABCDEF" : "0123456789abcdef"; var output = ""; var x; for(var i = 0; i < input.length; i++) { x = input.charCodeAt(i); output += hex_tab.charAt((x >>> 4) & 0x0F) + hex_tab.charAt( x & 0x0F); } return output;}/* * Convert a raw string to a base-64 string */function rstr2b64(input){ try { b64pad } catch(e) { b64pad=''; } var tab = "ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789+/"; var output = ""; var len = input.length; for(var i = 0; i < len; i += 3) { var triplet = (input.charCodeAt(i) < 16) | (i + 1 < len ? input.charCodeAt(i+1) < 8 : 0) | (i + 2 < len ? input.charCodeAt(i+2) : 0); for(var j = 0; j < 4; j++) { if(i * 8 + j * 6 > input.length * 8) output += b64pad; else output += tab.charAt((triplet >>> 6*(3-j)) & 0x3F); } } return output;}/* * Convert a raw string to an arbitrary string encoding */function rstr2any(input, encoding){ var divisor = encoding.length; var remainders = Array(); var i, q, x, quotient; /* Convert to an array of 16-bit big-endian values, forming the dividend */ var dividend = Array(Math.ceil(input.length / 2)); for(i = 0; i < dividend.length; i++) { dividend[i] = (input.charCodeAt(i * 2) < 8) | input.charCodeAt(i * 2 + 1); } /* * Repeatedly perform a long division. The binary array forms the dividend, * the length of the encoding is the divisor. Once computed, the quotient * forms the dividend for the next step. We stop when the dividend is zero. * All remainders are stored for later use. */ while(dividend.length > 0) { quotient = Array(); x = 0; for(i = 0; i < dividend.length; i++) { x = (x < 16) + dividend[i]; q = Math.floor(x / divisor); x -= q * divisor; if(quotient.length > 0 || q > 0) quotient[quotient.length] = q; } remainders[remainders.length] = x; dividend = quotient; } /* Convert the remainders to the output string */ var output = ""; for(i = remainders.length - 1; i >= 0; i--) output += encoding.charAt(remainders[i]); /* Append leading zero equivalents */ var full_length = Math.ceil(input.length * 8 / (Math.log(encoding.length) / Math.log(2))); for(i = output.length; i < full_length; i++) output = encoding[0] + output; return output;}/* * Encode a string as utf-8. * For efficiency, this assumes the input is valid utf-16. */function str2rstr_utf8(input){ var output = ""; var i = -1; var x, y; while(++i < input.length) { /* Decode utf-16 surrogate pairs */ x = input.charCodeAt(i); y = i + 1 < input.length ? input.charCodeAt(i + 1) : 0; if(0xD800 > 12) & 0x3F), 0x80 | ((x >>> 6 ) & 0x3F), 0x80 | ( x & 0x3F)); } return output;}/* * Encode a string as utf-16 */function str2rstr_utf16le(input){ var output = ""; for(var i = 0; i < input.length; i++) output += String.fromCharCode( input.charCodeAt(i) & 0xFF, (input.charCodeAt(i) >>> 8) & 0xFF); return output;}function str2rstr_utf16be(input){ var output = ""; for(var i = 0; i < input.length; i++) output += String.fromCharCode((input.charCodeAt(i) >>> 8) & 0xFF, input.charCodeAt(i) & 0xFF); return output;}/* * Convert a raw string to an array of big-endian words * Characters >255 have their high-byte silently ignored. */function rstr2binb(input){ var output = Array(input.length >> 2); for(var i = 0; i < output.length; i++) output[i] = 0; for(var i = 0; i < input.length * 8; i += 8) output[i>>5] |= (input.charCodeAt(i / 8) & 0xFF) < (24 - i % 32); return output;}/* * Convert an array of big-endian words to a string */function binb2rstr(input){ var output = ""; for(var i = 0; i < input.length * 32; i += 8) output += String.fromCharCode((input[i>>5] >>> (24 - i % 32)) & 0xFF); return output;}/* * Main sha256 function, with its support functions */function sha256_S (X, n) {return ( X >>> n ) | (X < (32 - n));}function sha256_R (X, n) {return ( X >>> n );}function sha256_Ch(x, y, z) {return ((x & y) ^ ((~x) & z));}function sha256_Maj(x, y, z) {return ((x & y) ^ (x & z) ^ (y & z));}function sha256_Sigma0256(x) {return (sha256_S(x, 2) ^ sha256_S(x, 13) ^ sha256_S(x, 22));}function sha256_Sigma1256(x) {return (sha256_S(x, 6) ^ sha256_S(x, 11) ^ sha256_S(x, 25));}function sha256_Gamma0256(x) {return (sha256_S(x, 7) ^ sha256_S(x, 18) ^ sha256_R(x, 3));}function sha256_Gamma1256(x) {return (sha256_S(x, 17) ^ sha256_S(x, 19) ^ sha256_R(x, 10));}function sha256_Sigma0512(x) {return (sha256_S(x, 28) ^ sha256_S(x, 34) ^ sha256_S(x, 39));}function sha256_Sigma1512(x) {return (sha256_S(x, 14) ^ sha256_S(x, 18) ^ sha256_S(x, 41));}function sha256_Gamma0512(x) {return (sha256_S(x, 1) ^ sha256_S(x, 8) ^ sha256_R(x, 7));}function sha256_Gamma1512(x) {return (sha256_S(x, 19) ^ sha256_S(x, 61) ^ sha256_R(x, 6));}var sha256_K = new Array( 1116352408, 1899447441, -1245643825, -373957723, 961987163, 1508970993, -1841331548, -1424204075, -670586216, 310598401, 607225278, 1426881987, 1925078388, -2132889090, -1680079193, -1046744716, -459576895, -272742522, 264347078, 604807628, 770255983, 1249150122, 1555081692, 1996064986, -1740746414, -1473132947, -1341970488, -1084653625, -958395405, -710438585, 113926993, 338241895, 666307205, 773529912, 1294757372, 1396182291, 1695183700, 1986661051, -2117940946, -1838011259, -1564481375, -1474664885, -1035236496, -949202525, -778901479, -694614492, -200395387, 275423344, 430227734, 506948616, 659060556, 883997877, 958139571, 1322822218, 1537002063, 1747873779, 1955562222, 2024104815, -2067236844, -1933114872, -1866530822, -1538233109, -1090935817, -965641998);function binb_sha256(m, l){ var HASH = new Array(1779033703, -1150833019, 1013904242, -1521486534, 1359893119, -1694144372, 528734635, 1541459225); var W = new Array(64); var a, b, c, d, e, f, g, h; var i, j, T1, T2; /* append padding */ m[l >> 5] |= 0x80 < (24 - l % 32); m[((l + 64 >> 9) < 4) + 15] = l; for(i = 0; i < m.length; i += 16) { a = HASH[0]; b = HASH[1]; c = HASH[2]; d = HASH[3]; e = HASH[4]; f = HASH[5]; g = HASH[6]; h = HASH[7]; for(j = 0; j < 64; j++) { if (j < 16) W[j] = m[j + i]; else W[j] = safe_add(safe_add(safe_add(sha256_Gamma1256(W[j - 2]), W[j - 7]), sha256_Gamma0256(W[j - 15])), W[j - 16]); T1 = safe_add(safe_add(safe_add(safe_add(h, sha256_Sigma1256(e)), sha256_Ch(e, f, g)), sha256_K[j]), W[j]); T2 = safe_add(sha256_Sigma0256(a), sha256_Maj(a, b, c)); h = g; g = f; f = e; e = safe_add(d, T1); d = c; c = b; b = a; a = safe_add(T1, T2); } HASH[0] = safe_add(a, HASH[0]); HASH[1] = safe_add(b, HASH[1]); HASH[2] = safe_add(c, HASH[2]); HASH[3] = safe_add(d, HASH[3]); HASH[4] = safe_add(e, HASH[4]); HASH[5] = safe_add(f, HASH[5]); HASH[6] = safe_add(g, HASH[6]); HASH[7] = safe_add(h, HASH[7]); } return HASH;}function safe_add (x, y){ var lsw = (x & 0xFFFF) + (y & 0xFFFF); var msw = (x >> 16) + (y >> 16) + (lsw >> 16); return (msw < 16) | (lsw & 0xFFFF);}function encode(){ var content = document.getElementById("codeContent"); var outcontent = document.getElementById("outcodeContent"); outcontent.value=hex_sha256(content.value);}function cl(){ var content = document.getElementById("codeContent"); var outcontent = document.getElementById("outcodeContent"); content.value=""; outcontent.value="";}]]></content>
      <categories>
        <category>在线工具</category>
      </categories>
      <tags>
        <tag>编码解码</tag>
        <tag>在线签名</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[sha1在线加密]]></title>
    <url>%2F2019%2F04%2F27%2Fsha1%E5%9C%A8%E7%BA%BF%E5%8A%A0%E5%AF%86%2F</url>
    <content type="text"><![CDATA[其他加密签名工具更多的实用小玩意请查看在线工具 var hexcase = 0; /* hex output format. 0 - lowercase; 1 - uppercase */var b64pad = ""; /* base-64 pad character. "=" for strict RFC compliance *//* * These are the functions you'll usually want to call * They take string arguments and return either hex or base-64 encoded strings */function hex_sha1(s) { return rstr2hex(rstr_sha1(str2rstr_utf8(s))); }function b64_sha1(s) { return rstr2b64(rstr_sha1(str2rstr_utf8(s))); }function any_sha1(s, e) { return rstr2any(rstr_sha1(str2rstr_utf8(s)), e); }function hex_hmac_sha1(k, d) { return rstr2hex(rstr_hmac_sha1(str2rstr_utf8(k), str2rstr_utf8(d))); }function b64_hmac_sha1(k, d) { return rstr2b64(rstr_hmac_sha1(str2rstr_utf8(k), str2rstr_utf8(d))); }function any_hmac_sha1(k, d, e) { return rstr2any(rstr_hmac_sha1(str2rstr_utf8(k), str2rstr_utf8(d)), e); }/* * Perform a simple self-test to see if the VM is working */function sha1_vm_test(){ return hex_sha1("abc").toLowerCase() == "a9993e364706816aba3e25717850c26c9cd0d89d";}/* * Calculate the SHA1 of a raw string */function rstr_sha1(s){ return binb2rstr(binb_sha1(rstr2binb(s), s.length * 8));}/* * Calculate the HMAC-SHA1 of a key and some data (raw strings) */function rstr_hmac_sha1(key, data){ var bkey = rstr2binb(key); if(bkey.length > 16) bkey = binb_sha1(bkey, key.length * 8); var ipad = Array(16), opad = Array(16); for(var i = 0; i < 16; i++) { ipad[i] = bkey[i] ^ 0x36363636; opad[i] = bkey[i] ^ 0x5C5C5C5C; } var hash = binb_sha1(ipad.concat(rstr2binb(data)), 512 + data.length * 8); return binb2rstr(binb_sha1(opad.concat(hash), 512 + 160));}/* * Convert a raw string to a hex string */function rstr2hex(input){ try { hexcase } catch(e) { hexcase=0; } var hex_tab = hexcase ? "0123456789ABCDEF" : "0123456789abcdef"; var output = ""; var x; for(var i = 0; i < input.length; i++) { x = input.charCodeAt(i); output += hex_tab.charAt((x >>> 4) & 0x0F) + hex_tab.charAt( x & 0x0F); } return output;}/* * Convert a raw string to a base-64 string */function rstr2b64(input){ try { b64pad } catch(e) { b64pad=''; } var tab = "ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789+/"; var output = ""; var len = input.length; for(var i = 0; i < len; i += 3) { var triplet = (input.charCodeAt(i) < 16) | (i + 1 < len ? input.charCodeAt(i+1) < 8 : 0) | (i + 2 < len ? input.charCodeAt(i+2) : 0); for(var j = 0; j < 4; j++) { if(i * 8 + j * 6 > input.length * 8) output += b64pad; else output += tab.charAt((triplet >>> 6*(3-j)) & 0x3F); } } return output;}/* * Convert a raw string to an arbitrary string encoding */function rstr2any(input, encoding){ var divisor = encoding.length; var remainders = Array(); var i, q, x, quotient; /* Convert to an array of 16-bit big-endian values, forming the dividend */ var dividend = Array(Math.ceil(input.length / 2)); for(i = 0; i < dividend.length; i++) { dividend[i] = (input.charCodeAt(i * 2) < 8) | input.charCodeAt(i * 2 + 1); } /* * Repeatedly perform a long division. The binary array forms the dividend, * the length of the encoding is the divisor. Once computed, the quotient * forms the dividend for the next step. We stop when the dividend is zero. * All remainders are stored for later use. */ while(dividend.length > 0) { quotient = Array(); x = 0; for(i = 0; i < dividend.length; i++) { x = (x < 16) + dividend[i]; q = Math.floor(x / divisor); x -= q * divisor; if(quotient.length > 0 || q > 0){ quotient[quotient.length] = q; } } remainders[remainders.length] = x; dividend = quotient; } /* Convert the remainders to the output string */ var output = ""; for(i = remainders.length - 1; i >= 0; i--){ output += encoding.charAt(remainders[i]); } /* Append leading zero equivalents */ var full_length = Math.ceil(input.length * 8 / (Math.log(encoding.length) / Math.log(2))); for(i = output.length; i < full_length; i++){ output = encoding[0] + output; } return output;}/* * Encode a string as utf-8. * For efficiency, this assumes the input is valid utf-16. */function str2rstr_utf8(input){ var output = ""; var i = -1; var x, y; while(++i < input.length) { /* Decode utf-16 surrogate pairs */ x = input.charCodeAt(i); y = i + 1 < input.length ? input.charCodeAt(i + 1) : 0; if(0xD800 > 12) & 0x3F), 0x80 | ((x >>> 6 ) & 0x3F), 0x80 | ( x & 0x3F)); } } return output;}/* * Encode a string as utf-16 */function str2rstr_utf16le(input){ var output = ""; for(var i = 0; i < input.length; i++){ output += String.fromCharCode( input.charCodeAt(i) & 0xFF, (input.charCodeAt(i) >>> 8) & 0xFF); } return output;}function str2rstr_utf16be(input){ var output = ""; for(var i = 0; i < input.length; i++){ output += String.fromCharCode((input.charCodeAt(i) >>> 8) & 0xFF, input.charCodeAt(i) & 0xFF); } return output;}/* * Convert a raw string to an array of big-endian words * Characters >255 have their high-byte silently ignored. */function rstr2binb(input){ var output = Array(input.length >> 2); for(var i = 0; i < output.length; i++){ output[i] = 0; } for(var i = 0; i < input.length * 8; i += 8){ output[i>>5] |= (input.charCodeAt(i / 8) & 0xFF) < (24 - i % 32); } return output;}/* * Convert an array of big-endian words to a string */function binb2rstr(input){ var output = ""; for(var i = 0; i < input.length * 32; i += 8){ output += String.fromCharCode((input[i>>5] >>> (24 - i % 32)) & 0xFF); } return output;}/* * Calculate the SHA-1 of an array of big-endian words, and a bit length */function binb_sha1(x, len){ /* append padding */ x[len >> 5] |= 0x80 < (24 - len % 32); x[((len + 64 >> 9) < 4) + 15] = len; var w = Array(80); var a = 1732584193; var b = -271733879; var c = -1732584194; var d = 271733878; var e = -1009589776; for(var i = 0; i < x.length; i += 16) { var olda = a; var oldb = b; var oldc = c; var oldd = d; var olde = e; for(var j = 0; j < 80; j++) { if(j < 16) w[j] = x[i + j]; else w[j] = bit_rol(w[j-3] ^ w[j-8] ^ w[j-14] ^ w[j-16], 1); var t = safe_add(safe_add(bit_rol(a, 5), sha1_ft(j, b, c, d)), safe_add(safe_add(e, w[j]), sha1_kt(j))); e = d; d = c; c = bit_rol(b, 30); b = a; a = t; } a = safe_add(a, olda); b = safe_add(b, oldb); c = safe_add(c, oldc); d = safe_add(d, oldd); e = safe_add(e, olde); } return Array(a, b, c, d, e);}/* * Perform the appropriate triplet combination function for the current * iteration */function sha1_ft(t, b, c, d){ if(t < 20) return (b & c) | ((~b) & d); if(t < 40) return b ^ c ^ d; if(t < 60) return (b & c) | (b & d) | (c & d); return b ^ c ^ d;}/* * Determine the appropriate additive constant for the current iteration */function sha1_kt(t){ return (t < 20) ? 1518500249 : (t < 40) ? 1859775393 : (t < 60) ? -1894007588 : -899497514;}/* * Add integers, wrapping at 2^32. This uses 16-bit operations internally * to work around bugs in some JS interpreters. */function safe_add(x, y){ var lsw = (x & 0xFFFF) + (y & 0xFFFF); var msw = (x >> 16) + (y >> 16) + (lsw >> 16); return (msw < 16) | (lsw & 0xFFFF);}/* * Bitwise rotate a 32-bit number to the left. */function bit_rol(num, cnt){ return (num < cnt) | (num >>> (32 - cnt));}function encode(){ var content = document.getElementById("codeContent"); var outcontent = document.getElementById("outcodeContent"); outcontent.value=hex_sha1(content.value);}function cl(){ var content = document.getElementById("codeContent"); var outcontent = document.getElementById("outcodeContent"); content.value=""; outcontent.value="";}]]></content>
      <categories>
        <category>在线工具</category>
      </categories>
      <tags>
        <tag>编码解码</tag>
        <tag>在线签名</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[md5在线加密]]></title>
    <url>%2F2019%2F04%2F27%2Fmd5%E5%9C%A8%E7%BA%BF%E5%8A%A0%E5%AF%86%2F</url>
    <content type="text"><![CDATA[其他加密签名工具更多的实用小玩意请查看在线工具 var hexcase = 0; /* hex output format. 0 - lowercase; 1 - uppercase */var b64pad = ""; /* base-64 pad character. "=" for strict RFC compliance *//* * These are the functions you'll usually want to call * They take string arguments and return either hex or base-64 encoded strings */function hex_md5(s) { return rstr2hex(rstr_md5(str2rstr_utf8(s))); }function b64_md5(s) { return rstr2b64(rstr_md5(str2rstr_utf8(s))); }function any_md5(s, e) { return rstr2any(rstr_md5(str2rstr_utf8(s)), e); }function hex_hmac_md5(k, d) { return rstr2hex(rstr_hmac_md5(str2rstr_utf8(k), str2rstr_utf8(d))); }function b64_hmac_md5(k, d) { return rstr2b64(rstr_hmac_md5(str2rstr_utf8(k), str2rstr_utf8(d))); }function any_hmac_md5(k, d, e) { return rstr2any(rstr_hmac_md5(str2rstr_utf8(k), str2rstr_utf8(d)), e); }/* * Perform a simple self-test to see if the VM is working */function md5_vm_test(){ return hex_md5("abc").toLowerCase() == "900150983cd24fb0d6963f7d28e17f72";}/* * Calculate the MD5 of a raw string */function rstr_md5(s){ return binl2rstr(binl_md5(rstr2binl(s), s.length * 8));}/* * Calculate the HMAC-MD5, of a key and some data (raw strings) */function rstr_hmac_md5(key, data){ var bkey = rstr2binl(key); if(bkey.length > 16) bkey = binl_md5(bkey, key.length * 8); var ipad = Array(16), opad = Array(16); for(var i = 0; i < 16; i++) { ipad[i] = bkey[i] ^ 0x36363636; opad[i] = bkey[i] ^ 0x5C5C5C5C; } var hash = binl_md5(ipad.concat(rstr2binl(data)), 512 + data.length * 8); return binl2rstr(binl_md5(opad.concat(hash), 512 + 128));}/* * Convert a raw string to a hex string */function rstr2hex(input){ try { hexcase } catch(e) { hexcase=0; } var hex_tab = hexcase ? "0123456789ABCDEF" : "0123456789abcdef"; var output = ""; var x; for(var i = 0; i < input.length; i++) { x = input.charCodeAt(i); output += hex_tab.charAt((x >>> 4) & 0x0F) + hex_tab.charAt( x & 0x0F); } return output;}/* * Convert a raw string to a base-64 string */function rstr2b64(input){ try { b64pad } catch(e) { b64pad=''; } var tab = "ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789+/"; var output = ""; var len = input.length; for(var i = 0; i < len; i += 3) { var triplet = (input.charCodeAt(i) < 16) | (i + 1 < len ? input.charCodeAt(i+1) < 8 : 0) | (i + 2 < len ? input.charCodeAt(i+2) : 0); for(var j = 0; j < 4; j++) { if(i * 8 + j * 6 > input.length * 8) output += b64pad; else output += tab.charAt((triplet >>> 6*(3-j)) & 0x3F); } } return output;}/* * Convert a raw string to an arbitrary string encoding */function rstr2any(input, encoding){ var divisor = encoding.length; var i, j, q, x, quotient; /* Convert to an array of 16-bit big-endian values, forming the dividend */ var dividend = Array(Math.ceil(input.length / 2)); for(i = 0; i < dividend.length; i++) { dividend[i] = (input.charCodeAt(i * 2) < 8) | input.charCodeAt(i * 2 + 1); } /* * Repeatedly perform a long division. The binary array forms the dividend, * the length of the encoding is the divisor. Once computed, the quotient * forms the dividend for the next step. All remainders are stored for later * use. */ var full_length = Math.ceil(input.length * 8 / (Math.log(encoding.length) / Math.log(2))); var remainders = Array(full_length); for(j = 0; j < full_length; j++) { quotient = Array(); x = 0; for(i = 0; i < dividend.length; i++) { x = (x < 16) + dividend[i]; q = Math.floor(x / divisor); x -= q * divisor; if(quotient.length > 0 || q > 0) quotient[quotient.length] = q; } remainders[j] = x; dividend = quotient; } /* Convert the remainders to the output string */ var output = ""; for(i = remainders.length - 1; i >= 0; i--) output += encoding.charAt(remainders[i]); return output;}/* * Encode a string as utf-8. * For efficiency, this assumes the input is valid utf-16. */function str2rstr_utf8(input){ var output = ""; var i = -1; var x, y; while(++i < input.length) { /* Decode utf-16 surrogate pairs */ x = input.charCodeAt(i); y = i + 1 < input.length ? input.charCodeAt(i + 1) : 0; if(0xD800 > 12) & 0x3F), 0x80 | ((x >>> 6 ) & 0x3F), 0x80 | ( x & 0x3F)); } return output;}/* * Encode a string as utf-16 */function str2rstr_utf16le(input){ var output = ""; for(var i = 0; i < input.length; i++) output += String.fromCharCode( input.charCodeAt(i) & 0xFF, (input.charCodeAt(i) >>> 8) & 0xFF); return output;}function str2rstr_utf16be(input){ var output = ""; for(var i = 0; i < input.length; i++) output += String.fromCharCode((input.charCodeAt(i) >>> 8) & 0xFF, input.charCodeAt(i) & 0xFF); return output;}/* * Convert a raw string to an array of little-endian words * Characters >255 have their high-byte silently ignored. */function rstr2binl(input){ var output = Array(input.length >> 2); for(var i = 0; i < output.length; i++) output[i] = 0; for(var i = 0; i < input.length * 8; i += 8) output[i>>5] |= (input.charCodeAt(i / 8) & 0xFF) < (i%32); return output;}/* * Convert an array of little-endian words to a string */function binl2rstr(input){ var output = ""; for(var i = 0; i < input.length * 32; i += 8) output += String.fromCharCode((input[i>>5] >>> (i % 32)) & 0xFF); return output;}/* * Calculate the MD5 of an array of little-endian words, and a bit length. */function binl_md5(x, len){ /* append padding */ x[len >> 5] |= 0x80 < ((len) % 32); x[(((len + 64) >>> 9) < 4) + 14] = len; var a = 1732584193; var b = -271733879; var c = -1732584194; var d = 271733878; for(var i = 0; i < x.length; i += 16) { var olda = a; var oldb = b; var oldc = c; var oldd = d; a = md5_ff(a, b, c, d, x[i+ 0], 7 , -680876936); d = md5_ff(d, a, b, c, x[i+ 1], 12, -389564586); c = md5_ff(c, d, a, b, x[i+ 2], 17, 606105819); b = md5_ff(b, c, d, a, x[i+ 3], 22, -1044525330); a = md5_ff(a, b, c, d, x[i+ 4], 7 , -176418897); d = md5_ff(d, a, b, c, x[i+ 5], 12, 1200080426); c = md5_ff(c, d, a, b, x[i+ 6], 17, -1473231341); b = md5_ff(b, c, d, a, x[i+ 7], 22, -45705983); a = md5_ff(a, b, c, d, x[i+ 8], 7 , 1770035416); d = md5_ff(d, a, b, c, x[i+ 9], 12, -1958414417); c = md5_ff(c, d, a, b, x[i+10], 17, -42063); b = md5_ff(b, c, d, a, x[i+11], 22, -1990404162); a = md5_ff(a, b, c, d, x[i+12], 7 , 1804603682); d = md5_ff(d, a, b, c, x[i+13], 12, -40341101); c = md5_ff(c, d, a, b, x[i+14], 17, -1502002290); b = md5_ff(b, c, d, a, x[i+15], 22, 1236535329); a = md5_gg(a, b, c, d, x[i+ 1], 5 , -165796510); d = md5_gg(d, a, b, c, x[i+ 6], 9 , -1069501632); c = md5_gg(c, d, a, b, x[i+11], 14, 643717713); b = md5_gg(b, c, d, a, x[i+ 0], 20, -373897302); a = md5_gg(a, b, c, d, x[i+ 5], 5 , -701558691); d = md5_gg(d, a, b, c, x[i+10], 9 , 38016083); c = md5_gg(c, d, a, b, x[i+15], 14, -660478335); b = md5_gg(b, c, d, a, x[i+ 4], 20, -405537848); a = md5_gg(a, b, c, d, x[i+ 9], 5 , 568446438); d = md5_gg(d, a, b, c, x[i+14], 9 , -1019803690); c = md5_gg(c, d, a, b, x[i+ 3], 14, -187363961); b = md5_gg(b, c, d, a, x[i+ 8], 20, 1163531501); a = md5_gg(a, b, c, d, x[i+13], 5 , -1444681467); d = md5_gg(d, a, b, c, x[i+ 2], 9 , -51403784); c = md5_gg(c, d, a, b, x[i+ 7], 14, 1735328473); b = md5_gg(b, c, d, a, x[i+12], 20, -1926607734); a = md5_hh(a, b, c, d, x[i+ 5], 4 , -378558); d = md5_hh(d, a, b, c, x[i+ 8], 11, -2022574463); c = md5_hh(c, d, a, b, x[i+11], 16, 1839030562); b = md5_hh(b, c, d, a, x[i+14], 23, -35309556); a = md5_hh(a, b, c, d, x[i+ 1], 4 , -1530992060); d = md5_hh(d, a, b, c, x[i+ 4], 11, 1272893353); c = md5_hh(c, d, a, b, x[i+ 7], 16, -155497632); b = md5_hh(b, c, d, a, x[i+10], 23, -1094730640); a = md5_hh(a, b, c, d, x[i+13], 4 , 681279174); d = md5_hh(d, a, b, c, x[i+ 0], 11, -358537222); c = md5_hh(c, d, a, b, x[i+ 3], 16, -722521979); b = md5_hh(b, c, d, a, x[i+ 6], 23, 76029189); a = md5_hh(a, b, c, d, x[i+ 9], 4 , -640364487); d = md5_hh(d, a, b, c, x[i+12], 11, -421815835); c = md5_hh(c, d, a, b, x[i+15], 16, 530742520); b = md5_hh(b, c, d, a, x[i+ 2], 23, -995338651); a = md5_ii(a, b, c, d, x[i+ 0], 6 , -198630844); d = md5_ii(d, a, b, c, x[i+ 7], 10, 1126891415); c = md5_ii(c, d, a, b, x[i+14], 15, -1416354905); b = md5_ii(b, c, d, a, x[i+ 5], 21, -57434055); a = md5_ii(a, b, c, d, x[i+12], 6 , 1700485571); d = md5_ii(d, a, b, c, x[i+ 3], 10, -1894986606); c = md5_ii(c, d, a, b, x[i+10], 15, -1051523); b = md5_ii(b, c, d, a, x[i+ 1], 21, -2054922799); a = md5_ii(a, b, c, d, x[i+ 8], 6 , 1873313359); d = md5_ii(d, a, b, c, x[i+15], 10, -30611744); c = md5_ii(c, d, a, b, x[i+ 6], 15, -1560198380); b = md5_ii(b, c, d, a, x[i+13], 21, 1309151649); a = md5_ii(a, b, c, d, x[i+ 4], 6 , -145523070); d = md5_ii(d, a, b, c, x[i+11], 10, -1120210379); c = md5_ii(c, d, a, b, x[i+ 2], 15, 718787259); b = md5_ii(b, c, d, a, x[i+ 9], 21, -343485551); a = safe_add(a, olda); b = safe_add(b, oldb); c = safe_add(c, oldc); d = safe_add(d, oldd); } return Array(a, b, c, d);}/* * These functions implement the four basic operations the algorithm uses. */function md5_cmn(q, a, b, x, s, t){ return safe_add(bit_rol(safe_add(safe_add(a, q), safe_add(x, t)), s),b);}function md5_ff(a, b, c, d, x, s, t){ return md5_cmn((b & c) | ((~b) & d), a, b, x, s, t);}function md5_gg(a, b, c, d, x, s, t){ return md5_cmn((b & d) | (c & (~d)), a, b, x, s, t);}function md5_hh(a, b, c, d, x, s, t){ return md5_cmn(b ^ c ^ d, a, b, x, s, t);}function md5_ii(a, b, c, d, x, s, t){ return md5_cmn(c ^ (b | (~d)), a, b, x, s, t);}/* * Add integers, wrapping at 2^32. This uses 16-bit operations internally * to work around bugs in some JS interpreters. */function safe_add(x, y){ var lsw = (x & 0xFFFF) + (y & 0xFFFF); var msw = (x >> 16) + (y >> 16) + (lsw >> 16); return (msw < 16) | (lsw & 0xFFFF);}/* * Bitwise rotate a 32-bit number to the left. */function bit_rol(num, cnt){ return (num < cnt) | (num >>> (32 - cnt));}function encode(){ var content = document.getElementById("codeContent"); var outcontent = document.getElementById("outcodeContent"); outcontent.value=hex_md5(content.value);}function cl(){ var content = document.getElementById("codeContent"); var outcontent = document.getElementById("outcodeContent"); content.value=""; outcontent.value="";}]]></content>
      <categories>
        <category>在线工具</category>
      </categories>
      <tags>
        <tag>编码解码</tag>
        <tag>在线签名</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[base64编码解码工具]]></title>
    <url>%2F2019%2F04%2F17%2Fbase64%E7%BC%96%E7%A0%81%E8%A7%A3%E7%A0%81%E5%B7%A5%E5%85%B7%2F</url>
    <content type="text"><![CDATA[更多的实用小玩意请查看在线工具 function encode(){var content = document.getElementById("codeContent");content.value=window.btoa(content.value);}function decode(){var content = document.getElementById("codeContent");content.value=window.atob(content.value);}]]></content>
      <categories>
        <category>在线工具</category>
      </categories>
      <tags>
        <tag>编码解码</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[url编码解码工具]]></title>
    <url>%2F2019%2F04%2F17%2Furl%E7%BC%96%E7%A0%81%E8%A7%A3%E7%A0%81%E5%B7%A5%E5%85%B7%2F</url>
    <content type="text"><![CDATA[更多的实用小玩意请查看在线工具 function encode(){var content = document.getElementById("urlcontent");content.value=encodeURIComponent(content.value);}function decode(){var content = document.getElementById("urlcontent");content.value=decodeURIComponent(content.value);}]]></content>
      <categories>
        <category>在线工具</category>
      </categories>
      <tags>
        <tag>编码解码</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[acme生成免费HTTPS证书]]></title>
    <url>%2F2019%2F03%2F24%2Facme%E7%94%9F%E6%88%90%E5%85%8D%E8%B4%B9HTTPS%E8%AF%81%E4%B9%A6%2F</url>
    <content type="text"><![CDATA[之前使用certbot-auto来生成https证书：免费SSL证书申请，发现续租比较麻烦，支持的dns服务商太少，不支持我的dns服务商 后续找了个acme的脚本比较强大，支持很多服务商，可以自动续租，推荐使用这个。 acme.sh 实现了 acme 协议, 可以从 letsencrypt 生成免费的证书. 主要步骤: 安装 acme.sh 生成证书 copy 证书到 nginx/apache 或者其他服务 更新证书 更新 acme.sh 出错怎么办, 如何调试 下面详细介绍. 1. 安装 acme.sh安装很简单, 一个命令: 1curl https://get.acme.sh | sh 普通用户和 root 用户都可以安装使用. 安装过程进行了以下几步: 把 acme.sh 安装到你的 home 目录下: 1~/.acme.sh/ 并创建 一个 bash 的 alias, 方便你的使用: alias acme.sh=~/.acme.sh/acme.sh 2). 自动为你创建 cronjob, 每天 0:00 点自动检测所有的证书, 如果快过期了, 需要更新, 则会自动更新证书. 更高级的安装选项请参考: https://github.com/Neilpang/acme.sh/wiki/How-to-install 安装过程不会污染已有的系统任何功能和文件, 所有的修改都限制在安装目录中: ~/.acme.sh/ 2. 生成证书acme.sh 实现了 acme 协议支持的所有验证协议. 一般有两种方式验证: http 和 dns 验证. 1. http 方式需要在你的网站根目录下放置一个文件, 来验证你的域名所有权,完成验证. 然后就可以生成证书了.1acme.sh --issue -d mydomain.com -d www.mydomain.com --webroot /home/wwwroot/mydomain.com/ 只需要指定域名, 并指定域名所在的网站根目录. acme.sh 会全自动的生成验证文件, 并放到网站的根目录, 然后自动完成验证. 最后会聪明的删除验证文件. 整个过程没有任何副作用. 如果你用的 apache服务器, acme.sh 还可以智能的从 apache的配置中自动完成验证, 你不需要指定网站根目录: 1acme.sh --issue -d mydomain.com --apache 如果你用的 nginx服务器, 或者反代, acme.sh 还可以智能的从 nginx的配置中自动完成验证, 你不需要指定网站根目录: 1acme.sh --issue -d mydomain.com --nginx 注意, 无论是 apache 还是 nginx 模式, acme.sh在完成验证之后, 会恢复到之前的状态, 都不会私自更改你本身的配置. 好处是你不用担心配置被搞坏, 也有一个缺点, 你需要自己配置 ssl 的配置, 否则只能成功生成证书, 你的网站还是无法访问https. 但是为了安全, 你还是自己手动改配置吧. 如果你还没有运行任何 web 服务, 80 端口是空闲的, 那么 acme.sh 还能假装自己是一个webserver, 临时听在80 端口, 完成验证: 1acme.sh --issue -d mydomain.com --standalone 更高级的用法请参考: https://github.com/Neilpang/acme.sh/wiki/How-to-issue-a-cert 2. dns 方式, 在域名上添加一条 txt 解析记录, 验证域名所有权.这种方式的好处是, 你不需要任何服务器, 不需要任何公网 ip, 只需要 dns 的解析记录即可完成验证. 坏处是，如果不同时配置 Automatic DNS API，使用这种方式 acme.sh 将无法自动更新证书，每次都需要手动再次重新解析验证域名所有权。 1acme.sh --issue --dns -d mydomain.com 然后, acme.sh 会生成相应的解析记录显示出来, 你只需要在你的域名管理面板中添加这条 txt 记录即可. 等待解析完成之后, 重新生成证书: 1acme.sh --renew -d mydomain.com 注意第二次这里用的是 --renew dns 方式的真正强大之处在于可以使用域名解析商提供的 api 自动添加 txt 记录完成验证. acme.sh 目前支持 cloudflare, dnspod, cloudxns, godaddy 以及 ovh 等数十种解析商的自动集成. 以 dnspod 为例, 你需要先登录到 dnspod 账号, 生成你的 api id 和 api key, 都是免费的. 然后: 12345export DP_Id=&quot;1234&quot;export DP_Key=&quot;sADDsdasdgdsf&quot;acme.sh --issue --dns dns_dp -d aa.com -d www.aa.com 证书就会自动生成了. 这里给出的 api id 和 api key 会被自动记录下来, 将来你在使用 dnspod api 的时候, 就不需要再次指定了. 直接生成就好了: 1acme.sh --issue -d mydomain2.com --dns dns_dp 更详细的 api 用法: https://github.com/Neilpang/acme.sh/blob/master/dnsapi/README.md 3. copy/安装 证书前面证书生成以后, 接下来需要把证书 copy 到真正需要用它的地方. 注意, 默认生成的证书都放在安装目录下: ~/.acme.sh/, 请不要直接使用此目录下的文件, 例如: 不要直接让 nginx/apache 的配置文件使用这下面的文件. 这里面的文件都是内部使用, 而且目录结构可能会变化. 正确的使用方法是使用 --installcert 命令,并指定目标位置, 然后证书文件会被copy到相应的位置, 例如: 1234acme.sh --installcert -d &lt;domain&gt;.com \ --key-file /etc/nginx/ssl/&lt;domain&gt;.key \ --fullchain-file /etc/nginx/ssl/fullchain.cer \ --reloadcmd &quot;service nginx force-reload&quot; (一个小提醒, 这里用的是 service nginx force-reload, 不是 service nginx reload, 据测试, reload 并不会重新加载证书, 所以用的 force-reload) Nginx 的配置 ssl_certificate 使用 /etc/nginx/ssl/fullchain.cer ，而非 /etc/nginx/ssl/&lt;domain&gt;.cer ，否则 SSL Labs 的测试会报 Chain issues Incomplete 错误。 --installcert命令可以携带很多参数, 来指定目标文件. 并且可以指定 reloadcmd, 当证书更新以后, reloadcmd会被自动调用,让服务器生效. 详细参数请参考: https://github.com/Neilpang/acme.sh#3-install-the-issued-cert-to-apachenginx-etc 值得注意的是, 这里指定的所有参数都会被自动记录下来, 并在将来证书自动更新以后, 被再次自动调用. 4. 更新证书目前证书在 60 天以后会自动更新, 你无需任何操作. 今后有可能会缩短这个时间, 不过都是自动的, 你不用关心. 5. 更新 acme.sh目前由于 acme 协议和 letsencrypt CA 都在频繁的更新, 因此 acme.sh 也经常更新以保持同步. 升级 acme.sh 到最新版 : 1acme.sh --upgrade 如果你不想手动升级, 可以开启自动升级: 1acme.sh --upgrade --auto-upgrade 之后, acme.sh 就会自动保持更新了. 你也可以随时关闭自动更新: 1acme.sh --upgrade --auto-upgrade 0 How to use DNS APIIf your dns provider doesn’t provide api access, you can use our dns alias mode: https://github.com/Neilpang/acme.sh/wiki/DNS-alias-mode 1. Use CloudFlare domain API to automatically issue certFirst you need to login to your CloudFlare account to get your API key. 12export CF_Key=&quot;sdfsdfsdfljlbjkljlkjsdfoiwje&quot;export CF_Email=&quot;xxxx@sss.com&quot; Ok, let’s issue a cert now: 1acme.sh --issue --dns dns_cf -d example.com -d www.example.com The CF_Key and CF_Email will be saved in ~/.acme.sh/account.conf and will be reused when needed. 2. Use DNSPod.cn domain API to automatically issue certFirst you need to login to your DNSPod account to get your API Key and ID. 12export DP_Id=&quot;1234&quot;export DP_Key=&quot;sADDsdasdgdsf&quot; Ok, let’s issue a cert now: 1acme.sh --issue --dns dns_dp -d example.com -d www.example.com The DP_Id and DP_Key will be saved in ~/.acme.sh/account.conf and will be reused when needed. 3. Use CloudXNS.com domain API to automatically issue certFirst you need to login to your CloudXNS account to get your API Key and Secret. 12export CX_Key=&quot;1234&quot;export CX_Secret=&quot;sADDsdasdgdsf&quot; Ok, let’s issue a cert now: 1acme.sh --issue --dns dns_cx -d example.com -d www.example.com The CX_Key and CX_Secret will be saved in ~/.acme.sh/account.conf and will be reused when needed. 4. Use GoDaddy.com domain API to automatically issue certFirst you need to login to your GoDaddy account to get your API Key and Secret. https://developer.godaddy.com/keys/ Please create a Production key, instead of a Test key. 12export GD_Key=&quot;sdfsdfsdfljlbjkljlkjsdfoiwje&quot;export GD_Secret=&quot;asdfsdafdsfdsfdsfdsfdsafd&quot; Ok, let’s issue a cert now: 1acme.sh --issue --dns dns_gd -d example.com -d www.example.com The GD_Key and GD_Secret will be saved in ~/.acme.sh/account.conf and will be reused when needed. 5. Use PowerDNS embedded API to automatically issue certFirst you need to login to your PowerDNS account to enable the API and set your API-Token in the configuration. https://doc.powerdns.com/md/httpapi/README/ 1234export PDNS_Url=&quot;http://ns.example.com:8081&quot;export PDNS_ServerId=&quot;localhost&quot;export PDNS_Token=&quot;0123456789ABCDEF&quot;export PDNS_Ttl=60 Ok, let’s issue a cert now: 1acme.sh --issue --dns dns_pdns -d example.com -d www.example.com The PDNS_Url, PDNS_ServerId, PDNS_Token and PDNS_Ttl will be saved in ~/.acme.sh/account.conf and will be reused when needed. 6. Use OVH/kimsufi/soyoustart/runabove API to automatically issue certhttps://github.com/Neilpang/acme.sh/wiki/How-to-use-OVH-domain-api 7. Use nsupdate to automatically issue certFirst, generate a key for updating the zone 12345678b=$(dnssec-keygen -a hmac-sha512 -b 512 -n USER -K /tmp foo)cat &gt; /etc/named/keys/update.key &lt;&lt;EOFkey &quot;update&quot; &#123; algorithm hmac-sha512; secret &quot;$(awk &apos;/^Key/&#123;print $2&#125;&apos; /tmp/$b.private)&quot;;&#125;;EOFrm -f /tmp/$b.&#123;private,key&#125; Include this key in your named configuration 1include &quot;/etc/named/keys/update.key&quot;; Next, configure your zone to allow dynamic updates. Depending on your named version, use either 1234zone &quot;example.com&quot; &#123; type master; allow-update &#123; key &quot;update&quot;; &#125;;&#125;; or 123456zone &quot;example.com&quot; &#123; type master; update-policy &#123; grant update subdomain example.com.; &#125;;&#125; Finally, make the DNS server and update Key available to acme.sh 12export NSUPDATE_SERVER=&quot;dns.example.com&quot;export NSUPDATE_KEY=&quot;/path/to/your/nsupdate.key&quot; and optionally (depending on DNS server) 1export NSUPDATE_ZONE=&quot;example.com&quot; Ok, let’s issue a cert now: 1acme.sh --issue --dns dns_nsupdate -d example.com -d www.example.com The NSUPDATE_SERVER, NSUPDATE_KEY, and NSUPDATE_ZONE settings will be saved in ~/.acme.sh/account.conf and will be reused when needed. 8. Use LuaDNS domain APIGet your API token at https://api.luadns.com/settings 12export LUA_Key=&quot;sdfsdfsdfljlbjkljlkjsdfoiwje&quot;export LUA_Email=&quot;xxxx@sss.com&quot; To issue a cert: 1acme.sh --issue --dns dns_lua -d example.com -d www.example.com The LUA_Key and LUA_Email will be saved in ~/.acme.sh/account.conf and will be reused when needed. 9. Use DNSMadeEasy domain APIGet your API credentials at https://cp.dnsmadeeasy.com/account/info 12export ME_Key=&quot;sdfsdfsdfljlbjkljlkjsdfoiwje&quot;export ME_Secret=&quot;qdfqsdfkjdskfj&quot; To issue a cert: 1acme.sh --issue --dns dns_me -d example.com -d www.example.com The ME_Key and ME_Secret will be saved in ~/.acme.sh/account.conf and will be reused when needed. 10. Use Amazon Route53 domain APIhttps://github.com/Neilpang/acme.sh/wiki/How-to-use-Amazon-Route53-API 12export AWS_ACCESS_KEY_ID=XXXXXXXXXXexport AWS_SECRET_ACCESS_KEY=XXXXXXXXXXXXXXX To issue a cert: 1acme.sh --issue --dns dns_aws -d example.com -d www.example.com The AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY will be saved in ~/.acme.sh/account.conf and will be reused when needed. 11. Use Aliyun domain API to automatically issue certFirst you need to login to your Aliyun account to get your API key. https://ak-console.aliyun.com/#/accesskey 12export Ali_Key=&quot;sdfsdfsdfljlbjkljlkjsdfoiwje&quot;export Ali_Secret=&quot;jlsdflanljkljlfdsaklkjflsa&quot; Ok, let’s issue a cert now: 1acme.sh --issue --dns dns_ali -d example.com -d www.example.com The Ali_Key and Ali_Secret will be saved in ~/.acme.sh/account.conf and will be reused when needed. 12. Use ISPConfig 3.1 APIThis only works for ISPConfig 3.1 (and newer). Create a Remote User in the ISPConfig Control Panel. The Remote User must have access to at least DNS zone functions and DNS txt functions. 1234export ISPC_User=&quot;xxx&quot;export ISPC_Password=&quot;xxx&quot;export ISPC_Api=&quot;https://ispc.domain.tld:8080/remote/json.php&quot;export ISPC_Api_Insecure=1 If you have installed ISPConfig on a different port, then alter the 8080 accordingly. Leaver ISPC_Api_Insecure set to 1 if you have not a valid ssl cert for your installation. Change it to 0 if you have a valid ssl cert. To issue a cert: 1acme.sh --issue --dns dns_ispconfig -d example.com -d www.example.com The ISPC_User, ISPC_Password, ISPC_Apiand ISPC_Api_Insecure will be saved in ~/.acme.sh/account.conf and will be reused when needed. 13. Use Alwaysdata domain APIFirst you need to login to your Alwaysdata account to get your API Key. 1export AD_API_KEY=&quot;myalwaysdataapikey&quot; Ok, let’s issue a cert now: 1acme.sh --issue --dns dns_ad -d example.com -d www.example.com The AD_API_KEY will be saved in ~/.acme.sh/account.conf and will be reused when needed. 14. Use Linode domain APIThe tokens created in the classic manager and cloud manager are incompatible with one another. While the classic manager makes an all or nothing API, the newer cloud manager interface promises to produce API keys with a finer permission system. However, either way works just fine. Classic ManagerClassic Manager: https://manager.linode.com/profile/api First you need to login to your Linode account to get your API Key. Then add an API key with label ACME and copy the new key into the following command. 1export LINODE_API_KEY=&quot;...&quot; Due to the reload time of any changes in the DNS records, we have to use the dnssleep option to wait at least 15 minutes for the changes to take effect. Ok, let’s issue a cert now: 1acme.sh --issue --dns dns_linode --dnssleep 900 -d example.com -d www.example.com The LINODE_API_KEY will be saved in ~/.acme.sh/account.conf and will be reused when needed. Cloud ManagerCloud Manager: https://cloud.linode.com/profile/tokens First you need to login to your Linode account to get your API Key. Click on “Add a Personal Access Token”. Give the new key a “Label” (we recommend ACME) Give it Read/Write access to “Domains” “Submit” and copy the new key into the LINODE_V4_API_KEY command below. 1export LINODE_V4_API_KEY=&quot;...&quot; Due to the reload time of any changes in the DNS records, we have to use the dnssleep option to wait at least 15 minutes for the changes to take effect. Ok, let’s issue a cert now: 1acme.sh --issue --dns dns_linode_v4 --dnssleep 900 -d example.com -d www.example.com The LINODE_V4_API_KEY will be saved in ~/.acme.sh/account.conf and will be reused when needed. 15. Use FreeDNSFreeDNS (https://freedns.afraid.org/) does not provide an API to update DNS records (other than IPv4 and IPv6 dynamic DNS addresses). The acme.sh plugin therefore retrieves and updates domain TXT records by logging into the FreeDNS website to read the HTML and posting updates as HTTP. The plugin needs to know your userid and password for the FreeDNS website. 12export FREEDNS_User=&quot;...&quot;export FREEDNS_Password=&quot;...&quot; You need only provide this the first time you run the acme.sh client with FreeDNS validation and then again whenever you change your password at the FreeDNS site. The acme.sh FreeDNS plugin does not store your userid or password but rather saves an authentication token returned by FreeDNS in ~/.acme.sh/account.conf and reuses that when needed. Now you can issue a certificate. 1acme.sh --issue --dns dns_freedns -d example.com -d www.example.com Note that you cannot use acme.sh automatic DNS validation for FreeDNS public domains or for a subdomain that you create under a FreeDNS public domain. You must own the top level domain in order to automatically validate with acme.sh at FreeDNS. 16. Use cyon.chYou only need to set your cyon.ch login credentials. If you also have 2 Factor Authentication (OTP) enabled, you need to set your secret token too and have oathtool installed. 123export CY_Username=&quot;your_cyon_username&quot;export CY_Password=&quot;your_cyon_password&quot;export CY_OTP_Secret=&quot;your_otp_secret&quot; # Only required if using 2FA To issue a cert: 1acme.sh --issue --dns dns_cyon -d example.com -d www.example.com The CY_Username, CY_Password and CY_OTP_Secret will be saved in ~/.acme.sh/account.conf and will be reused when needed. 17. Use Domain-Offensive/Resellerinterface/Domainrobot APIATTENTION: You need to be a registered Reseller to be able to use the ResellerInterface. As a normal user you can not use this method. You will need your login credentials (Partner ID+Password) to the Resellerinterface, and export them before you run acme.sh: 12export DO_PID=&quot;KD-1234567&quot;export DO_PW=&quot;cdfkjl3n2&quot; Ok, let’s issue a cert now: 1acme.sh --issue --dns dns_do -d example.com -d www.example.com 18. Use Gandi LiveDNS APIYou must enable the new Gandi LiveDNS API first and the create your api key, See: http://doc.livedns.gandi.net/ 1export GANDI_LIVEDNS_KEY=&quot;fdmlfsdklmfdkmqsdfk&quot; Ok, let’s issue a cert now: 1acme.sh --issue --dns dns_gandi_livedns -d example.com -d www.example.com 19. Use Knot (knsupdate) DNS API to automatically issue certFirst, generate a TSIG key for updating the zone. 1keymgr tsig generate -t acme_key hmac-sha512 &gt; /etc/knot/acme.key Include this key in your knot configuration file. 1include: /etc/knot/acme.key Next, configure your zone to allow dynamic updates. Dynamic updates for the zone are allowed via proper ACL rule with the update action. For in-depth instructions, please see Knot DNS’s documentation. 12345678910acl: - id: acme_acl address: 192.168.1.0/24 key: acme_key action: updatezone: - domain: example.com file: example.com.zone acl: acme_acl Finally, make the DNS server and TSIG Key available to acme.sh 12export KNOT_SERVER=&quot;dns.example.com&quot;export KNOT_KEY=`grep \# /etc/knot/acme.key | cut -d&apos; &apos; -f2` Ok, let’s issue a cert now: 1acme.sh --issue --dns dns_knot -d example.com -d www.example.com The KNOT_SERVER and KNOT_KEY settings will be saved in ~/.acme.sh/account.conf and will be reused when needed. 20. Use DigitalOcean API (native)You need to obtain a read and write capable API key from your DigitalOcean account. See: https://www.digitalocean.com/help/api/ 1export DO_API_KEY=&quot;75310dc4ca779ac39a19f6355db573b49ce92ae126553ebd61ac3a3ae34834cc&quot; Ok, let’s issue a cert now: 1acme.sh --issue --dns dns_dgon -d example.com -d www.example.com 21. Use ClouDNS.net APIYou need to set the HTTP API user ID and password credentials. See: https://www.cloudns.net/wiki/article/42/. For security reasons, it’s recommended to use a sub user ID that only has access to the necessary zones, as a regular API user has access to your entire account. 12345# Use this for a sub auth IDexport CLOUDNS_SUB_AUTH_ID=XXXXX# Use this for a regular auth ID#export CLOUDNS_AUTH_ID=XXXXXexport CLOUDNS_AUTH_PASSWORD=&quot;YYYYYYYYY&quot; Ok, let’s issue a cert now: 1acme.sh --issue --dns dns_cloudns -d example.com -d www.example.com The CLOUDNS_AUTH_ID and CLOUDNS_AUTH_PASSWORD will be saved in ~/.acme.sh/account.confand will be reused when needed. 22. Use Infoblox APIFirst you need to create/obtain API credentials on your Infoblox appliance. 12export Infoblox_Creds=&quot;username:password&quot;export Infoblox_Server=&quot;ip or fqdn of infoblox appliance&quot; Ok, let’s issue a cert now: 1acme.sh --issue --dns dns_infoblox -d example.com -d www.example.com Note: This script will automatically create and delete the ephemeral txt record. The Infoblox_Creds and Infoblox_Server will be saved in ~/.acme.sh/account.conf and will be reused when needed. 23. Use VSCALE APIFirst you need to create/obtain API tokens on your settings panel. 1export VSCALE_API_KEY=&quot;sdfsdfsdfljlbjkljlkjsdfoiwje&quot; Ok, let’s issue a cert now: 1acme.sh --issue --dns dns_vscale -d example.com -d www.example.com 24. Use Dynu APIFirst you need to create/obtain API credentials from your Dynu account. See: https://www.dynu.com/resources/api/documentation 12export Dynu_ClientId=&quot;xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx&quot;export Dynu_Secret=&quot;yyyyyyyyyyyyyyyyyyyyyyyyy&quot; Ok, let’s issue a cert now: 1acme.sh --issue --dns dns_dynu -d example.com -d www.example.com The Dynu_ClientId and Dynu_Secret will be saved in ~/.acme.sh/account.conf and will be reused when needed. 25. Use DNSimple APIFirst you need to login to your DNSimple account and generate a new oauth token. https://dnsimple.com/a/{your account id}/account/access_tokens Note that this is an account token and not a user token. The account token is needed to infer the account_id used in requests. A user token will not be able to determine the correct account to use. 1export DNSimple_OAUTH_TOKEN=&quot;sdfsdfsdfljlbjkljlkjsdfoiwje&quot; To issue the cert just specify the dns_dnsimple API. 1acme.sh --issue --dns dns_dnsimple -d example.com The DNSimple_OAUTH_TOKEN will be saved in ~/.acme.sh/account.conf and will be reused when needed. If you have any issues with this integration please report them tohttps://github.com/pho3nixf1re/acme.sh/issues. 26. Use NS1.com API1export NS1_Key=&quot;fdmlfsdklmfdkmqsdfk&quot; Ok, let’s issue a cert now: 1acme.sh --issue --dns dns_nsone -d example.com -d www.example.com 27. Use DuckDNS.org API1export DuckDNS_Token=&quot;aaaaaaaa-bbbb-cccc-dddd-eeeeeeeeeeee&quot; Please note that since DuckDNS uses StartSSL as their cert provider, thus –insecure may need to be used when issuing certs: 1acme.sh --insecure --issue --dns dns_duckdns -d mydomain.duckdns.org For issues, please report to https://github.com/raidenii/acme.sh/issues. 28. Use Name.com APICreate your API token here: https://www.name.com/account/settings/api Note: Namecom_Username should be your Name.com username and not the token name. If you accidentally run the script with the token name as the username see ~/.acme.sh/account.confto fix the issue 12export Namecom_Username=&quot;testuser&quot;export Namecom_Token=&quot;xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx&quot; And now you can issue certs with: 1acme.sh --issue --dns dns_namecom -d example.com -d www.example.com For issues, please report to https://github.com/raidenii/acme.sh/issues. 29. Use Dyn Managed DNS API to automatically issue certFirst, login to your Dyn Managed DNS account: https://portal.dynect.net/login/ It is recommended to add a new user specific for API access. The minimum “Zones &amp; Records Permissions” required are: 12345678RecordAddRecordUpdateRecordDeleteRecordGetZoneGetZoneAddNodeZoneRemoveNodeZonePublish Pass the API user credentials to the environment: 123export DYN_Customer=&quot;customer&quot;export DYN_Username=&quot;apiuser&quot;export DYN_Password=&quot;secret&quot; Ok, let’s issue a cert now: 1acme.sh --issue --dns dns_dyn -d example.com -d www.example.com The DYN_Customer, DYN_Username and DYN_Password will be saved in ~/.acme.sh/account.conf and will be reused when needed. 30. Use pdd.yandex.ru API1export PDD_Token=&quot;xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx&quot; Follow these instructions to get the token for your domain https://tech.yandex.com/domain/doc/concepts/access-docpage/ 1acme.sh --issue --dns dns_yandex -d mydomain.example.org For issues, please report to https://github.com/non7top/acme.sh/issues. 31. Use Hurricane ElectricHurricane Electric (https://dns.he.net/) doesn’t have an API so just set your login credentials like so: 12export HE_Username=&quot;yourusername&quot;export HE_Password=&quot;password&quot; Then you can issue your certificate: 1acme.sh --issue --dns dns_he -d example.com -d www.example.com The HE_Username and HE_Password settings will be saved in ~/.acme.sh/account.conf and will be reused when needed. Please report any issues to https://github.com/angel333/acme.sh or to me@ondrejsimek.com. 32. Use UnoEuro API to automatically issue certFirst you need to login to your UnoEuro account to get your API key. 12export UNO_Key=&quot;sdfsdfsdfljlbjkljlkjsdfoiwje&quot;export UNO_User=&quot;UExxxxxx&quot; Ok, let’s issue a cert now: 1acme.sh --issue --dns dns_unoeuro -d example.com -d www.example.com The UNO_Key and UNO_User will be saved in ~/.acme.sh/account.conf and will be reused when needed. 33. Use INWXINWX offers an xmlrpc api with your standard login credentials, set them like so: 12export INWX_User=&quot;yourusername&quot;export INWX_Password=&quot;password&quot; Then you can issue your certificates with: 1acme.sh --issue --dns dns_inwx -d example.com -d www.example.com The INWX_User and INWX_Password settings will be saved in ~/.acme.sh/account.conf and will be reused when needed. If your account is secured by mobile tan you have also defined the shared secret. 1export INWX_Shared_Secret=&quot;shared secret&quot; You may need to re-enable the mobile tan to gain the shared secret. 34. User Servercow API v1Create a new user from the servercow control center. Don’t forget to activate DNS API for this user. 12export SERVERCOW_API_Username=usernameexport SERVERCOW_API_Password=password Now you cann issue a cert: 1acme.sh --issue --dns dns_servercow -d example.com -d www.example.com Both, SERVERCOW_API_Username and SERVERCOW_API_Password will be saved in ~/.acme.sh/account.conf and will be reused when needed. 35. Use Namesilo.com APIYou’ll need to generate an API key at https://www.namesilo.com/account_api.php Optionally you may restrict the access to an IP range there. 1export Namesilo_Key=&quot;xxxxxxxxxxxxxxxxxxxxxxxx&quot; And now you can issue certs with: 1acme.sh --issue --dns dns_namesilo --dnssleep 900 -d example.com -d www.example.com 36. Use autoDNS (InternetX)InternetX offers an xml api with your standard login credentials, set them like so: 123export AUTODNS_USER=&quot;yourusername&quot;export AUTODNS_PASSWORD=&quot;password&quot;export AUTODNS_CONTEXT=&quot;context&quot; Then you can issue your certificates with: 1acme.sh --issue --dns dns_autodns -d example.com -d www.example.com The AUTODNS_USER, AUTODNS_PASSWORD and AUTODNS_CONTEXT settings will be saved in ~/.acme.sh/account.conf and will be reused when needed. 37. Use Azure DNSYou have to create a service principal first. See:How to use Azure DNS 1234export AZUREDNS_SUBSCRIPTIONID=&quot;12345678-9abc-def0-1234-567890abcdef&quot;export AZUREDNS_TENANTID=&quot;11111111-2222-3333-4444-555555555555&quot;export AZUREDNS_APPID=&quot;3b5033b5-7a66-43a5-b3b9-a36b9e7c25ed&quot;export AZUREDNS_CLIENTSECRET=&quot;1b0224ef-34d4-5af9-110f-77f527d561bd&quot; Then you can issue your certificates with: 1acme.sh --issue --dns dns_azure -d example.com -d www.example.com AZUREDNS_SUBSCRIPTIONID, AZUREDNS_TENANTID,AZUREDNS_APPID and AZUREDNS_CLIENTSECRETsettings will be saved in ~/.acme.sh/account.conf and will be reused when needed. 38. Use selectel.com(selectel.ru) domain API to automatically issue certFirst you need to login to your account to get your API key from: https://my.selectel.ru/profile/apikeys. 1export SL_Key=&quot;sdfsdfsdfljlbjkljlkjsdfoiwje&quot; Ok, let’s issue a cert now: 1acme.sh --issue --dns dns_selectel -d example.com -d www.example.com The SL_Key will be saved in ~/.acme.sh/account.conf and will be reused when needed. 39. Use zonomi.com domain API to automatically issue certFirst you need to login to your account to find your API key from: http://zonomi.com/app/dns/dyndns.jsp Your will find your api key in the example urls: 12https://zonomi.com/app/dns/dyndns.jsp?host=example.com&amp;api_key=1063364558943540954358668888888888export ZM_Key=&quot;1063364558943540954358668888888888&quot; Ok, let’s issue a cert now: 1acme.sh --issue --dns dns_zonomi -d example.com -d www.example.com The ZM_Key will be saved in ~/.acme.sh/account.conf and will be reused when needed. 40. Use DreamHost DNS APIDNS API keys may be created at https://panel.dreamhost.com/?tree=home.api. Ensure the created key has add and remove privelages. 12export DH_API_KEY=&quot;&lt;api key&gt;&quot;acme.sh --issue --dns dns_dreamhost -d example.com -d www.example.com The ‘DH_API_KEY’ will be saved in ~/.acme.sh/account.conf and will be reused when needed. 41. Use DirectAdmin APIThe DirectAdmin interface has it’s own Let’s encrypt functionality, but this script can be used to generate certificates for names which are not hosted on DirectAdmin User must provide login data and URL to the DirectAdmin incl. port. You can create an user which only has access to CMD_API_DNS_CONTROL CMD_API_SHOW_DOMAINS By using the Login Keys function. See also https://www.directadmin.com/api.php and https://www.directadmin.com/features.php?id=1298 12export DA_Api=&quot;https://remoteUser:remotePassword@da.domain.tld:8443&quot;export DA_Api_Insecure=1 Set DA_Api_Insecure to 1 for insecure and 0 for secure -&gt; difference is whether ssl cert is checked for validity (0) or whether it is just accepted (1) Ok, let’s issue a cert now: 1acme.sh --issue --dns dns_da -d example.com -d www.example.com The DA_Api and DA_Api_Insecure will be saved in ~/.acme.sh/account.conf and will be reused when needed. 42. Use KingHost DNS APIAPI access must be enabled at https://painel.kinghost.com.br/painel.api.php 123export KINGHOST_Username=&quot;yourusername&quot;export KINGHOST_Password=&quot;yourpassword&quot;acme.sh --issue --dns dns_kinghost -d example.com -d *.example.com The KINGHOST_username and KINGHOST_Password will be saved in ~/.acme.sh/account.confand will be reused when needed. 43. Use Zilore DNS APIFirst, get your API key at https://my.zilore.com/account/api 1export Zilore_Key=&quot;5dcad3a2-36cb-50e8-cb92-000002f9&quot; Ok, let’s issue a cert now: 1acme.sh --issue --dns dns_zilore -d example.com -d *.example.com The Zilore_Key will be saved in ~/.acme.sh/account.conf and will be reused when needed. 44. Use Loopia.se APIUser must provide login credentials to the Loopia API. The user needs the following permissions: getDomains getSubdomains addSubdomain removeSubdomain getZoneRecords addZoneRecord Set the login credentials: 12export LOOPIA_User=&quot;user@loopiaapi&quot;export LOOPIA_Password=&quot;password&quot; And to issue a cert: 1acme.sh --issue --dns dns_loopia -d example.com -d *.example.com The username and password will be saved in ~/.acme.sh/account.conf and will be reused when needed. 45. Use ACME DNS APIACME DNS is a limited DNS server with RESTful HTTP API to handle ACME DNS challenges easily and securely. https://github.com/joohoi/acme-dns 123456export ACMEDNS_UPDATE_URL=&quot;https://auth.acme-dns.io/update&quot;export ACMEDNS_USERNAME=&quot;&lt;username&gt;&quot;export ACMEDNS_PASSWORD=&quot;&lt;password&gt;&quot;export ACMEDNS_SUBDOMAIN=&quot;&lt;subdomain&gt;&quot;acme.sh --issue --dns dns_acmedns -d example.com -d www.example.com The credentials will be saved in ~/.acme.sh/account.conf and will be reused when needed. 46. Use TELE3 APIFirst you need to login to your TELE3 account to set your API-KEY. https://www.tele3.cz/system-acme-api.html 1234export TELE3_Key=&quot;MS2I4uPPaI...&quot;export TELE3_Secret=&quot;kjhOIHGJKHg&quot;acme.sh --issue --dns dns_tele3 -d example.com -d *.example.com The TELE3_Key and TELE3_Secret will be saved in ~/.acme.sh/account.conf and will be reused when needed. 47. Use Euserv.eu APIFirst you need to login to your euserv.eu account and activate your API Administration (API Verwaltung). https://support.euserv.com Once you’ve activate, login to your API Admin Interface and create an API account. Please specify the scope (active groups: domain) and assign the allowed IPs. 12export EUSERV_Username=&quot;99999.user123&quot;export EUSERV_Password=&quot;Asbe54gHde&quot; Ok, let’s issue a cert now: (Be aware to use the --insecure flag, cause euserv.eu is still using self-signed certificates!) 1acme.sh --issue --dns dns_euserv -d example.com -d *.example.com --insecure The EUSERV_Username and EUSERV_Password will be saved in ~/.acme.sh/account.conf and will be reused when needed. Please report any issues to https://github.com/initit/acme.sh or to github@initit.de 48. Use DNSPod.com domain API to automatically issue certFirst you need to get your API Key and ID by this get-the-user-token. 12export DPI_Id=&quot;1234&quot;export DPI_Key=&quot;sADDsdasdgdsf&quot; Ok, let’s issue a cert now: 1acme.sh --issue --dns dns_dpi -d example.com -d www.example.com The DPI_Id and DPI_Key will be saved in ~/.acme.sh/account.conf and will be reused when needed. 49. Use Google Cloud DNS API to automatically issue certFirst you need to authenticate to gcloud. 1gcloud init The dns_gcloud script uses the active gcloud configuration and credentials. There is no logic inside dns_gcloud to override the project and other settings. If needed, create additional gcloud configurations. You can change the configuration being used without activating it; simply set the CLOUDSDK_ACTIVE_CONFIG_NAME environment variable. To issue a certificate you can: 12export CLOUDSDK_ACTIVE_CONFIG_NAME=default # see the note aboveacme.sh --issue --dns dns_gcloud -d example.com -d &apos;*.example.com&apos; dns_gcloud also supports DNS alias mode. 50. Use ConoHa APIFirst you need to login to your ConoHa account to get your API credentials. 1234export CONOHA_Username=&quot;xxxxxx&quot;export CONOHA_Password=&quot;xxxxxx&quot;export CONOHA_TenantId=&quot;xxxxxx&quot;export CONOHA_IdentityServiceApi=&quot;https://identity.xxxx.conoha.io/v2.0&quot; To issue a cert: 1acme.sh --issue --dns dns_conoha -d example.com -d www.example.com The CONOHA_Username, CONOHA_Password, CONOHA_TenantId and CONOHA_IdentityServiceApiwill be saved in ~/.acme.sh/account.conf and will be reused when needed. 51. Use netcup DNS API to automatically issue certFirst you need to login in your CCP account to get your API Key and API Password. 123export NC_Apikey=&quot;&lt;Apikey&gt;&quot;export NC_Apipw=&quot;&lt;Apipassword&gt;&quot;export NC_CID=&quot;&lt;Customernumber&gt;&quot; Now, let’s issue a cert: 1acme.sh --issue --dns dns_netcup -d example.com -d www.example.com The NC_Apikey,NC_Apipw and NC_CID will be saved in ~/.acme.sh/account.conf and will be reused when needed. 52. Use GratisDNS.dkGratisDNS.dk (https://gratisdns.dk/) does not provide an API to update DNS records (other than IPv4 and IPv6 dynamic DNS addresses). The acme.sh plugin therefore retrieves and updates domain TXT records by logging into the GratisDNS website to read the HTML and posting updates as HTTP. The plugin needs to know your userid and password for the GratisDNS website. 12export GDNSDK_Username=&quot;...&quot;export GDNSDK_Password=&quot;...&quot; The username and password will be saved in ~/.acme.sh/account.conf and will be reused when needed. Now you can issue a certificate. Note: It usually takes a few minutes (usually 3-4 minutes) before the changes propagates to gratisdns.dk nameservers (ns3.gratisdns.dk often are slow), and in rare cases I have seen over 5 minutes before google DNS catches it. Therefor a DNS sleep of at least 300 seconds are recommended- 1acme.sh --issue --dns dns_gdnsdk --dnssleep 300 -d example.com -d *.example.com 53. Use NamecheapYou will need your namecheap username, API KEY (https://www.namecheap.com/support/api/intro.aspx) and your external IP address (or an URL to get it), this IP will need to be whitelisted at Namecheap. Due to Namecheap’s API limitation all the records of your domain will be read and re applied, make sure to have a backup of your records you could apply if any issue would arise. 123export NAMECHEAP_USERNAME=&quot;...&quot;export NAMECHEAP_API_KEY=&quot;...&quot;export NAMECHEAP_SOURCEIP=&quot;...&quot; NAMECHEAP_SOURCEIP can either be an IP address or an URL to provide it (e.g. https://ifconfig.co/ip). The username and password will be saved in ~/.acme.sh/account.conf and will be reused when needed. Now you can issue a certificate. 1acme.sh --issue --dns dns_namecheap -d example.com -d *.example.com 54. Use MyDNS.JP APIFirst, register to MyDNS.JP and get MasterID and Password. 12export MYDNSJP_MasterID=MasterIDexport MYDNSJP_Password=Password To issue a certificate: 1acme.sh --issue --dns dns_mydnsjp -d example.com -d www.example.com The MYDNSJP_MasterID and MYDNSJP_Password will be saved in ~/.acme.sh/account.conf and will be reused when needed. 55. Use hosting.de APICreate an API key in your hosting.de account here: https://secure.hosting.de The key needs the following rights: DNS_ZONES_EDIT DNS_ZONES_LIST Set your API Key and endpoint: 12export HOSTINGDE_APIKEY=&apos;xxx&apos;export HOSTINGDE_ENDPOINT=&apos;https://secure.hosting.de&apos; The plugin can also be used for the http.net API. http.net customers have to set endpoint to https://partner.http.net. Ok, let’s issue a cert now: 1acme.sh --issue --dns dns_hostingde -d example.com -d *.example.com The hosting.de API key and endpoint will be saved in ~/.acme.sh/account.conf and will be reused when needed. 56. Use Neodigit.net API1export NEODIGIT_API_TOKEN=&quot;eXJxTkdUVUZmcHQ3QWJackQ4ZGlMejRDSklRYmo5VG5zcFFKK2thYnE0WnVnNnMy&quot; Ok, let’s issue a cert now: 1acme.sh --issue --dns dns_neodigit -d example.com -d www.example.com Neodigit API Token will be saved in ~/.acme.sh/account.conf and will be used when needed. 57. Use Exoscale APICreate an API key and secret key in the Exoscale account section Set your API and secret key: 12export EXOSCALE_API_KEY=&apos;xxx&apos;export EXOSCALE_SECRET_KEY=&apos;xxx&apos; Now, let’s issue a cert: 1acme.sh --issue --dns dns_exoscale -d example.com -d www.example.com The EXOSCALE_API_KEY and EXOSCALE_SECRET_KEY will be saved in ~/.acme.sh/account.confand will be reused when needed. 58. Using PointHQ API to issue certsLog into PointHQ account management and copy the API key from the page there. 1exportPointHQ_Email=&quot;accountemail@yourdomain.com&quot; You can then issue certs by using: 12 59. Use Active24 APICreate an API token in the Active24 account section, documentation on https://faq.active24.com/cz/790131-REST-API-rozhran%C3%AD. Set your API token: 1export ACTIVE24_Token=&apos;xxx&apos; Now, let’s issue a cert, set dnssleep for propagation new DNS record: 1acme.sh --issue --dns dns_active24 -d example.com -d www.example.com --dnssleep 1000 The ACTIVE24_Token will be saved in ~/.acme.sh/account.conf and will be reused when needed. 60. Use do.de APICreate an API token in your do.de account (Create token here | Documentation). Set your API token: 1export DO_LETOKEN=&apos;FmD408PdqT1E269gUK57&apos; To issue a certificate run: 1acme.sh --issue --dns dns_doapi -d example.com -d *.example.com The API token will be saved in ~/.acme.sh/account.conf and will be reused when needed. 61. Use Nexcess APIFirst, you’ll need to login to the Nexcess.net Client Portal and generate a new API token. Once you have a token, set it in your systems environment: 12export NW_API_TOKEN=&quot;YOUR_TOKEN_HERE&quot;export NW_API_ENDPOINT=&quot;https://portal.nexcess.net&quot; Finally, we’ll issue the certificate: (Nexcess DNS publishes at max every 15 minutes, we recommend setting a 900 second --dnssleep) 1acme.sh --issue --dns dns_nw -d example.com --dnssleep 900 The NW_API_TOKEN and NW_API_ENDPOINT will be saved in ~/.acme.sh/account.conf and will be reused when needed. 62. Use Thermo.io APIFirst, you’ll need to login to the Thermo.io Client Portal and generate a new API token. Once you have a token, set it in your systems environment: 12export NW_API_TOKEN=&quot;YOUR_TOKEN_HERE&quot;export NW_API_ENDPOINT=&quot;https://core.thermo.io&quot; Finally, we’ll issue the certificate: (Thermo DNS publishes at max every 15 minutes, we recommend setting a 900 second --dnssleep) 1acme.sh --issue --dns dns_nw -d example.com --dnssleep 900 The NW_API_TOKEN and NW_API_ENDPOINT will be saved in ~/.acme.sh/account.conf and will be reused when needed. 63. Use Futurehosting APIFirst, you’ll need to login to the Futurehosting Client Portal and generate a new API token. Once you have a token, set it in your systems environment: 12export NW_API_TOKEN=&quot;YOUR_TOKEN_HERE&quot;export NW_API_ENDPOINT=&quot;https://my.futurehosting.com&quot; Finally, we’ll issue the certificate: (Futurehosting DNS publishes at max every 15 minutes, we recommend setting a 900 second --dnssleep) 1acme.sh --issue --dns dns_nw -d example.com --dnssleep 900 The NW_API_TOKEN and NW_API_ENDPOINT will be saved in ~/.acme.sh/account.conf and will be reused when needed. 64. Use Rackspace APISet username and API key, which is available under “My Profile &amp; Settings” 12export RACKSPACE_Username=&apos;username&apos;export RACKSPACE_Apikey=&apos;xxx&apos; Now, let’s issue a cert: 1acme.sh --issue --dns dns_rackspace -d example.com -d www.example.com 65. Use Online APIFirst, you’ll need to retrive your API key, which is available under https://console.online.net/en/api/access 1export ONLINE_API_KEY=&apos;xxx&apos; To issue a cert run: 1acme.sh --issue --dns dns_online -d example.com -d www.example.com ONLINE_API_KEY will be saved in ~/.acme.sh/account.conf and will be reused when needed. 66. Use MyDevil.netMake sure that you can execute own binaries: 1devil binexec on Install acme.sh, or simply git clone it into some directory on your MyDevil host account (in which case you should link to it from your ~/bin directory). If you’re not using private IP and depend on default IP provided by host, you may want to edit crontab too, and make sure that acme.sh --cron is run also after reboot (you can find out how to do that on their wiki pages). To issue a new certificate, run: 1acme.sh --issue --dns dns_mydevil -d example.com -d *.example.com After certificate is ready, you can install it with deploy command. 67. Use Core-Networks API to automatically issue certFirst you need to login to your Core-Networks account to to set up an API-User. Then export username and password to use these credentials. 12export CN_User=&quot;user&quot;export CN_Password=&quot;passowrd&quot; Ok, let’s issue a cert now: 1acme.sh --issue --dns dns_cn -d example.com -d www.example.com The CN_User and CN_Password will be saved in ~/.acme.sh/account.conf and will be reused when needed. 68. Use NederHost APICreate an API token in Mijn NederHost. Set your API key: 1export NederHost_Key=&apos;xxx&apos; To issue a certificate run: 1acme.sh --issue --dns dns_nederhost -d example.com -d *.example.com 69. Use Zone.ee DNS APIFirst, you’ll need to retrive your API key. Estonian insructions https://help.zone.eu/kb/zoneid-api-v2/ 12export ZONE_Username=yourusernameexport ZONE_Key=keygoeshere To issue a cert run: 1acme.sh --issue -d example.com -d www.example.com --dns dns_zone ZONE_Username and ZONE_Key will be saved in ~/.acme.sh/account.conf and will be reused when needed. 70. Use UltraDNS APIUltraDNS is a paid for service that provides DNS, as well as Web and Mail forwarding (as well as reporting, auditing, and advanced tools). More information can be found here: https://www.security.neustar/lp/ultra20/index.html The REST API documentation for this service is found here: https://portal.ultradns.com/static/docs/REST-API_User_Guide.pdf Set your UltraDNS User name, and password; these would be the same you would use here: https://portal.ultradns.com/ - or if you create an API only user, that username and password would be better utilized. 123456export ULTRA_USR=&quot;abcd&quot;export ULTRA_PWD=&quot;efgh&quot;To issue a cert run:acme.sh --issue --dns dns_ultra -d example.com -d www.example.com ULTRA_USR and ULTRA_PWD will be saved in ~/.acme.sh/account.conf and will be resued when needed. 71. Use deSEC.ioSign up for dynDNS at https://desec.io first. Set your API token (password) and domain (username) from your email sent by desec.io 12export DEDYN_TOKEN=d41d8cd98f00b204e9800998ecf8427eexport DEDYN_NAME=foobar.dedyn.io To issue a certificate run: 1acme.sh --issue --dns dns_desec -d foobar.dedyn.io -d *.foobar.dedyn.io 72. Use OpenProvider APIFirst, you need to enable API access and retrieve your password hash on https://rcp.openprovider.eu/account/dashboard.php 1234export OPENPROVIDER_USER=&apos;username&apos;export OPENPROVIDER_PASSWORDHASH=&apos;xxx&apos;acme.sh --issue --dns dns_openprovider -d example.com -d www.example.com OPENPROVIDER_USER and OPENPROVIDER_PASSWORDHASH will be saved in ~/.acme.sh/account.conf and will be reused when needed.]]></content>
      <categories>
        <category>笔记</category>
      </categories>
      <tags>
        <tag>SSL证书</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[时间戳转换工具]]></title>
    <url>%2F2019%2F03%2F13%2F%E6%97%B6%E9%97%B4%E6%88%B3%E8%BD%AC%E6%8D%A2%E5%B7%A5%E5%85%B7%2F</url>
    <content type="text"><![CDATA[当前时间: 时间戳: 时间戳： 时间： 时 间 ： 时间戳： 更多的实用小玩意请查看在线工具 input.mainInput{border: 1px solid #ccc;padding: 7px 0px;border-radius: 3px;padding-left:5px;-webkit-box-shadow: inset 0 1px 1px rgba(0,0,0,.075);box-shadow: inset 0 1px 1px rgba(0,0,0,.075);-webkit-transition: border-color ease-in-out .15s,-webkit-box-shadow ease-in-out .15s;-o-transition: border-color ease-in-out .15s,box-shadow ease-in-out .15s;transition: border-color ease-in-out .15s,box-shadow ease-in-out .15s}input.mainInput:focus{border-color: #66afe9;outline: 0;-webkit-box-shadow: inset 0 1px 1px rgba(0,0,0,.075),0 0 8px rgba(102,175,233,.6);box-shadow: inset 0 1px 1px rgba(0,0,0,.075),0 0 8px rgba(102,175,233,.6)} function padding(num, length) { for (var len = (num + "").length; len < length; len = num.length) { num = "0" + num; } return num; } function ts2str(date) { Y = date.getFullYear() + '-'; M = padding(date.getMonth() + 1, 2) + '-'; D = padding(date.getDate(), 2) + ' '; h = padding(date.getHours(), 2) + ':'; m = padding(date.getMinutes(), 2) + ':'; s = padding(date.getSeconds(), 2); return Y + M + D + h + m + s; } function str2ts(str) { var date = new Date(str); return date.getTime(); } var timer; function start() { document.getElementById("start").style.display = "none"; document.getElementById("stop").style.display = ""; timer=setInterval(function(){var date=new Date();var clock=document.getElementById("clock");clock.innerHTML=ts2str(date);var ts_clock=document.getElementById("ts_clock");ts_clock.innerHTML=date.getTime();},1000);} start(); function stop() { document.getElementById("stop").style.display = "none"; document.getElementById("start").style.display = ""; clearInterval(timer);}function clickts2str(){var inputTs=document.getElementById("inputTs");var str=ts2str(new Date(parseInt(inputTs.value)));document.getElementById("outputStr").value=str; } function clickstr2ts() { var inputStr = document.getElementById("inputStr"); var ts = str2ts(inputStr.value); document.getElementById("outputTs").value = ts; }]]></content>
      <categories>
        <category>在线工具</category>
      </categories>
      <tags>
        <tag>时间戳</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[http状态码]]></title>
    <url>%2F2019%2F03%2F10%2Fhttp%E7%8A%B6%E6%80%81%E7%A0%81%2F</url>
    <content type="text"><![CDATA[100 ~ 199——信息状态码 状态码 原因短语 含义 100 Continue 说明服务器收到了请求的初始部分，请客户端继续发送 101 Switching Protocol 说明服务器正在根据客户端的指定，将协议转换成为Update首部所列的协议 200 ~ 299——成功状态码 状态码 原因短语 含义 200 OK 请求没问题 201 Ctreated 用于创建服务器对象的请求（如PUT） 202 Accepted 请求已被接受，但服务器还未对其执行任何动作，不保证服务器会完成这个请求 203 Non-Authoritative 实体首部包含的信息不是来自于源端服务器，而是来自资源的一份副本 204 No content 报文中没有包含实体内容 205 Reset Content 用于浏览器的代码，负责告知浏览器清除当前页面中所有的HTML表单元素 206 Partial Content 说明成功执行了一部分（或某个范围内）的请求，但尚未完全完成请求 300 ~ 399——重定向状态码 状态码 原因短语 含义 300 Multiple Choices（多种选择） 当客户端请求的URL实际上指向多个资源的时候返回此码，通常会附带可供选择的选项，让客户端选择，如某个URL请求的资源含有中文和英文的两个版本，服务器不知道客户端需要哪一个，所以返回此码 301 Moved Permanently（永久移动） 当请求的URL已被永久地移除时候使用，返回的Location首部中应该会包含资源的新地方URL 302 Found （临时移动） 与301类似，但是由于不是永久移动，将来可能还会移回去原处，因此现在客户端应该使用location首部给出的URL来临时定位资源，以后再次使用还是用原来的老URL . 注：在http1.0版本中，虽然对于RFC规范规定302不允许重定向是改变请求的方法，但很多浏览器都会无视原来的方法，使用GET请求新的地址。所以在http1.1版本中，302拆分了303和307两个状态码来做不同的重定向。 303 See other（ 参看其他位置） 告知客户端应该用另一个URL来获取资源。新的URL位于响应报文的Location首部。其主要目的是允许Post的请求重定向时使用GET请求来获取新的资源，请求主体部分会被丢弃。跟302作用是一样的。 304 Not Modified（未被修改） 客户端通过所包含的请求首部，使得请求变成有条件的，自从上次请求后，请求的网页未修改过。服务器返回此响应时，不会返回网页内容。如果网页自请求者上次请求后再也没有更改过，您应将服务器配置为返回此响应（称为 If-Modified-Since HTTP 标头）。 305 Use Proxy 出于安全考虑，客户端不能直接访问该资源，要通过代理来访问，代理的位置由Location的首部给出。 306 未使用 307 Temporary Redirect(临时重定向) 与301状态码类似，但客户端应该使用Location首部给出的URL来临时定位资源。将来的请求应该使用老的URL。功能其实就是302规范上定义的功能，如果是POST请求，则重定向时，也会带着请求的主体部分POST到新的地址。 400 ~ 499——客户端错误状态码 状态码 原因短语 含义 400 Bad Request 用于告知客户端它发送了一个错误的请求 401 Unauthorized 请求要求身份验证 402 Payment Required 未使用 403 Forbidden 服务器拒绝请求 404 Not Found 资源未找到 405 Method Not Allowed 不支持客户端请求所使用的http方法，返回这个状态码的同时，可以在响应中添加Allow首部，来告知客户端对请求的资源可以使用那些http方法。 406 Not Acceptable 客户端可以指定参数来说明他们愿意接收什么类型的实体，如果服务端没有这种类型的资源，则返回此状态码。即客户端想要的响应实体类型，服务端给不了。 407 Proxy Authentication Required 此状态码与401类似，但指定请求者应当授权使用代理。如果服务器返回此响应，还表示请求者应当使用代理。 408 Request Timeout 服务器等候请求时发生超时。 409 Conflict 服务器在完成请求时发生冲突。服务器必须在响应中包含有关冲突的信息。 410 Gone 如果请求的资源已永久删除，服务器就会返回此响应。该代码与 404（未找到）代码类似，但在资源以前存在而现在不存在的情况下，有时会用来替代 404 代码。如果资源已永久移动，您应使用 301 指定资源的新位置。 411 Length Required 服务器不接受不含有效内容长度标头字段的请求。 412 Precondition Failed 服务器未满足请求者在请求中设置的其中一个前提条件。 413 Request Entity Too Large 服务器无法处理请求，因为请求实体过大，超出服务器的处理能力。 414 Request URI Too Long 请求的 URL过长，服务器无法处理。 415 Unsupported Media Type 服务端无法理解或无法支撑客户端所发实体的内容类型。即客户端给的请求实体类型，服务端支持不了。（注意与406的区分） 416 Requested Range Not Satisfiable 请求报文所请求的是指定资源的某个范围，而此范围无效或无法满足。 417 Expectation Failed 服务器无法满足Expect首部字段的期望。 500 ~ 599——服务器错误状态码 状态码 原因短语 含义 500 Internal Server Error 服务器遇到错误，无法完成请求。 501 Not Implemented 客户端发起的请求超出服务的能力范围。 502 Bad Gateway 服务器作为网关或代理，从上游服务器收到无效响应。 503 Service Unavailable 服务器目前无法使用（由于超载或停机维护）。通常，这只是暂时状态。可以添加Retry-After首部来告知什么时候可用。 504 Gateway Timeout 服务器作为网关或代理，但是没有及时从上游服务器收到请求。 505 HTTP Version Not Supported 服务器不支持请求中所用的 HTTP 协议版本。]]></content>
      <categories>
        <category>http协议</category>
      </categories>
      <tags>
        <tag>http</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java 11新特性]]></title>
    <url>%2F2019%2F03%2F09%2Fjava-11%E6%96%B0%E7%89%B9%E6%80%A7%2F</url>
    <content type="text"><![CDATA[之前写了一篇java 8新特性，相信目前很多人还是使用着java 8，那到底java 11值不值得升级呢，结合之前写的java 8与java 11的性能对比，你可以看完这篇java 11的新特性，再考虑要不要升级到java 11。 局部变量类型推断java 10新出的特性，局部变量类型推断，使用的是var关键字，只能在方法体内使用。 在java 10之前，我们的代码需要这么写： 1String text = "Hello Java 9"; 在java 10后，可以使用var来代替String： 1var text = "Hello Java 10"; 这种用法只能使用在局部变量上，即方法体内，不能用在成员变量上： 12345678public class VarTest &#123; var v = "123"; //编译错误 public void t1()&#123; var str = "hello world"; //编译正常 &#125;&#125; var说到底其实就是java的语法糖，是靠编译器推断var的类型并转换回去的，并没有改变java静态语言的本质，如： 12var text = "Hello Java 11";text = 23; // 编译错误,类型改变了，这种写法在动态语言如JavaScript中是可以的，但静态语言的java是不允许的。 想通这一点之后，就可以将var看成只是简化我们代码书写优点，所以使用final关键字，也是可以防止var变量改变的： 12final var text = "Banana";text = "Joe"; // 编译错误，text声明了final，不可改变 确定了var是语法糖后，对于编译器推断不出来场景，是用不了var的： 12345//下面四种都会编译错误var a; var nothing = null;var lambda = () -&gt; System.out.println("Pity!");var method = this::someMethod; var对于简化我们的书写还是很有帮助的，对于很长的变量如Map&lt;String, List&lt;Integer&gt;&gt;： 123456var myList = new ArrayList&lt;Map&lt;String, List&lt;Integer&gt;&gt;&gt;();for (var current : myList) &#123; // var指向类型: Map&lt;String, List&lt;Integer&gt;&gt; System.out.println(current);&#125; Java 11后，可以将var使用在lambda表达式的参数中: 1Predicate&lt;String&gt; predicate = (@Nullable var a) -&gt; true; 相信写惯静态语言的程序猿，看动态语言其实是很费劲的，因为不知道这个变量到底是什么类型的，对于var也是一样，好在IDEA可以通过CMD/CTRL指定var可以提示变量类型。 HTTP Client之前的网络请求，我们一般都是使用Apache的Httpclient工具包，Java 9增加了HttpClient，并在java 11可以在java.net中正式使用。 新的HttpClient可以使用同步或异步的方式请求，并可以使用BodyHandlers来解析响应体，如同步的写法： 1234567var request = HttpRequest.newBuilder() .uri(URI.create("https://www.edjdhbb.com")) .GET() .build();var client = HttpClient.newHttpClient();HttpResponse&lt;String&gt; response = client.send(request, HttpResponse.BodyHandlers.ofString());System.out.println(response.body()); 异步的写法： 1234567var request = HttpRequest.newBuilder() .uri(URI.create("https://www.edjdhbb.com")) .build(); //不写HTTP方法默认是GET方法var client = HttpClient.newHttpClient();client.sendAsync(request, HttpResponse.BodyHandlers.ofString()) .thenApply(HttpResponse::body) .thenAccept(System.out::println); 可以使用BodyPublishers来发送请求内容，类型可以是字符串、字节数组、文件、输入流等： 12345678var request = HttpRequest.newBuilder() .uri(URI.create("https://www.edjdhbb.com")) .header("Content-Type", "text/plain") .POST(HttpRequest.BodyPublishers.ofString("Hi there!"))//发送字符串内容 .build();var client = HttpClient.newHttpClient();var response = client.send(request, HttpResponse.BodyHandlers.ofString());System.out.println(response.statusCode()); // 200 如果服务端的身份验证是HTTP协议的授权认证，那么HttpClient也提供了对应的方法： 12345678910111213var request = HttpRequest.newBuilder() .uri(URI.create("https://www.edjdhbb.com")) .build();var client = HttpClient.newBuilder() .authenticator(new Authenticator() &#123; @Override protected PasswordAuthentication getPasswordAuthentication() &#123; return new PasswordAuthentication("postman", "password".toCharArray()); &#125; &#125;)//其实是生成了头信息Authorization: Basic $账号密码的basic64编码 .build();var response = client.send(request, HttpResponse.BodyHandlers.ofString());System.out.println(response.statusCode()); // 200 Collectionsjava 11里面的集合如List、Set、Map都提供了新的创建方法，List.of可以创建新的不可变集合，List.copyOf可以通过拷贝list来创建新的不可变的集合，如： 123var list = List.of("A", "B", "C");var copy = List.copyOf(list);System.out.println(list == copy); //因为list是不可变的，copyOf方法的目的也是要一个不可变的集合，所以copyOf方法做了个优化，如果输入的集合是不可变的，则直接返回原集合，故这里会打印true 123var list = new ArrayList&lt;String&gt;();var copy = List.copyOf(list);System.out.println(list == copy); // list是可变的，copyOf则通过list重新创建了一个新的不可变集合，故这里会打印false 通过of方法创建不可变map： 12var map = Map.of("A", 1, "B", 2);System.out.println(map); // &#123;B=2, A=1&#125; java 11的不可变集合还是以前的那套不可变实现，熟悉java不可变集合的读者都知道，java原生的不可变集合并非真不可变，只是在原集合上套了一个门面。如：A为可变集合，B为通过A构建的不可变集合，如果只是操作B，那是没问题的，但是操作A就有问题了，A添加了一个元素，B也会添加，因为B只是套在A上的门面。 不过java 11的of和copyOf方法保证了A不会被操作到，所以问题不大。 Streams在java 8新特性里面笔者已经介绍了一些Streams的方法及用法，也在java 8 stream底层原理里面讲解了它的技术原理，不在重复。在java 11中，streams增加了三个新的方法，Stream.ofNullable类似于Optional可接受一个可能空的实例来构建流： 12Stream.ofNullable(null) .count() // 打印0 还有dropWhile and takeWhile两个方法，不同于filter方法作用于所有的元素，这两个方法都是遇到第一个false则不在执行while循环： 1234567Stream.of(1, 2, 3, 2, 1) .dropWhile(n -&gt; n &lt; 3) //遇到3则跳出了dropWhile，后面的元素即使小于3也不会被drop .collect(Collectors.toList()); // [3, 2, 1]Stream.of(1, 2, 3, 2, 1) .takeWhile(n -&gt; n &lt; 3) //遇到3则跳出了takeWhile，后面元素即使小于3也不会被take .collect(Collectors.toList()); // [1, 2] Optionalsoptional增加了转换为stream的方法： 1Optional.of("foo").stream().count(); // 1 Strings字符串增加了一些字符操作方法： 123456" ".isBlank(); // true" Foo Bar ".strip(); // "Foo Bar"" Foo Bar ".stripTrailing(); // " Foo Bar"" Foo Bar ".stripLeading(); // "Foo Bar ""Java".repeat(3); // "JavaJavaJava""A\nB\nC".lines().count(); // 3 InputStreamsInputStream增加了一个非常好用的方法，可以直接传递数据导OutputStream: 123456var classLoader = ClassLoader.getSystemClassLoader();var inputStream = classLoader.getResourceAsStream("myFile.txt");var tempFile = File.createTempFile("myFileCopy", "txt");try (var outputStream = new FileOutputStream(tempFile)) &#123; inputStream.transferTo(outputStream);&#125; JVM 特性java 11除了上面那些语言上的API特性，还增加了一些JVM的新特性，可以直接查看官网： Flow API for reactive programming Java Module System Application Class Data Sharing Dynamic Class-File Constants Java REPL (JShell) Flight Recorder Unicode 10 G1: Full Parallel Garbage Collector ZGC: Scalable Low-Latency Garbage Collector Epsilon: No-Op Garbage Collector Deprecate the Nashorn JavaScript Engine 点击了解更多的java知识]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java 8 stream底层原理]]></title>
    <url>%2F2019%2F02%2F23%2Fjava-8-stream%E5%BA%95%E5%B1%82%E5%8E%9F%E7%90%86%2F</url>
    <content type="text"><![CDATA[深入理解Java Stream流水线stream用起来真的很爽，但简洁的方法下面似乎隐藏着无尽的秘密，如此强大的API是如何实现的呢？Pipeline是怎么执行的，每次方法调用都会导致一次迭代吗？自动并行又是怎么做到的，线程个数是多少？本节我们学习Stream流水线的原理，这是Stream实现的关键所在。 首先回顾一下容器执行Lambda表达式的方式，以ArrayList.forEach()方法为例，具体代码如下： 12345678// ArrayList.forEach()public void forEach(Consumer&lt;? super E&gt; action) &#123; ... for (int i=0; modCount == expectedModCount &amp;&amp; i &lt; size; i++) &#123; action.accept(elementData[i]);// 回调方法 &#125; ...&#125; 我们看到ArrayList.forEach()方法的主要逻辑就是一个for循环，在该for循环里不断调用action.accept()回调方法完成对元素的遍历。这完全没有什么新奇之处，回调方法在Java GUI的监听器中广泛使用。Lambda表达式的作用就是相当于一个回调方法，这很好理解。 Stream API中大量使用Lambda表达式作为回调方法，但这并不是关键。理解Stream我们更关心的是另外两个问题：流水线和自动并行。使用Stream或许很容易写入如下形式的代码： 12345int longestStringLengthStartingWithA = strings.stream() .filter(s -&gt; s.startsWith("A")) .mapToInt(String::length) .max(); 上述代码求出以字母A开头的字符串的最大长度，一种直白的方式是为每一次函数调用都执一次迭代，这样做能够实现功能，但效率上肯定是无法接受的。类库的实现着使用流水线（Pipeline）的方式巧妙的避免了多次迭代，其基本思想是在一次迭代中尽可能多的执行用户指定的操作。为讲解方便我们汇总了Stream的所有操作。 Stream操作分类中间操作(Intermediate operations)无状态(Stateless)unordered() filter() map() mapToInt() mapToLong() mapToDouble() flatMap() flatMapToInt() flatMapToLong() flatMapToDouble() peek()有状态(Stateful)distinct() sorted() sorted() limit() skip() 结束操作(Terminal operations)非短路操作forEach() forEachOrdered() toArray() reduce() collect() max() min() count()短路操作(short-circuiting)anyMatch() allMatch() noneMatch() findFirst() findAny() Stream上的所有操作分为两类：中间操作和结束操作，中间操作只是一种标记，只有结束操作才会触发实际计算。中间操作又可以分为无状态的(Stateless)和有状态的(Stateful)，无状态中间操作是指元素的处理不受前面元素的影响，而有状态的中间操作必须等到所有元素处理之后才知道最终结果，比如排序是有状态操作，在读取所有元素之前并不能确定排序结果；结束操作又可以分为短路操作和非短路操作，短路操作是指不用处理全部元素就可以返回结果，比如找到第一个满足条件的元素。之所以要进行如此精细的划分，是因为底层对每一种情况的处理方式不同。 一种直白的实现方式 仍然考虑上述求最长字符串的程序，一种直白的流水线实现方式是为每一次函数调用都执一次迭代，并将处理中间结果放到某种数据结构中（比如数组，容器等）。具体说来，就是调用filter()方法后立即执行，选出所有以A开头的字符串并放到一个列表list1中，之后让list1传递给mapToInt()方法并立即执行，生成的结果放到list2中，最后遍历list2找出最大的数字作为最终结果。程序的执行流程如如所示： 这样做实现起来非常简单直观，但有两个明显的弊端： 迭代次数多。迭代次数跟函数调用的次数相等。 频繁产生中间结果。每次函数调用都产生一次中间结果，存储开销无法接受。 这些弊端使得效率底下，根本无法接受。如果不使用Stream API我们都知道上述代码该如何在一次迭代中完成，大致是如下形式： 1234567int longest = 0;for(String str : strings)&#123; if(str.startsWith("A"))&#123;// 1. filter(), 保留以A开头的字符串 int len = str.length();// 2. mapToInt(), 转换成长度 longest = Math.max(len, longest);// 3. max(), 保留最长的长度 &#125;&#125; 采用这种方式我们不但减少了迭代次数，也避免了存储中间结果，显然这就是流水线，因为我们把三个操作放在了一次迭代当中。只要我们事先知道用户意图，总是能够采用上述方式实现跟Stream API等价的功能，但问题是Stream类库的设计者并不知道用户的意图是什么。如何在无法假设用户行为的前提下实现流水线，是类库的设计者要考虑的问题。 Stream流水线解决方案我们大致能够想到，应该采用某种方式记录用户每一步的操作，当用户调用结束操作时将之前记录的操作叠加到一起在一次迭代中全部执行掉。沿着这个思路，有几个问题需要解决： 用户的操作如何记录？ 操作如何叠加？ 叠加之后的操作如何执行？ 执行后的结果（如果有）在哪里？ 操作如何记录 注意这里使用的是“操作(operation)”一词，指的是“Stream中间操作”的操作，很多Stream操作会需要一个回调函数（Lambda表达式），因此一个完整的操作是&lt;数据来源，操作，回调函数&gt;构成的三元组。Stream中使用Stage的概念来描述一个完整的操作，并用某种实例化后的PipelineHelper来代表Stage，将具有先后顺序的各个Stage连到一起，就构成了整个流水线。跟Stream相关类和接口的继承关系图示。 还有IntPipeline, LongPipeline, DoublePipeline没在图中画出，这三个类专门为三种基本类型（不是包装类型）而定制的，跟ReferencePipeline是并列关系。图中Head用于表示第一个Stage，即调用调用诸如Collection.stream()方法产生的Stage，很显然这个Stage里不包含任何操作；StatelessOp和StatefulOp分别表示无状态和有状态的Stage，对应于无状态和有状态的中间操作。 Stream流水线组织结构示意图如下： 图中通过Collection.stream()方法得到Head也就是stage0，紧接着调用一系列的中间操作，不断产生新的Stream。这些Stream对象以双向链表的形式组织在一起，构成整个流水线，由于每个Stage都记录了前一个Stage和本次的操作以及回调函数，依靠这种结构就能建立起对数据源的所有操作。这就是Stream记录操作的方式。 操作如何叠加以上只是解决了操作记录的问题，要想让流水线起到应有的作用我们需要一种将所有操作叠加到一起的方案。你可能会觉得这很简单，只需要从流水线的head开始依次执行每一步的操作（包括回调函数）就行了。这听起来似乎是可行的，但是你忽略了前面的Stage并不知道后面Stage到底执行了哪种操作，以及回调函数是哪种形式。换句话说，只有当前Stage本身才知道该如何执行自己包含的动作。这就需要有某种协议来协调相邻Stage之间的调用关系。 这种协议由Sink接口完成，Sink接口包含的方法如下表所示： 方法名 作用 void begin(long size) 开始遍历元素之前调用该方法，通知Sink做好准备。 void end() 所有元素遍历完成之后调用，通知Sink没有更多的元素了。 boolean cancellationRequested() 是否可以结束操作，可以让短路操作尽早结束。 void accept(T t) 遍历元素时调用，接受一个待处理元素，并对元素进行处理。Stage把自己包含的操作和回调方法封装到该方法里，前一个Stage只需要调用当前Stage.accept(T t)方法就行了。 有了上面的协议，相邻Stage之间调用就很方便了，每个Stage都会将自己的操作封装到一个Sink里，前一个Stage只需调用后一个Stage的accept()方法即可，并不需要知道其内部是如何处理的。当然对于有状态的操作，Sink的begin()和end()方法也是必须实现的。比如Stream.sorted()是一个有状态的中间操作，其对应的Sink.begin()方法可能创建一个乘放结果的容器，而accept()方法负责将元素添加到该容器，最后end()负责对容器进行排序。对于短路操作，Sink.cancellationRequested()也是必须实现的，比如Stream.findFirst()是短路操作，只要找到一个元素，cancellationRequested()就应该返回true，以便调用者尽快结束查找。Sink的四个接口方法常常相互协作，共同完成计算任务。实际上Stream API内部实现的的本质，就是如何重载Sink的这四个接口方法。 有了Sink对操作的包装，Stage之间的调用问题就解决了，执行时只需要从流水线的head开始对数据源依次调用每个Stage对应的Sink.{begin(), accept(), cancellationRequested(), end()}方法就可以了。一种可能的Sink.accept()方法流程是这样的： 1234void accept(U u)&#123; 1. 使用当前Sink包装的回调函数处理u 2. 将处理结果传递给流水线下游的Sink&#125; Sink接口的其他几个方法也是按照这种[处理-&gt;转发]的模型实现。下面我们结合具体例子看看Stream的中间操作是如何将自身的操作包装成Sink以及Sink是如何将处理结果转发给下一个Sink的。先看Stream.map()方法： 1234567891011121314151617// Stream.map()，调用该方法将产生一个新的Streampublic final &lt;R&gt; Stream&lt;R&gt; map(Function&lt;? super P_OUT, ? extends R&gt; mapper) &#123; ... return new StatelessOp&lt;P_OUT, R&gt;(this, StreamShape.REFERENCE, StreamOpFlag.NOT_SORTED | StreamOpFlag.NOT_DISTINCT) &#123; @Override /*opWripSink()方法返回由回调函数包装而成Sink*/ Sink&lt;P_OUT&gt; opWrapSink(int flags, Sink&lt;R&gt; downstream) &#123; return new Sink.ChainedReference&lt;P_OUT, R&gt;(downstream) &#123; @Override public void accept(P_OUT u) &#123; R r = mapper.apply(u);// 1. 使用当前Sink包装的回调函数mapper处理u downstream.accept(r);// 2. 将处理结果传递给流水线下游的Sink &#125; &#125;; &#125; &#125;;&#125; 上述代码看似复杂，其实逻辑很简单，就是将回调函数mapper包装到一个Sink当中。由于Stream.map()是一个无状态的中间操作，所以map()方法返回了一个StatelessOp内部类对象（一个新的Stream），调用这个新Stream的opWripSink()方法将得到一个包装了当前回调函数的Sink。 再来看一个复杂一点的例子。Stream.sorted()方法将对Stream中的元素进行排序，显然这是一个有状态的中间操作，因为读取所有元素之前是没法得到最终顺序的。抛开模板代码直接进入问题本质，sorted()方法是如何将操作封装成Sink的呢？sorted()一种可能封装的Sink代码如下： 123456789101112131415161718192021222324252627282930313233// Stream.sort()方法用到的Sink实现class RefSortingSink&lt;T&gt; extends AbstractRefSortingSink&lt;T&gt; &#123; private ArrayList&lt;T&gt; list;// 存放用于排序的元素 RefSortingSink(Sink&lt;? super T&gt; downstream, Comparator&lt;? super T&gt; comparator) &#123; super(downstream, comparator); &#125; @Override public void begin(long size) &#123; ... // 创建一个存放排序元素的列表 list = (size &gt;= 0) ? new ArrayList&lt;T&gt;((int) size) : new ArrayList&lt;T&gt;(); &#125; @Override public void end() &#123; list.sort(comparator);// 只有元素全部接收之后才能开始排序 downstream.begin(list.size()); if (!cancellationWasRequested) &#123;// 下游Sink不包含短路操作 list.forEach(downstream::accept);// 2. 将处理结果传递给流水线下游的Sink &#125; else &#123;// 下游Sink包含短路操作 for (T t : list) &#123;// 每次都调用cancellationRequested()询问是否可以结束处理。 if (downstream.cancellationRequested()) break; downstream.accept(t);// 2. 将处理结果传递给流水线下游的Sink &#125; &#125; downstream.end(); list = null; &#125; @Override public void accept(T t) &#123; list.add(t);// 1. 使用当前Sink包装动作处理t，只是简单的将元素添加到中间列表当中 &#125;&#125; 上述代码完美的展现了Sink的四个接口方法是如何协同工作的： 首先beging()方法告诉Sink参与排序的元素个数，方便确定中间结果容器的的大小； 之后通过accept()方法将元素添加到中间结果当中，最终执行时调用者会不断调用该方法，直到遍历所有元素； 最后end()方法告诉Sink所有元素遍历完毕，启动排序步骤，排序完成后将结果传递给下游的Sink； 如果下游的Sink是短路操作，将结果传递给下游时不断询问下游cancellationRequested()是否可以结束处理。 叠加之后的操作如何执行 Sink完美封装了Stream每一步操作，并给出了[处理-&gt;转发]的模式来叠加操作。这一连串的齿轮已经咬合，就差最后一步拨动齿轮启动执行。是什么启动这一连串的操作呢？也许你已经想到了启动的原始动力就是结束操作(Terminal Operation)，一旦调用某个结束操作，就会触发整个流水线的执行。 结束操作之后不能再有别的操作，所以结束操作不会创建新的流水线阶段(Stage)，直观的说就是流水线的链表不会在往后延伸了。结束操作会创建一个包装了自己操作的Sink，这也是流水线中最后一个Sink，这个Sink只需要处理数据而不需要将结果传递给下游的Sink（因为没有下游）。对于Sink的[处理-&gt;转发]模型，结束操作的Sink就是调用链的出口。 我们再来考察一下上游的Sink是如何找到下游Sink的。一种可选的方案是在PipelineHelper中设置一个Sink字段，在流水线中找到下游Stage并访问Sink字段即可。但Stream类库的设计者没有这么做，而是设置了一个Sink AbstractPipeline.opWrapSink(int flags, Sink downstream)方法来得到Sink，该方法的作用是返回一个新的包含了当前Stage代表的操作以及能够将结果传递给downstream的Sink对象。为什么要产生一个新对象而不是返回一个Sink字段？这是因为使用opWrapSink()可以将当前操作与下游Sink（上文中的downstream参数）结合成新Sink。试想只要从流水线的最后一个Stage开始，不断调用上一个Stage的opWrapSink()方法直到最开始（不包括stage0，因为stage0代表数据源，不包含操作），就可以得到一个代表了流水线上所有操作的Sink，用代码表示就是这样： 12345678910// AbstractPipeline.wrapSink()// 从下游向上游不断包装Sink。如果最初传入的sink代表结束操作，// 函数返回时就可以得到一个代表了流水线上所有操作的Sink。final &lt;P_IN&gt; Sink&lt;P_IN&gt; wrapSink(Sink&lt;E_OUT&gt; sink) &#123; ... for (AbstractPipeline p=AbstractPipeline.this; p.depth &gt; 0; p=p.previousStage) &#123; sink = p.opWrapSink(p.previousStage.combinedFlags, sink); &#125; return (Sink&lt;P_IN&gt;) sink;&#125; 现在流水线上从开始到结束的所有的操作都被包装到了一个Sink里，执行这个Sink就相当于执行整个流水线，执行Sink的代码如下： 12345678910// AbstractPipeline.copyInto(), 对spliterator代表的数据执行wrappedSink代表的操作。final &lt;P_IN&gt; void copyInto(Sink&lt;P_IN&gt; wrappedSink, Spliterator&lt;P_IN&gt; spliterator) &#123; ... if (!StreamOpFlag.SHORT_CIRCUIT.isKnown(getStreamAndOpFlags())) &#123; wrappedSink.begin(spliterator.getExactSizeIfKnown());// 通知开始遍历 spliterator.forEachRemaining(wrappedSink);// 迭代 wrappedSink.end();// 通知遍历结束 &#125; ...&#125; 上述代码首先调用wrappedSink.begin()方法告诉Sink数据即将到来，然后调用spliterator.forEachRemaining()方法对数据进行迭代（Spliterator是容器的一种迭代器，参阅），最后调用wrappedSink.end()方法通知Sink数据处理结束。逻辑如此清晰。 执行后的结果在哪里最后一个问题是流水线上所有操作都执行后，用户所需要的结果（如果有）在哪里？首先要说明的是不是所有的Stream结束操作都需要返回结果，有些操作只是为了使用其副作用(Side-effects)，比如使用Stream.forEach()方法将结果打印出来就是常见的使用副作用的场景（事实上，除了打印之外其他场景都应避免使用副作用），对于真正需要返回结果的结束操作结果存在哪里呢？ 特别说明：副作用不应该被滥用，也许你会觉得在Stream.forEach()里进行元素收集是个不错的选择，就像下面代码中那样，但遗憾的是这样使用的正确性和效率都无法保证，因为Stream可能会并行执行。 12345678// 错误的收集方式ArrayList&lt;String&gt; results = new ArrayList&lt;&gt;();stream.filter(s -&gt; pattern.matcher(s).matches()) .forEach(s -&gt; results.add(s)); // Unnecessary use of side-effects!// 正确的收集方式List&lt;String&gt;results = stream.filter(s -&gt; pattern.matcher(s).matches()) .collect(Collectors.toList()); // No side-effects! 回到流水线执行结果的问题上来，需要返回结果的流水线结果存在哪里呢？这要分不同的情况讨论，下表给出了各种有返回结果的Stream结束操作。 返回类型 对应的结束操作 boolean anyMatch() allMatch() noneMatch() Optional findFirst() findAny() 归约结果 reduce() collect() 数组 toArray() 对于表中返回boolean或者Optional的操作（Optional是存放 一个 值的容器）的操作，由于值返回一个值，只需要在对应的Sink中记录这个值，等到执行结束时返回就可以了。 对于归约操作，最终结果放在用户调用时指定的容器中（容器类型通过收集器指定）。collect(), reduce(), max(), min()都是归约操作，虽然max()和min()也是返回一个Optional，但事实上底层是通过调用reduce()方法实现的。 对于返回是数组的情况，毫无疑问的结果会放在数组当中。这么说当然是对的，但在最终返回数组之前，结果其实是存储在一种叫做Node的数据结构中的。Node是一种多叉树结构，元素存储在树的叶子当中，并且一个叶子节点可以存放多个元素。这样做是为了并行执行方便。关于Node的具体结构，我们会在下一节探究Stream如何并行执行时给出详细说明。 结语本文详细介绍了Stream流水线的组织方式和执行过程，学习本文将有助于理解原理并写出正确的Stream代码，同时打消你对Stream API效率方面的顾虑。如你所见，Stream API实现如此巧妙，即使我们使用外部迭代手动编写等价代码，也未必更加高效。 本文转自博客园]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[复杂性应对之道 - 领域建模]]></title>
    <url>%2F2019%2F02%2F23%2F%E5%A4%8D%E6%9D%82%E6%80%A7%E5%BA%94%E5%AF%B9%E4%B9%8B%E9%81%93-%E9%A2%86%E5%9F%9F%E5%BB%BA%E6%A8%A1%2F</url>
    <content type="text"><![CDATA[为什么要领域建模维护过企业级业务系统的同学，基本上没有一个不抱怨业务代码烂的，过程式的面条代码充斥着屏幕，程序员的心力和体力都经受着极大的考验，怎么破？ DDD革命DDD革命性在于，领域模型准确反映了业务语言，而传统J2EE或Spring+Hibernate等事务性编程模型只关心数据，这些数据对象除了简单setter/getter方法外，没有任何业务方法，被比喻成贫血模式。 以银行账号Account为案例，Account有“存款”，“计算利息”和“取款”等业务行为，但是传统经典的方式是将“存款”，“计算利息”和“取款”行为放在账号的服务AccountService中，而不是放在Account对象本身之中。我们不能因为用了计算机，用了数据库，用了框架，业务模型反而被技术框架给绑架了，就像人虽然是由母亲生的，但是人的吃喝拉撒母亲不能替代，更不能以母爱名义剥夺人的正常职责行为，如果是这样，这个人就是被母爱绑架了。 DDD不是银弹软件的世界里没有银弹，是用事务脚本还是领域模型没有对错之分，关键看是否合适。就像自营和平台哪个模式更好？答案是都很好，所以亚马逊可以有三方入住，阿里也可以有自建仓嘛。 实际上，CQRS就是对事务脚本和领域模型两种模式的综合，因为对于Query和报表的场景，使用领域模型往往会把简单的事情弄复杂，此时完全可以用奥卡姆剃刀把领域层剃掉，直接访问Infrastructure。 我个人也是坚决反对过度设计的，因此对于简单业务场景，我强力建议还是使用事务脚本，其优点是简单、直观、易上手。但对于复杂的业务场景，你再这么玩就不行了，因为一旦业务变得复杂，事务脚本就很难应对，容易造成代码的“一锅粥”，系统的腐化速度和复杂性呈指数级上升。目前比较有效的治理办法就是领域建模，因为领域模型是面向对象的，在封装业务逻辑的同时，提升了对象的内聚性和重用性，因为使用了通用语言（Ubiquitous Language），使得隐藏的业务逻辑得到显性化表达，使得复杂性治理成为可能。 接下来，让我们看一个银行转账的实例，对比下事务脚本和领域模型两者编程模型的不同。 DDD初体验银行转账事务脚本实现在事务脚本的实现中，关于在两个账号之间转账的领域业务逻辑都被写在了MoneyTransferService的实现里面了，而Account仅仅是getters和setters的数据结构，也就是我们说的贫血模型： 123456789101112131415161718192021222324252627282930313233public class MoneyTransferServiceTransactionScriptImpl implements MoneyTransferService &#123; private AccountDao accountDao; private BankingTransactionRepository bankingTransactionRepository; . . . @Override public BankingTransaction transfer( String fromAccountId, String toAccountId, double amount) &#123; Account fromAccount = accountDao.findById(fromAccountId); Account toAccount = accountDao.findById(toAccountId); . . . double newBalance = fromAccount.getBalance() - amount; switch (fromAccount.getOverdraftPolicy()) &#123; case NEVER: if (newBalance &lt; 0) &#123; throw new DebitException("Insufficient funds"); &#125; break; case ALLOWED: if (newBalance &lt; -limit) &#123; throw new DebitException( "Overdraft limit (of " + limit + ") exceeded: " + newBalance); &#125; break; &#125; fromAccount.setBalance(newBalance); toAccount.setBalance(toAccount.getBalance() + amount); BankingTransaction moneyTransferTransaction = new MoneyTranferTransaction(fromAccountId, toAccountId, amount); bankingTransactionRepository.addTransaction(moneyTransferTransaction); return moneyTransferTransaction; &#125;&#125; 上面的代码大家看起来应该比较眼熟，因为目前大部分系统都是这么写的。需求评审完，工程师画几张UML图完成设计，就开始向上面这样怼业务代码了，这样写基本不用太费脑，完全是面向过程的代码风格。有些同学可能会说，我这样写也可以实现系统功能啊，还是那句话“just because you can, doesn’t mean you should”。说句不好听的，正是有这么多“没有追求”、“不求上进”的码农才造成了应用系统的混乱、败坏了应用开发的名声。这也是为什么很多应用开发工程师觉得工作没意思，技术含量低，觉得整天就是写if-else的业务逻辑代码，系统又烂，工作繁琐、无聊、没有成长、没有成就感，所以转向去做中间件啊，去写JDK啊，觉得那个NB。实际上，应用开发一点都不简单也不无聊，业务的变化比底层Infrastructure的变化要多得多，解决的难度也丝毫不比写底层代码容易，只是很多人选择了用无聊的方式去做。其实我们是有办法做的更优雅的，这种优雅的方式就是领域建模，唯有掌握了这种优雅你才能实现从工程师向应用架构的转型。同样的业务逻辑，接下来就让我们看一下用DDD是怎么做的。 银行转账领域模型实现如果用DDD的方式实现，Account实体除了账号属性之外，还包含了行为和业务逻辑，比如debit( )和credit( )方法。 1234567891011121314151617// @Entitypublic class Account &#123; // @Id private String id; private double balance; private OverdraftPolicy overdraftPolicy; . . . public double balance() &#123; return balance; &#125; public void debit(double amount) &#123; this.overdraftPolicy.preDebit(this, amount); this.balance = this.balance - amount; this.overdraftPolicy.postDebit(this, amount); &#125; public void credit(double amount) &#123; this.balance = this.balance + amount; &#125;&#125; 而且透支策略OverdraftPolicy也不仅仅是一个Enum了，而是被抽象成包含了业务规则并采用了策略模式的对象。 123456789101112131415161718192021222324252627public interface OverdraftPolicy &#123; void preDebit(Account account, double amount); void postDebit(Account account, double amount);&#125;public class NoOverdraftAllowed implements OverdraftPolicy &#123; public void preDebit(Account account, double amount) &#123; double newBalance = account.balance() - amount; if (newBalance &lt; 0) &#123; throw new DebitException("Insufficient funds"); &#125; &#125; public void postDebit(Account account, double amount) &#123; &#125;&#125;public class LimitedOverdraft implements OverdraftPolicy &#123; private double limit; . . . public void preDebit(Account account, double amount) &#123; double newBalance = account.balance() - amount; if (newBalance &lt; -limit) &#123; throw new DebitException( "Overdraft limit (of " + limit + ") exceeded: " + newBalance); &#125; &#125; public void postDebit(Account account, double amount) &#123; &#125;&#125; 而Domain Service只需要调用Domain Entity对象完成业务逻辑即可。 12345678910111213141516171819public class MoneyTransferServiceDomainModelImpl implements MoneyTransferService &#123; private AccountRepository accountRepository; private BankingTransactionRepository bankingTransactionRepository; . . . @Override public BankingTransaction transfer( String fromAccountId, String toAccountId, double amount) &#123; Account fromAccount = accountRepository.findById(fromAccountId); Account toAccount = accountRepository.findById(toAccountId); . . . fromAccount.debit(amount); toAccount.credit(amount); BankingTransaction moneyTransferTransaction = new MoneyTranferTransaction(fromAccountId, toAccountId, amount); bankingTransactionRepository.addTransaction(moneyTransferTransaction); return moneyTransferTransaction; &#125;&#125; 通过上面的DDD重构后，原来在事务脚本中的逻辑，被分散到Domain Service，Domain Entity和OverdraftPolicy三个满足SOLID的对象中，在继续阅读之前，我建议可以自己先体会一下DDD的好处。 领域建模的好处DDD最大的好处是：接触到需求第一步就是考虑领域模型，而不是将其切割成数据和行为，然后数据用数据库实现，行为使用服务实现，最后造成需求的首肢分离。DDD让你首先考虑的是业务语言，而不是数据。DDD强调业务抽象和面向对象编程，而不是过程式业务逻辑实现。重点不同导致编程世界观不同。 面向对象 封装：Account的相关操作都封装在Account Entity上，提高了内聚性和可重用性。 多态：采用策略模式的OverdraftPolicy（多态的典型应用）提高了代码的可扩展性。 业务语义显性化 通用语言：“一个团队，一种语言”，将模型作为语言的支柱。确保团队在内部的所有交流中，代码中，画图，写东西，特别是讲话的时候都要使用这种语言。例如账号，转账，透支策略，这些都是非常重要的领域概念，如果这些命名都和我们日常讨论以及PRD中的描述保持一致，将会极大提升代码的可读性，减少认知成本。说到这，稍微吐槽一下我们有些工程师的英语水平，有些神翻译让一些核心领域概念变得面目全非。 显性化：就是将隐式的业务逻辑从一推if-else里面抽取出来，用通用语言去命名、去写代码、去扩展，让其变成显示概念，比如“透支策略”这个重要的业务概念，按照事务脚本的写法，其含义完全淹没在代码逻辑中没有突显出来，看代码的人自然也是一脸懵逼，而领域模型里面将其用策略模式抽象出来，不仅提高了代码的可读性，可扩展性也好了很多。 如何进行领域建模初步建模好的模型应该是建立在对业务深入理解的基础上。就我自己的经验而言，建模是一个不断迭代的过程，一开始可以简单点来。 首先抓住一些核心概念，这些业务知识和核心概念可以通过和业务专家沟通，也可以通过头脑风暴的形式从User Story或者Event Storming去扣。 然后假设一些业务场景走查一下，再写一些伪代码验证一下run一下，看看顺不顺，如果很顺滑，说明没毛病，否则就要看看是不是需要调整一下模型，随着项目的进行和对业务理解的不断深入，这种迭代将持续进行。 举个栗子，比如让你设计一个中介系统，一个典型的User Story可能是“小明去找工作，中介说你留个电话，有工作机会我会通知你”，这里面的关键名词很可能就是我们需要的领域对象： 小明是求职者。 电话是求职者的属性。 中介包含了中介公司，中介员工两个关键对象。 工作机会肯定也是关键领域对象； 通知这个动词暗示我们这里用观察者模式会比较合适。 然后再梳理一下领域对象之间的关系，一个求职者可以应聘多个工作机会，一个工作机会也可以被多个求职者应聘，M2M的关系，中介公司可以包含多个员工，O2M的关系。对于这样简单的场景，这个建模就差不多了。 当然我们的业务场景往往比这个要复杂，而且不是所有的名词都是领域对象也可能是属性，也不是所有的动词都是方法也可能是领域对象，再者，看的见实体好找，看不见的、隐藏的，需要深入理解业务，需要“无中生有”才能得到的抽象就没那么容易发现了，所以要具体问题具体对待，这个进化的过程需要我们有很好的业务理解力，抽象能力以及建模的经验（知道为什么公司的job model里那么强调技术人员的业务理解力和抽象能力了吧。 比如通常情况下，价格和库存只是订单和商品的一个属性，但是在阿里系电商业务场景下，价格计算和库存扣减的复杂程度可以让你怀疑人生，因此作为电商中台，把价格和库存单独当成一个域（Domain）去对待是很必要的。 当然这个只是最初级的模型，接下来我会通过DDD中的一些核心概念的介绍，让大家更清楚的了解建模的过程。 领域事件(Domain Event) An event is something that has happened in the past. A domain event is, logically, something that happened in a particular domain, and something you want other parts of the same domain (in-process) or domain in aonther bounded context to be aware of and potentially react to. Domain Event是由一个特定领域触因为一个用户Command触发的发生在过去的行为产生的事件，而这个事件是系统中其它部分感兴趣的。 为什么Domain Event如此重要？ 因为在现在的分布式环境下，没有一个业务系统是割裂的，而Messaging绝对是系统之间耦合度最低，最健壮，最容易扩展的一种通信机制。因此理论上它是分布式系统的必选项。 但是目前大部分系统的Event都设计的很随性，没有统一的指导和规范，导致Event滥用和无用的情况时有发生，而Domain Event给我们一个很好的方向，指引我们该如何设计我们系统的Event。 Event命名 Your Domain Event type names should be a statement of a past occurrence, that is, a verb in the past tense. 因为表示的是过去事件，所以推荐命名为Domain Name + 动词的过去式 + Event。这样比较可以确切的表达业务语义。 下面是几个举例： CustomerCreatedEvent,表示客户创建后发出的领域事件。 OpportunityTransferedEvent，表示机会转移后发出的领域事件。 LeadsCreatedEvent，表示线索创建后发出的领域事件。 Event内容Event的内容有两种形式： Enrichment：也就是在Event的payload中尽量多多放data，这样consumer就可以自恰（Autonomy）的处理消息了。 Query-Back：这种是在Event中通过回调拿到更多的data，这种形式会加重系统的负载，performance也会差一些。 所以如果要在Enrichment和Query-Back之间做选择的话，首先推荐使用Enrichment。 Event SourcingEvent Sourcing是在Domain Event上面的一个扩展，是一个可选项。也就是要有一个Event Store保存所有的Events，其实如果你是用MetaQ作为Event机制的话，这些Events都是存储在MetaQ当中的，只是MetaQ并没有提供很好的Event查询和回溯，所以如果决定使用Event Sourcing的话，最好还是自己单独建立一个Event Store。 使用Event Sourcing主要有以下好处，如果用不到的话，完全可以不用，但是Domain Event还是强烈建议要使用。 Event Sourcing存储了所有发生在Core Domain上面的事件。 基于这些事件，我们可以做系统回放，系统Debug，以及做用户行为的分析（类似于打点） Event Storming事件风暴是《DDD Distilled》书中提出的一个业务分析的方法论，其主要作用是从Domian事件出发，来分析用户Command，来找到Ubiquitous Languange，来抽象Domain Entity以及Bounded Context。 可以和User Story的方法论结合起来使用，其最大的优点是，这种分析方式即使是non-tech的人，比如产品，业务专家等也能听得懂，也能参与进来。相比较一上来就使用UML画领域模型图而言。 聚合根(Aggreagte)聚合根（Aggregate Root）是DDD中的一个概念，是一种更大范围的封装，把一组有相同生命周期、在业务上不可分隔的实体和值对象放在一起考虑，只有根实体可以对外暴露引用，也是一种内聚性的表现。 确定聚合边界要满足固定规则（Invariant），是指在数据变化时必须保持的一致性规则，具体规则如下 根实体具有全局标识，最终负责检查规定规则 聚合内的实体具有本地标识，这些标识在Aggregate内部才是唯一的 外部对象不能引用除根Entity之外的任何内部对象 只有Aggregate的根Entity才能直接通过数据库查询获取，其他对象必须通过遍历关联来发现 Aggegate内部的对象可以保持对其他Aggregate根的引用 Aggregate边界内的任何对象修改时，整个Aggregate的所有固定规则都必须满足 还是看银行的例子，Account（账号）是CustomerInfo（客户信息）Entity和Address（值对象）的聚合根，Tansaction（交易）是流水（Journal）的聚合根，因为流水是因为交易才产生的，具有相同的生命周期。 最后提醒一下，聚合根是一个逻辑概念，主观性很强，所以在建模过程中很容易产生分歧，因此在日常工作中千万不要教条，把握住一条主要原则，我们的最终目的是为了业务语义显现化，如果因为聚合根把模型弄的晦涩难懂那就得不偿失了 领域服务(Domain Service)什么是领域服务有些领域中的动作，它们是一些动词，看上去却不属于任何对象。它们代表了领域中的一个重要的行为，所以不能忽略它们或者简单地把它们合并到某个实体或者值对象中。当这样的行为从领域中被识别出来时，最佳实践是将它声明成一个服务。这样的对象不再拥有内置的状态。它的作用仅仅是为领域提供相应的功能。Service往往是以一个活动来命名，而不是Entity来命名。例如开篇转账的例子，转账（transfer）这个行为是一个非常重要的领域概念，但是它是发生在两个账号之间的，归属于账号Entity并不合适，因为一个账号Entity没有必要去关联他需要转账的账号Entity，这种情况下，使用MoneyTransferDomainService就比较合适了。识别领域服务，主要看它是否满足以下三个特征： 服务执行的操作代表了一个领域概念，这个领域概念无法自然地隶属于一个实体或者值对象。 被执行的操作涉及到领域中的其他的对象。 操作是无状态的。 领域服务陷阱在使用领域服务时要特别当心，一个比较常见的错误是没有努力为行为找到一个适当的对象，就直接抽象成领域服务，这会使我们的代码逐渐转化为过程式的编程，一个极端的例子是把所有的行为都放到领域服务中，而领域模型退化成只有属性的贫血DO，那DDD就没有任何意义了。所以一定要深入思考，既不能勉强将行为放到不符合对象定义的对象中，破坏对象的内聚性，使其语义变得模糊。也不能不加思考的都放到领域服务中，从而退化成面向过程的编程。 应用服务和领域服务如何划分在领域建模中，我们一般将系统划分三个大的层次，即应用层（Application Layer），领域层（Domain Layer）和基础实施层（Infrastructure Layer），关于这三个层次的详细内容可以参考我的另一篇COLA框架的分层设计。可以看到在App层和Domain层都有服务（Service），这两个Service如何划分呢，什么样的功能应该放在应用层，什么样的功能应该放在领域层呢？决定一个服务（Service）应该归属于哪一层是很困难的。如果所执行的操作概念上属于应用层，那么服务就应该放到这个层。如果操作是关于领域对象的，而且确实是与领域有关的、为领域的需要服务，那么它就应该属于领域层。总的来说，涉及到重要领域概念的行为应该放在Domain层，而其它非领域逻辑的技术代码放在App层，例如参数的解析，上下文的组装，调用领域服务，消息发送等。还是银行转账的case为例，下图给出了划分的建议： 边界上下文(Bounded Context)领域实体是有边界上下文的，比如Apple这个实体不同的上下文，表达的含义就完全不一样，在水果店它就是水果，在苹果专卖店它就是手机。 所以边界上下文（Bounded Context）在DDD里面是一个非常重要的概念，Bounded Context明确地限定了模型的应用范围，在Context中，要保证模型在逻辑上统一，而不用考虑它是不是适用于边界之外的情况。在其他Context中，会使用其他模型，这些模型具有不同的术语、概念、规则和Ubiquitous Language的行话。 上下文映射(Context Mapping)那么不同Context下的业务要互相通信怎么办呢？这就涉及跨边界的Context Mapping（上下文映射），首先不同上下文之间的通信可以是同步的，也可以是异步的，同步的话一般是RPC或者RESTFul，异步的话会推荐上文提到的Domain Event. Mapping的方式有很多种，有Shared Kernal（共享内核），Conformist（追随者），以及Anti-Corruption（防腐层）等等。 我个人比较推崇Domain Event + AC，这样可以将系统之间的耦合降到最低。 以我们真实的业务场景举个例子，比如会员这个概念在ICBU网站是指网站上的Buyer，但是在CRM领域是指Customer，虽然很多的属性都是一样的，但是二者在不同的Context下其语义和概念是有差别的，我们需要用AC做一下转换： 边界上下文和微服务先来说一下微服务，抛开以Docker为代表的底层容器化技术不看，微服务和我们之前的SOA么有本质区别。 这不是我一个人的观点，关于这个想法我还专门求证了业界大牛Randy Shoup，问他微服务和SOA的区别，下面是他给我的回答 那么如何划分系统，才能得到一个比较合适的粒度，不会太粗，也不会太细呢。此时我们可以考虑DDD的战略设计，即从战略角度整体描述业务领域全貌，然后通过边界上下文将不同的实体归类到相对应的域里面。 比如在CRM领域，我们按照下面的战略设计图，我会自然的把CRM系统划分成销售服务，组织权限服务，营销服务，售卖服务。 模型重构最后我想强调的是，建模不是一次性的工作，也不可能是一次性的工作，业务在演化，随之而来的模型也需要演化和重构，当模型和业务部匹配的时候，你还是要霸王硬上弓的往里面塞，其结果可想而知。 模型统一建模的过程很像盲人摸象，不同背景人用不同的视角看同一个东西，其理解也是不一样的。比如两个盲人都摸到大象鼻子，一个人认为是像蛇（活的能动），而另一个人认为像消防水管（可以喷水），那么他们将很难集成。双方都无法接受对方的模型，因为那不符合自己的体验。事实上，他们需要一个新的抽象，这个抽象需要把蛇的“活着的特性”与消防水管的“喷水功能”合并到一起，而这个抽象还应该排除先前两个模型中一些不确切的含义和属性，比如毒牙，或者卷起来放到消防车上去的行为，这就是模型的统一。统一完的模型也许还不叫大象鼻子，但是已经很接近大象鼻子的属性和功能了，随着我们对模型对象、对业务理解的越来越深入、越来越透彻，我们会不断的调整演化我们的模型，所以建模不是一个one-time off的工作，而是一个持续不断演化重构的过程。 模型演化世界上唯一不变的就是变化，模型和代码一样也需要不断的重构和精化，每一次的精化之后，开发人员应该对领域知识有了更加清晰的认识。这使得理解上的突破成为可能，之后，一系列快速的改变得到了更符合用户需要并更加切合实际的模型。其功能性及说明性急速增强，而复杂性却随之消失。这种突破需要我们对业务有更加深刻的领悟和思考，然后再加上重构的勇气和能力，勇气是项目工期很紧你敢不敢重构，能力是你有没有完备的CI保证你的重构不破坏现有的业务逻辑。 实体在演变以开篇的银行账号为例，假如一开始账号都有withdraw（取钱）的行为，此时只需要在Account上加上withdraw方法就好了。 演变一：随着业务的发展，我们需要支持ATM账号和Online账号，而Online账号是不能withdraw的，此时最差的做法是保持模型不变，而是在withdraw方法中判断如果是OnlineAccount则抛出异常。这种简单的代码堆砌可以满足业务功能，但是其业务语义完全被掩盖。更好的重构方法应该是将withdraw抽成一个接口IWithdrawable。 演变二：好的，没有什么可以阻挡业务对变化的向往。现在公司出于安全性的考虑，为新开通的ATMAccount设置了取款上线，超过则不能支取。简单做法是在IWithdrawable中再加入一个setLimit行为，可是我们并不想改动影响到老的账号业务，所以更好的重构应该是重新写一个ILimitedWithdrawable接口，让其继承老接口，这样老的代码就可以保持不变了。通过上面的例子，我们可以看到领域模型和面向对象是一对孪生兄弟，我们会用到大量的OO原则，比如上面的重构就用到了SOLID的SRP（单一职责）和OCP（开闭原则）。在实际工作中，我的确也有这样的体会，自从践行DDD以后，我们采用OOA和OOD的时候比以前明显多了很多，OO的能力也在不断的提升。 引入新抽象还是以开篇的转账来举个例子，假如转账业务开始变的复杂，要支持现金，信用卡，支付宝，比特币等多种通道，且没种通道的约束不一样，还要支持一对多的转账。那么你还是用一个transfer(fromAccount, toAccount)就不合适了，可能需要抽象出一个专门的领域对象Transaction，这样才能更好的表达业务，其演化过程如下： 业务可视化和可配置化好的领域建模可以降低应用的复杂性，而可视化和可配置化主要是帮助大家（主要是非技术人员，比如产品，业务和客户）直观地了解系统和配置系统，提供了一种“code free”的解决方案，也是SaaS软件的主要卖点。要注意的是可视化和可配置化难免会给系统增加额外的复杂度，必须慎之又慎，最好是能使可视化和配置化的逻辑与业务逻辑尽量少的耦合，否则破坏了原有的架构，把事情搞的更复杂就得不偿失了。 在可扩展设计中，我已经介绍了我们SOFA架构是如何通过扩展点的设计来支撑不同业务差异化的需求的，那么可否更进一步，我们将领域的行为（也叫能力）和扩展点用可视化的方式呈现出来，并对于一些不需要编码实现的扩展点用配置的方式去完成呢。当然是可以的，比如还是开篇转账的例子，对于透支策略OverdraftPolicy这个业务扩展点，新来一个业务说透支额度不能超过1000，我们可以完全结合规则引擎进行配置化完成，而不需要编码。 所以我能想到的一种还比较优雅的方式，是通过Annotation注解的方式对领域能力和扩展点进行标注，然后在系统bootstrap阶段，通过代码扫描的方式，将这些能力点和扩展点收集起来上传到中心服务器，然后再通过GUI的方式呈现出来，从而做到业务的可视化和可配置化。大概的示意图如下： 有同学可能会问流程要不要可视化，这里要分清楚两个概念，业务逻辑流和工作流，很多同学混淆了这两个概念。业务逻辑流是响应一次用户请求的业务处理过程，其本身就是业务逻辑，对其编排和可视化的意义并不是很大，无外乎只是把代码逻辑可视化了，在我们的SOFA框架中，是通过扩展点和策略模式来处理业务的分支情况，而我看到我们阿里很多的内部系统将这种响应一次用户请求的业务逻辑用很重的工作流引擎来做，美其名曰流程可编排，实质上往往是把简单的事情复杂化了。而工作流是指完成一项任务所需要不同节点的连接，节点主要分为自动节点和人工节点，其中每个人工节点都需要用户的参与，也就是响应一次用户的请求，比如审批流程中的经理审批节点，CRM销售过程的业务员的处理节点等等。此时可以考虑使用工作流引擎，特别是当你的系统需要让用户自定义流程的时候，那就不得不使用可视化和可配置的工作流引擎了，除此之外，最好不要自找麻烦。我曾在银行工作过，亲眼看见过IBM是怎么忽悠银行使用它们的BPM系统，然后把系统弄的巨复杂无比，所以我对工作流引擎的印象并不好，当然也不排除有用的特别合适的案例，只是我还没看见，如果有看见的同学麻烦告诉我一声，学习一下。因为我们现在还没有让用户自定义流程的诉求，所以使用工作流引擎并不在我们现阶段的考虑范围之内。 本文转自领域建模的方法论]]></content>
      <categories>
        <category>编程思想</category>
      </categories>
      <tags>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[复杂度应对之道 - 阿里的COLA应用架构]]></title>
    <url>%2F2019%2F02%2F23%2F%E5%A4%8D%E6%9D%82%E5%BA%A6%E5%BA%94%E5%AF%B9%E4%B9%8B%E9%81%93%20-%20%E9%98%BF%E9%87%8C%E7%9A%84COLA%E5%BA%94%E7%94%A8%E6%9E%B6%E6%9E%84%2F</url>
    <content type="text"><![CDATA[最近阿里开源了一个COLA架构，COLA是Clean Object-Oriented and Layered Architecture的缩写，代表“整洁面向对象分层架构”，也叫“可乐”架构。 前言从业这么多年，接触过银行的应用，Apple的应用，eBay的应用和现在阿里的应用，虽然分属于不同的公司，使用了不同的架构，但有一个共同点就是都很复杂。导致复杂性的原因有很多，如果从架构的层面看，主要有两点，一个是架构设计过于复杂，层次太多能把人绕晕。另一个是根本就没架构，ServiceImpl作为上帝类包揽一切，一杆捅到DAO（就简单场景而言，这种Transaction Script也还凑合，至少实现上手都快。 这种人为的复杂性导致系统越来越臃肿，越来越难维护，酱缸的老代码发出一阵阵恶臭，新来的同学，往往要捂着鼻子抠几天甚至几个月，才能理清系统和业务脉络，然后又一头扎进各种bug fix，业务修补的恶性循环中，暗无天日！ CRM作为阿里最老的应用系统，自然也逃不过这样的宿命。不甘如此的我们开始反思到底是什么造成了系统复杂性？ 我们到底能不能通过架构来治理这种复杂性？基于这个出发点，我们团队开始了一段非常有意义的架构重构之旅（Redefine the Arch），期间我们参考了SalesForce，TMF2.0，汇金和盒马的架构，从他们那里汲取了很多有价值的输入，再结合我们自己的思考最终形成了我们自己现在的基于扩展点+元数据+CQRS+DDD的应用架构。该架构的特点是可扩展性好，很好的贯彻了OO思想，有一套完整的规范标准，并采用了CQRS和领域建模技术，在很大程度上可以降低应用的复杂度。本文主要阐述了我们的思考过程和架构实现，希望能对在路上的你有所帮助。 复杂性来自哪里经过我们分析、讨论，发现造成现在系统异常复杂的罪魁祸首主要来自以下四个方面： 可扩展性差对于只有一个业务的简单场景，并不需要扩展，问题也不突出，这也是为什么这个点经常被忽略的原因，因为我们大部分的系统都是从单一业务开始的。但是随着支持的业务越来越多，代码里面开始出现大量的if-else逻辑，这个时候代码开始有坏味道，没闻到的同学就这么继续往上堆，闻到的同学会重构一下，但因为系统没有统一的可扩展架构，重构的技法也各不相同，这种代码的不一致性也是一种理解上的复杂度。 久而久之，系统就变得复杂难维护。像我们CRM应用，有N个业务方，每个业务方又有N个租户，如果都要用if-else判断业务差异，那简直就是惨绝人寰。其实这种扩展点（Extension Point），或者叫插件（Plug-in）的设计在架构设计中是非常普遍的。比较成功的案例有eclipse的plug-in机制，集团的TMF2.0架构。还有一个扩展性需求就是字段扩展，这一点对SaaS应用尤为重要，因为有很多客户定制化需求，但是我们很多系统也没有统一的字段扩展方案。 面向过程是的，不管你承认与否，很多时候，我们都是操着面向对象的语言干着面向过程的勾当。面向对象不仅是一个语言，更是一种思维方式。在我们追逐云计算、深度学习、区块链这些技术热点的时候，静下心来问问自己我们是不是真的掌握了OOD；在我们强调工程师要具备业务Sense，产品Sense，数据Sense，算法Sense，XXSense的时候，是不是忽略了对工程能力的要求。 据我观察大部分工程师（包括我自己）的OO能力还远没有达到精通的程度，这种OO思想的缺乏主要体现在两个方面，一个是很多同学不了解SOLID原则，不懂设计模式，不会画UML图，或者只是知道，但从来不会运用到实践中；另一个是不会进行领域建模，关于领域建模争论已经很多了，我的观点是DDD很好，但不是银弹，用和不用取决于场景。 但不管怎样，请你抛开偏见，好好的研读一下Eric Evans的《领域驱动设计》，如果有认知升级的感悟，恭喜你，你进阶了。我个人认为DDD最大的好处是将业务语义显现化，把原先晦涩难懂的业务算法逻辑，通过领域对象（Domain Object），统一语言（Ubiquitous Language）将领域概念清晰的显性化表达出来。相信我，这种表达带来的代码可读性的提升，会让接手你代码的人对你心怀感恩的。借用Abelson的一句话是 Programs must be written for people to read, and only incidentally for machines to execute 所以强烈谴责那些不顾他人感受的编码行为。 分层不合理俗话说的好，All problems in computer science can be solved by another level of indirection（计算机科学领域的任何问题都可以通过增加一个间接的中间层来解决），怎样？ 是不是感受到间接层的强大了。分层最大的好处就是分离关注点，让每一层只解决该层关注的问题，从而将复杂的问题简化，起到分而治之的作用。 我们平时看到的MVC，pipeline，以及各种valve的模式，都是这个道理。好吧，那是不是层次越多越好，越灵活呢。当然不是，就像我开篇说的，过多的层次不仅不能带来好处，反而会增加系统的复杂性和降低系统性能。就拿ISO的网络七层协议来说，你这个七层分的很清楚，很好，但也很繁琐，四层就够了嘛。再比如我前面提到的过度设计的例子，如果没记错的话应该是Apple的Directory Service应用，整个系统有7层之多，把什么validator，assembler都当成一个层次来对待，能不复杂么。所以分层太多和没有分层都会导致系统复杂度的上升，因此我们的原则是不可以没有分层，但是只分有必要的层。 随心所欲随心所欲是因为缺少规范和约束。这个规范非常非常非常的重要（重要事情说三遍），但也是最容易被无视的点，其结果就是架构的consistency被严重破坏，代码的可维护性将急剧下降，国将不国，架构将形同虚设。 有同学会说不就是个naming的问题么，不就是个分包的问题么，不就是2个module还是3个module的问题么，只要功能能跑起来，这些问题都是小问题。是的，对于这些同学，我再丢给你一句名言“Just because you can, doesn’t mean you should”。 就拿package来说，它不仅仅是一个放一堆类的地方，更是一种表达机制，当你将一些类放到Package中时，相当于告诉下一位看到你设计的开发人员要把这些类放在一起考虑。理想很丰满，现实很骨感，规范的执行是个大问题，最好能在架构层面进行约束，例如在我们架构中，扩展点必须以ExtPt结尾，扩展实现必须以Ext结尾，你不这么写就会给你抛异常。 但是架构的约束毕竟有限，更多的还是要靠Code Review，暂时没想到什么更好的办法。这种对架构约束的近似严苛follow，确保了系统的consistency，最终形成了一个规整的收纳箱（如下图所示），就像我和团队说的，我们在评估代码改动点时，应该可以像Hash查找一样，直接定位到对应的module，对应的package里面对应的class。而不是到“一锅粥”里去慢慢抠。本章节最后，上一张我们老系统中比较典型的代码，也许你可以从中看到你自己应用的影子。 复杂性应对之道知道了问题所在，接下来看下我们是如何一个个解决这些问题的。回头站在山顶再看这些解决方案时，每个都不足为奇，但当你还“身在此山中”的时候，这个拨开层层迷雾，看到山的全貌的过程，并不是想象的那么容易。庆幸的是我团队在艰难跋涉之后，终有所收获。 扩展点设计扩展点的设计思想主要得益于TMF2.0的启发，其实这种设计思想也一直在用，但都是在局部的代码重构和优化，比如基于Strategy Pattern的扩展，但是一直没有找到一个很好的固化到框架中的方法。直到毗卢到团队分享，给了我们两个关键的提示，一个是业务身份识别，用他的话说，如果当时TMF1.0如果有身份识别的话，就没有TMF2.0什么事了；另一个是抽象的扩展点机制。 身份识别业务身份识别在我们的应用中非常重要，因为我们的CRM系统要服务不同的业务方，而且每个业务方又有多个租户。比如中供销售，中供拍档，中供商家都是不同的业务方，而拍档下的每个公司，中供商家下的每个供应商又是不同的租户。所以传统的基于多租户（TenantId）的业务身份识别还不能满足我们的要求，于是在此基础上我们又引入了业务码（BizCode）来标识业务。所以我们的业务身份实际上是（BizCode，TenantId）二元组。在每一个业务身份下面，又可以有多个扩展点（ExtensionPoint），所以一个扩展点实现（Extension）实际上是一个三维空间中的向量。借鉴Maven Coordinate的概念我给它起了个名字叫扩展坐标（Extension Coordinate），这个坐标可以用（ExtensionPoint，BizCode，TenantId）来唯一标识。有了业务身份这个关键抽象之后，通过身份来获取扩展实现的过程就变得水到渠成了，具体流程如下 扩展点扩展点的设计是这样的，所有的扩展点（ExtensionPoint）必须通过接口申明，扩展实现（Extension）是通过Annotation的方式标注的，Extension里面使用BizCode和TenantId两个属性用来标识身份，框架的Bootstrap类会在Spring启动的时候做类扫描，进行Extension注册，在Runtime的时候，通过TenantContext来选择要使用的Extension。TenantContext是通过Interceptor在调用业务逻辑之前进行初始化的。整个过程如下图所示： 实例展示比如在一个CRM系统里，客户要添加联系人Contact是一个，但是在添加联系人之前，我们要判断这个Contact是不是已经存在了，如果存在那么就不能添加了。不过在一个支持多业务的系统里面，可能每个业务的冲突检查都不一样，这是一个典型的可以扩展的场景。那么在SOFA框架中，我们可以这样去做。 1、定义扩展点 12345678910public interface ContactConflictRuleExtPt extends RuleI, ExtensionPointI &#123; /** * 查询联系人冲突 * * @param contact 冲突条件，不同业务会有不同的判断规则 * @return 冲突结果 */ public boolean queryContactConflict(ContactE contact); &#125; 2、实现业务的扩展实现 123456789101112131415161718192021222324@Extension(bizCode = BizCode.ICBU)public class IcbuContactConflictRuleExt implements ContactConflictRuleExtPt &#123; @Autowired private RepeatCheckServiceI repeatCheckService; @Autowired private MemberMappingQueryTunnel memberMappingQueryTunnel; private Logger logger = LoggerFactory.getLogger(getClass()); /** * 查询联系人冲突 * * @param contact 冲突条件，不同业务会有不同的判断规则 * @return 冲突结果 */ @Override public boolean queryContactConflict(ContactE contact) &#123; Set&lt;String&gt; emails = contact.getEmail(); //具体的业务逻辑 return false; &#125; 3、在领域实体中调用扩展实现 123456789101112131415161718192021222324252627282930@ToString@Getter@Setterpublic class CustomerE extends Entity &#123; /** * 公司ID */ private String companyId; /** * 公司(客户)名字 */ private String companyName; /** * 公司(客户)英文名字 */ private String companyNameEn; /** * 给客户添加联系人 * @param contact */ public void addContact(ContactE contact,boolean checkConflict)&#123; // 业务检查 if (checkConflict) &#123; ruleExecutor.execute(ContactConflictRuleExtPt.class, p -&gt; p.queryContactConflict(contact)); &#125; contact.setCustomerId(this.getId()); contactRepository.create(contact); &#125;&#125; 在上面的代码中，框架在runtime的时候之所以可以找到对应的扩展实现，主要是靠@Extension(bizCode = BizCode.ICBU)这个Annotation, 因为在系统启动时，Bootstrap会扫描所有的扩展实现并注册并缓存到HashMap里面。 面向对象面向对象不仅是一种编程语言，更是一种思维模式。所以看到很多简历里面写“精通Java”，没写“精通OO”，也算是中肯，因为会Java语言并不代表你就掌握了面向对象思维（当然，精通Java也不是件易事），要想做到精通，必须要对OO设计原则，模式，方法论有很深入的理解，同时要具备非常好的业务理解力和抽象能力，才能说是精通，这种思维的训练是一个长期不断累积的过程，我也在路上，下面是我对面向对象设计的两点体会： SOLIDSOLID是单一职责原则(SRP)，开闭原则(OCP)，里氏替换原则(LSP)，接口隔离原则(ISP)和依赖倒置原则(DIP)的缩写，原则是要比模式（Design Pattern）更基础更重要的指导准则，是面向对象设计的Bible。深入理解后，会极大的提升我们的OOD能力和代码质量。 单一职责比如我在开篇提到的ServiceImpl上帝类的例子，很明显就是违背了单一职责，你一个类把所有事情都做了，把不是你的功能也往自己身上揽，所以你的内聚性就会很差，内聚性差将导致代码很难被复用，不能复用，只能复制（Repeat Yourself），其结果就是一团乱麻。 依赖倒置再比如在java应用中使用logger框架有很多选择，什么log4j，logback，common logging等，每个logger的API和用法都稍有不同，有的需要用isLoggable()来进行预判断以便提高性能，有的则不需要。对于要切换不同的logger框架的情形，就更是头疼了，有可能要改动很多地方。产生这些不便的原因是我们直接依赖了logger框架，应用和框架的耦合性很高。 怎么破？ 遵循下依赖倒置原则就能很容易解决，依赖倒置就是你不要直接依赖我，你和我都同时依赖一个接口（所以有时候也叫面向接口的编程），这样我们之间就解耦了，依赖和被依赖方都可以自由改动了。在我们的框架设计中，这种对SOLID的遵循也是随处可见，Service Facade设计思想来自于单一职责SRP；扩展点设计符合关闭原则OCP；日志设计，以及Repository和Tunnel的交互就用到了依赖倒置DIP原则，这样的点还有很多，就不一一枚举了。当然了，SOLID不是OO的全部。抽象能力，设计模式，架构模式，UML，以及阅读优秀框架源码（我们的Command设计就是参考了Activiti的Command）也都很重要。只是SOLID更基础，更重要，所以我在这里重点拿出来讲一下，希望能得到大家的重视。 领域建模准确的说DDD不是一个架构，而是思想和方法论，关于如何领域建模的详细请参看我另一篇文章领域建模。所以在架构层面我们并没有强制约束要使用DDD，但对于像我们这样的复杂业务场景，我们强烈建议使用DDD代替事务脚本（TS: Transaction Script）。因为TS的贫血模式，里面只有数据结构，完全没有对象（数据+行为）的概念，这也是为什么我们叫它是面向过程的原因。 然而DDD是面向对象的，是一种知识丰富的设计（Knowledge Rich Design），怎么理解？，就是通过领域对象（Domain Object），领域语言（Ubiquitous Language）将核心的领域概念通过代码的形式表达出来，从而增加代码的可理解性。这里的领域核心不仅仅是业务里的“名词”，所有的业务活动和规则如同实体一样，都需要明确的表达出来。 业务语义显性化例如前面典型代码图中所展示的，分配策略（DistributionPolicy）你把它隐藏在一堆业务逻辑中，没有人知道它是干什么的，也不会把它当成一个重要的领域概念去重视。但是你把它抽出来，凸显出来，给它一个合理的命名叫DistributionPolicy，后面的人一看就明白了，哦，这是一个分配策略，这样理解和使用起来就容易的多了，添加新的策略也更方便，不需要改原来的代码了。所以说好的代码不仅要让程序员能读懂，还要能让领域专家也能读懂。 通用语言再比如在CRM领域中，公海和私海是非常重要领域概念，是用来做领地划分的，每个销售人员只能销售私海（自己领地）内的客户，不能越界。但是在我们的代码中却没有这两个实体（Entity），也没有相应的语言和其对应，这就导致了领域专家描述的，和我们日常沟通的，以及我们模型和代码呈现的都是相互割裂的，没有关联性。这就给后面系统维护的同学造成了极大的困扰，因为所有关于公海私海的操作，都是散落着各处的repeat itself的逻辑代码，导致看不懂也没办法维护。 采用领域建模以后，我们在系统中定义了清晰的机会（Opportunity），公海（PublicSea）和私海（PrivateSea）的Entity，相应的行为和业务逻辑也被封装到对应的领域实体身上，让代码充分展现业务语义，让曾经散落在各处找到了业务代码找到了属于它们自己的家，它们应该在的地方。相信我，这种代码可读性的提升，会让后来接手系统的同学对你心怀感恩。下面就是我们重构后Opportunity实体的代码，即使你对CRM领域不了解，是不是也很容易看懂： 12345678910111213141516171819202122232425262728293031public class OpportunityE extends Entity&#123; private String customerId; private String ownerId; private OpportunityType opportunityType; //是否可以捡入 public boolean isCanPick()&#123; return "y".equals(canPick) &amp;&amp; opportunityStatus == OpportunityStatus.NEW || opportunityStatus == OpportunityStatus.ACTIVE; &#125; //是否可以开放 public boolean isCanOpen()&#123; return (opportunityStatus == OpportunityStatus.NEW || opportunityStatus == OpportunityStatus.ACTIVE) &amp;&amp; CommonUtils.isNotEmpty(ownerId); &#125; //捡入机会到私海 public void pickupTo(PrivateSeaE privateSea)&#123; privateSea.addOpportunity(this); &#125; //开放到公海 public void openTo(PublicSea publicSea)&#123; publicSea.addOpportunity(this); &#125; //机会转移 public void transfer(PrivateSeaE from, PrivateSeaE to)&#123; from.removeOpportunity(this);//从一个私海移出 to.addOpportunity(this);//添加到另一个私海中 &#125; 如果整个系统都采用DDD，不仅代码的可读性和系统的可维护性会大大提升，系统之间的边界和交互也会更加的清晰。下图是CRM域的简要领域模型，基本上可以完整的表达CRM领域的核心概念 关于CQRS简要说一下，我们只使用了Command，Query分离的概念，我们提倡使用Domain Event，但是认为Event Sourcing是可选项。关于Command的实现我们使用了命令模式，因此以前的ServiceImpl的职责就只是一个Facade，所有的处理逻辑都在CommandExecutor里面。 分层设计这一块的设计比较直观，整个应用层划分为三个大的层次，分别是App层，Domain层和Infrastructure层。 App层主要负责获取输入，组装context，做输入校验，发送消息给领域层做业务处理，监听确认消息，如果需要的话使用MetaQ进行消息通知； Domain层主要是通过领域服务（Domain Service），领域对象（Domain Object）的交互，对上层提供业务逻辑的处理，然后调用下层Repository做持久化处理； Infrastructure层主要包含Repository，Config，Common和message，Repository负责数据的CRUD操作，这里我们借用了盒马的数据通道（Tunnel）的概念，通过Tunnel的抽象概念来屏蔽具体的数据来源，来源可以是MySQL，NoSql，Search，甚至是HSF等；Config负责应用的配置；Common是一写工具类；负责message通信的也应该放在这一层。这里需要注意的是从其他系统获取的数据是有界上下文（Bounded Context）下的数据，为了弥合Bounded Context下的语义Gap，通常有两种方式，一个是用大领域（Big Domain）把两边的差异都合起来，另一个是增加防腐层（Anticorruption Layer）做转换。什么是Bounded Context？ 简单阐述一下，就是我们的领域概念是有作用范围的（Context）的，例如摇头这个动作，在中国的Context下表示NO，但是在印度的Context下却是YES。 规范设计整洁的代码就像开篇提到的收纳整洁的玩具柜，和玩具收纳一样，需要做到以下两点： 放对位置东西不要乱放，我们的每一个组件（Module），每一个包（Package）都有明确的职责定义和范围，不可以放错，例如extension包就只是用来放扩展实现的，不允许放其他东西，而Interceptor包就只是放拦截器的，validator包就只是放校验器的。我们的主要组件如下图：组件里面的Package如下图： 贴好标签东西放在合适位置后还要贴上合适的标签，也就是要按照规范合理命名，例如我们架构里面和数据有关的Object，主要有Client Object，Domain Object和Data Object，Client Object是放在二方库中和外部交互使用的DTO，其命名必须以CO结尾，相应的Data Object主要是持久层使用的，命名必须以DO结尾。 这个类名应该是自明的（self-evident)，也就是看到类名就知道里面是干了什么事，这也就反向要求我们的类也必须是单一职责的（Single Responsibility）的，如果你做的事情不单纯，自然也就很难自明了。如果我们Class Name是自明的，Package Name是自明的，Module Name也是自明的，那么我们整个应用系统就会很容易被理解，看起来就会很舒服，维护效率会提高很多。除了组件和包的命名规范以外，我们对类、方法名和错误码等都做了相关规定。 类名约定 方法名约定从某种意义上来说，任何的业务操作，落到数据的层面，都是对数据的CRUD（增删改查），因此在写业务代码的时候，会经常碰到关于CRUD的命名，就拿查询来说，fetch, retrieve, get, find, query等等都能表示查询的意思，为了命名的一致性和统一性，为了保证每个概念对应一个词，我们有必要对CRUD的命名做一个约定。 比如，在SOFA框架中，我们就对CRUD的命名做了如下约定： CRUD操作 方法名约定 新增 create 添加 add 删除 remove（App和Domain层），delete（Infrastructure层） 修改 update 查询（返回单个结果） get 查询（返回多个结果） list 分页查询 page 统计 count 业务命名最好不用直接用CRUD，除非其行为有非常强的CRUD语义，比如用addContact表示添加联系人，removeContact表示删除联系人是可以接受的。但是如果你用createOrder和deleteOrder来表示下单和取消订单是不合适的，在业务层，更贴切的命名应该是placeOrder和cancelOrder。 当然在Infrastructure层，比如OrderDAO，你还是应该采用CRUD的命名方式，用createOrder和deleteOrder来表示对订单数据的创建和删除。 Tips：在业务层，尽量避免CRUD，努力找到更好的业务词汇来表达业务语义，如果非用不可，请使用约定好的CRUD命名。 错误码约定异常主要分为系统异常和业务异常，系统异常是指不可预期的系统错误，如网络连接，服务调用超时等，是可以retry的；而业务异常是指有明确业务语义的错误，再细分的话，又可以分为参数异常和业务逻辑异常，参数异常是指用户过来的请求不准确，逻辑异常是指不满足系统约束，比如客户已存在。业务异常是不需要retry的。 我们的错误码主要有3部分组成：类型+场景+自定义标识 错误类型 错误码约定 举例 参数异常 P_XX_XX P_CAMPAIGN_NameNotNull: 运营活动名不能为空 业务异常 B_XX_XX B_CAMPAIGN_NameAlreadyExist: 运营活动名已存在 系统异常 S_XX_ERROR S_DATABASE_ERROR: 数据库错误 Domain Event约定Domain Event(领域事件)，是领域实体发生状态变化后，向外界publish出来的事件。该事件既可以在当前的Bounded Context下面被消费，也可以被其它Bounded Context消费。 其命名规则是：领域名称+动词的一般过去式+Event 这里的动词的一般过去式，非常关键，因为在语义上表达的是发生过的事情，因为Event总是在动作发生后再发出的。下面是几个举例： CustomerCreatedEvent,表示客户创建后发出的领域事件。 OpportunityTransferedEvent，表示机会转移后发出的领域事件。 LeadsCreatedEvent，表示线索创建后发出的领域事件。 测试约定从开发的视角来看，主要是两方面的测试，一个是单元测试，一个是基于command的集成测试。 单元测试，主要是针对Domain层的业务逻辑测试，有下面3个约定： 测试粒度要小，通常是业务方法，其scope不要超过一个类。 要稳定，要快，使用mock，不要对外部环境有依赖。 放在Domain的test里面，命名方式为Entity + Test，例如CustomerTest, ContactTest, OpportunityTest等 案例： 123456789101112131415161718192021222324@RunWith(MockitoJUnitRunner.class)public class CustomerTest &#123; @InjectMocks private IcbuCustomerE customerE = new IcbuCustomerE(); @Mock private CustomerSearchTunnel customerSearchTunnel; @Test public void testIsCustomerAailableForPrivateSea()&#123; //没有业务员 PvgContext.set("","","",""); Assert.assertFalse(customerE.isAvailableForPrivateSea()); //是"删除客户"落入公海 PvgContext.set("121212","","",""); customerE.setCustomerGroup(CustomerGroup.AliCrmCustomerGroup.CANCEL_GROUP); Assert.assertFalse(customerE.isAvailableForPrivateSea()); //可以捡入私海 PvgContext.set("121212","","",""); customerE.setCustomerGroup(CustomerGroup.AliCrmCustomerGroup.ENQUIRY_GROUP); Assert.assertTrue(customerE.isAvailableForPrivateSea()); &#125; 集成测试，主要是针对Command的业务流程测试，有下面两个约定： 每个Command，以及分支场景必须要有集成测试覆盖。 不需要mock，依赖日常环境。 放在start的test里面，命名方式为XXXCmd + Test，例如IcbuLeadsAddCmdTest,CustomerPickupCmdTest等 案例： 12345678910111213141516171819202122232425public class IcbuLeadsAddCmdTest &#123; @Autowired protected LeadsRepository leadsRepository; @Autowired private CommandBusI commandBus; @Test public void testLeadsExistWithNoCustomer()&#123; //1. Prepare IcbuLeadsAddCmd cmd = getIcbuIMLeadsAddCmd(); String tenantId = "cn_center_10002404"; LeadsE leadsE = leadsRepository.getLeads(tenantId, cmd.getContactId()); if(leadsE != null) &#123; leadsE.setCustomerId(null); leadsRepository.updateCustomerId(leadsE); &#125; //2. Execute commandBus.send(cmd); //3. Check leadsE = leadsRepository.getLeads(tenantId, cmd.getContactId()); Assert.assertEquals(leadsE.getCustomerId(), "179001"); &#125; COLA应用架构经过上面的长篇大论，我希望我把我们的架构理念阐述清楚了，最后再从整体上看下我们的架构吧。我讲这个架构命名为COLA，全称是Clean Object-oriented and Layered Architecture，是一个整洁的，面向对象的，分层的，可扩展的应用架构，可以帮助降低复杂应用场景的系统熵值，提升系统开发和运维效率。目前COLA框架已经开源，要阅读源码或直接在项目中使用COLA，请移步：https://github.com/alibaba/COLA 整体架构我们的架构原则很简单，即在高内聚，低耦合，可扩展，易理解大的指导思想下，尽可能的贯彻OO的设计思想和原则。我们最终形成的架构是集成了扩展点+元数据+CQRS+DDD的思想，关于元数据前面没怎么提到，这里稍微说一下，对于字段扩展，简单一点的解决方案就是预留扩展字段，复杂一点的就是使用元数据引擎。使用元数据的好处是不仅能支持字段扩展，还提供了丰富的字段描述，等于是为以后的SaaS化配置提供了可能性，所以我们选择了使用元数据引擎。和DDD一样，元数据也是可选的，如果对没有字段扩展的需求，就不要用。最后的整体架构图如下： 提效工具Archetype因为框架包含了5个Module，20+的Package，如果手动创建的话很费时，而且很容易出错，所以创建了这个Archetype，可以一键构建框架的所有Artifacts，使用时，只需要将下面的命令中的demo替换成自己应用的名字即可： 1mvn archetype:generate -DgroupId=com.alibaba.sample -DartifactId=demo -Dversion=1.0.0-SNAPSHOT -Dpackage=com.alibaba.sample -DarchetypeArtifactId=cola-framework-archetype -DarchetypeGroupId=com.alibaba.cola -DarchetypeVersion=1.0.0-SNAPSHOT 测试容器不管你是不是TDD吧，写几行代码，然后本地跑下测试验证一下总是个不错的习惯。因为代码还是热的，出错也容易定位。 但是本地启动PandoraBoot可不是个省心的事，我这台2.3G双核平均也要4分钟，严重的影响了效率。所以开发了这个工具，就是等PandoraBoot启动后，将线程Hold住，然后通过Console控制台输入要测试的方法或者类。使用这个工具很简单： 依赖COLA框架 使用TestApplication启动应用 1234567public class TestApplication &#123; public static void main(String[] args) &#123; PandoraBootstrap.run(args); SpringApplication.run(Application.class, args); TestsContainer.start();//启动测试容器，避免重复启动PandoraBoot &#125;&#125; 本文转自COLA架构的设计思想和原理]]></content>
      <categories>
        <category>编程思想</category>
      </categories>
      <tags>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java 8新特性]]></title>
    <url>%2F2019%2F02%2F17%2Fjava-8%E6%96%B0%E7%89%B9%E6%80%A7%2F</url>
    <content type="text"><![CDATA[今天介绍一下java8主要的新特性，分别是默认接口方法、lambda表达式、方法引用和可重复注解等，结尾还会介绍到最新的Api，如streams、函数式接口和新的Date api。 Interfaces的默认方法java 8可以使用default关键字来给接口添加非抽象方法，如下： 1234567interface Formula &#123; double calculate(int a); default double sqrt(int a) &#123; //非抽象方法 return Math.sqrt(a); &#125;&#125; 123456789Formula formula = new Formula() &#123; @Override public double calculate(int a) &#123; return sqrt(a * 100);//sqrt方法可在内部调用 &#125;&#125;;formula.calculate(100); // 100.0formula.sqrt(16); // 4.0，sqrt方法也可在外部调用 lambda表达式java 8终于可以使用简洁的代码语法：lambda表达式，有多简洁，下面说明。 以前写一个排序的程序，需要这么写： 12345678List&lt;String&gt; names = Arrays.asList("peter", "anna", "mike", "xenia");Collections.sort(names, new Comparator&lt;String&gt;() &#123; @Override public int compare(String a, String b) &#123; return b.compareTo(a); &#125;&#125;); 使用lambda表达式后，可以修改为： 123Collections.sort(names, (String a, String b) -&gt; &#123; return b.compareTo(a);&#125;); 因为对比的逻辑比较简单，可将大括号和return关键字去掉，如下： 1Collections.sort(names, (String a, String b) -&gt; b.compareTo(a)); java编译器强大的推算能力，甚至可将参数类型去掉，如下： 1Collections.sort(names, (a, b) -&gt; b.compareTo(a)); 函数式接口java中使用lambda表达式，其根本原理是匿名内部类的语法糖，编译器根据类型上下文推断lambda语句转为对应匿名内部类的字节码，所有就需要对接口做一些限制：接口必须有且只有一个抽象方法，即后续给lambda表达式实现的方法，这种接口就叫函数式接口。 函数式接口可以添加@FunctionalInterface注解来声明自己为函数式接口，编译器编译时，如果检查到带有@FunctionalInterface注解但不满足函数式接口限制的，会编译异常报警，相当于增加多了一层检查。 1234@FunctionalInterfaceinterface Converter&lt;F, T&gt; &#123; T convert(F from);&#125; 123Converter&lt;String, Integer&gt; converter = (from) -&gt; Integer.valueOf(from);Integer converted = converter.convert("123");System.out.println(converted); // 123 方法引用java 8使用::标识符来传递方法引用，如静态方法引用： 123Converter&lt;String, Integer&gt; converter = Integer::valueOf;//Integer的静态方法valueOfInteger converted = converter.convert("123");System.out.println(converted); // 123 对象实例方法引用： 12345class Something &#123; String startsWith(String s) &#123; return String.valueOf(s.charAt(0)); &#125;&#125; 1234Something something = new Something();Converter&lt;String, String&gt; converter = something::startsWith;//实例something的方法startsWithString converted = converter.convert("Java");System.out.println(converted); // "J" 甚至还可以是构造方法引用： 1234567891011class Person &#123; String firstName; String lastName; Person() &#123;&#125; Person(String firstName, String lastName) &#123; this.firstName = firstName; this.lastName = lastName; &#125;&#125; 123interface PersonFactory&lt;P extends Person&gt; &#123;//函数式接口 P create(String firstName, String lastName);&#125; 12PersonFactory&lt;Person&gt; personFactory = Person::new;//构造方法引用Person person = personFactory.create("Peter", "Parker"); java的方法引用其实也是语法糖，并没有改变原来底层的那一套东西，也就是说底层不存在方法的实例化，这里的方法引用只是接口实例的引用，编译器做了一层转化而已。 lambda表达式的访问范围lambda表达式底层本身就是匿名内部类，所以它的访问范围和匿名内部类是一样的，只能访问外部不可变的变量。如下： 局部变量访问final声明的变量： 12345final int num = 1;Converter&lt;Integer, String&gt; stringConverter = (from) -&gt; String.valueOf(from + num);stringConverter.convert(2); // 3 不同于匿名内部类，如果编译器可以推断某个变量在上下文中是不会改变的，可以免去final的声明，如下： 12345int num = 1;Converter&lt;Integer, String&gt; stringConverter = (from) -&gt; String.valueOf(from + num);stringConverter.convert(2); // 3 但是如果变量在上下文中会变的，则会编译错误： 1234int num = 1;Converter&lt;Integer, String&gt; stringConverter = (from) -&gt; String.valueOf(from + num);//lambda中使用了numnum = 3;//改变了num，会编译不通过 成员变量和静态变量与匿名内部类相似，lambda表达式可以读写成员变量和静态变量，如： 12345678910111213141516class Lambda4 &#123; static int outerStaticNum; int outerNum; void testScopes() &#123; Converter&lt;Integer, String&gt; stringConverter1 = (from) -&gt; &#123; outerNum = 23; return String.valueOf(from); &#125;; Converter&lt;Integer, String&gt; stringConverter2 = (from) -&gt; &#123; outerStaticNum = 72; return String.valueOf(from); &#125;; &#125;&#125; 范围默认接口方法前面有个例子是，在匿名对象中可以在接口实现中调用默认方法： 123456Formula formula = new Formula() &#123; @Override public double calculate(int a) &#123; return sqrt(a * 100);//sqrt方法可在内部调用 &#125;&#125;; 如果转化成lambda表达式，是编译不通过的： 1Formula formula = (a) -&gt; sqrt( a * 100);//编译错误 内置的函数式接口Java 8内置了很多函数式接口，比如老版本的Comparator和Runnable，同时也新增了很多很好用的函数式接口。 PredicatesPredicates是一个接收一个参数并返回true或false的方法接口，也内置了一些好用的默认接口方法，如： 12345678910Predicate&lt;String&gt; predicate = (s) -&gt; s.length() &gt; 0;predicate.test("foo"); // truepredicate.negate().test("foo"); // falsePredicate&lt;Boolean&gt; nonNull = Objects::nonNull;Predicate&lt;Boolean&gt; isNull = Objects::isNull;Predicate&lt;String&gt; isEmpty = String::isEmpty;Predicate&lt;String&gt; isNotEmpty = isEmpty.negate(); FunctionsFunctions是一个接收一个参数并生成一个结果的方法接口，可以使用默认方法将多个Functions串在一起，如： 1234Function&lt;String, Integer&gt; toInteger = Integer::valueOf;Function&lt;String, String&gt; backToString = toInteger.andThen(String::valueOf);backToString.apply("123"); // "123" SuppliersSuppliers是一个不接收参数并生成一个结果的方法接口： 12Supplier&lt;Person&gt; personSupplier = Person::new;personSupplier.get(); // new Person ConsumersConsumers是一个只接收参数不返回结果的方法接口： 12Consumer&lt;Person&gt; greeter = (p) -&gt; System.out.println("Hello, " + p.firstName);greeter.accept(new Person("Luke", "Skywalker")); ComparatorsComparators接口就不用多讲了： 1234567Comparator&lt;Person&gt; comparator = (p1, p2) -&gt; p1.firstName.compareTo(p2.firstName);Person p1 = new Person("John", "Doe");Person p2 = new Person("Alice", "Wonderland");comparator.compare(p1, p2); // &gt; 0comparator.reversed().compare(p1, p2); // &lt; 0 OptionalOptional不是一个函数式接口，而是用来防止出现空指针的工具类，非常好用： 1234567Optional&lt;String&gt; optional = Optional.of("bam");optional.isPresent(); // trueoptional.get(); // "bam"optional.orElse("fallback"); // "bam"optional.ifPresent((s) -&gt; System.out.println(s.charAt(0))); // "b" 其他语言如groovy会使用?.代替.来防止方法调用空指针，而已java 8之后可以使用Optional的map方法，如： 123// company可能为null. company?.address?.street?.name //groovy的写法Optional.ofNullable(company).map(o -&gt; o.address).map(o -&gt; o.street).map(o -&gt; o.name).orElse("不存在"); //java的写法 如果用习惯了，发现会回不去了，篇幅有限，后面可以另起一篇文章详细讲解。 Streamsjava 8另一个新增的超级好用的类Streams，它可以执行一个或多个操作序列元素，Streams的操作元素可以分为中间操作和最终操作，下面简单讲解一些常用操作。（同样篇幅有限，后续另起篇幅来讲解里面的底层原理） 12345678910//创建一个string类型的listList&lt;String&gt; stringCollection = new ArrayList&lt;&gt;();stringCollection.add("ddd2");stringCollection.add("aaa2");stringCollection.add("bbb1");stringCollection.add("aaa1");stringCollection.add("bbb3");stringCollection.add("ccc");stringCollection.add("bbb2");stringCollection.add("ddd1"); FilterFilter是一个中间操作元素，用来过滤数据，如： 123456stringCollection .stream() .filter((s) -&gt; s.startsWith("a"))//只保留以a开头的字符串数据 .forEach(System.out::println);//forEach是一个最终操作元素，迭代处理输入的数据// "aaa2", "aaa1" SortedSorted是一个中间操作元素，对数据列表进行排序，如： 1234567stringCollection .stream() .sorted() //数据流排序 .filter((s) -&gt; s.startsWith("a")) .forEach(System.out::println);// "aaa1", "aaa2" Sorted操作只是修改了流的顺序，并不会影响到原数据列表： 12System.out.println(stringCollection);// ddd2, aaa2, bbb1, aaa1, bbb3, ccc, bbb2, ddd1 MapMap是一个中间操作，将原数据映射到另一种数据，如: 1234567stringCollection .stream() .map(String::toUpperCase) //将字符串转为全部大写 .sorted((a, b) -&gt; b.compareTo(a)) .forEach(System.out::println);// "DDD2", "DDD1", "CCC", "BBB3", "BBB2", "AAA2", "AAA1" MatchMatch有一系列的最终操作，any、all、none，表示匹配的判断，如： 1234567891011121314151617181920boolean anyStartsWithA = stringCollection .stream() .anyMatch((s) -&gt; s.startsWith("a")); //列表有只要有以a开头的数据，则返回trueSystem.out.println(anyStartsWithA); // trueboolean allStartsWithA = stringCollection .stream() .allMatch((s) -&gt; s.startsWith("a")); //列表中的所有数据都以a开头，才返回trueSystem.out.println(allStartsWithA); // falseboolean noneStartsWithZ = stringCollection .stream() .noneMatch((s) -&gt; s.startsWith("z")); //列表中所有数据都不是以z开头，才返回trueSystem.out.println(noneStartsWithZ); // true CountCount是一个最终操作，用来统计数据流大小： 1234567long startsWithB = stringCollection .stream() .filter((s) -&gt; s.startsWith("b")) .count();System.out.println(startsWithB); // 3 ReduceReduce是一个最终操作，返回的Optional： 12345678Optional&lt;String&gt; reduced = stringCollection .stream() .sorted() .reduce((s1, s2) -&gt; s1 + "#" + s2);reduced.ifPresent(System.out::println);// "aaa1#aaa2#bbb1#bbb2#bbb3#ccc#ddd1#ddd2" Parallel StreamsParallel Streams能使Streams在多线程上并发运行，从而大幅提升操作性能。 123456int max = 1000000;List&lt;String&gt; values = new ArrayList&lt;&gt;(max); //创建一个超大的数据列表for (int i = 0; i &lt; max; i++) &#123; UUID uuid = UUID.randomUUID(); values.add(uuid.toString());&#125; 下面对比串行排序和并发排序的用时时长： 1234567891011long t0 = System.nanoTime();long count = values.stream().sorted().count();System.out.println(count);long t1 = System.nanoTime();long millis = TimeUnit.NANOSECONDS.toMillis(t1 - t0);System.out.println(String.format("sequential sort took: %d ms", millis));// 串行排序耗时: 899 ms 1234567891011long t0 = System.nanoTime();long count = values.parallelStream().sorted().count();System.out.println(count);long t1 = System.nanoTime();long millis = TimeUnit.NANOSECONDS.toMillis(t1 - t0);System.out.println(String.format("parallel sort took: %d ms", millis));// 并行排序耗时: 472 ms MapMap是不支持转化为Streams的，但java 8版本新增了一些好用的方法，如： 1234567Map&lt;Integer, String&gt; map = new HashMap&lt;&gt;();for (int i = 0; i &lt; 10; i++) &#123; map.putIfAbsent(i, "val" + i);//如果不存在则添加&#125;map.forEach((id, val) -&gt; System.out.println(val)); //迭代处理key value 1234567891011map.computeIfPresent(3, (num, val) -&gt; val + num);//如果存在则修改valuemap.get(3); // val33map.computeIfPresent(9, (num, val) -&gt; null);map.containsKey(9); // falsemap.computeIfAbsent(23, num -&gt; "val" + num);//如果不存在则添加，比putIfAbsent方法多了key的参数输入map.containsKey(23); // truemap.computeIfAbsent(3, num -&gt; "bam");map.get(3); // val33 12345map.remove(3, "val3"); //如果key为3，value为"val3"，则删除map.get(3); // val33map.remove(3, "val33");map.get(3); // null 12//如果value为null，则返回默认值map.getOrDefault(42, "not found"); // not found 12345map.merge(9, "val9", (value, newValue) -&gt; value.concat(newValue));//如果key不存在，则添加map.get(9); // val9map.merge(9, "concat", (value, newValue) -&gt; value.concat(newValue));//如果key存在，则按合并规则合并valuemap.get(9); // val9concat Date APIJava 8出了个类似于Joda-Time的Date API。 Clock12345Clock clock = Clock.systemDefaultZone();long millis = clock.millis();Instant instant = clock.instant();Date legacyDate = Date.from(instant); // 转化为 java.util.Date 时区12345678910System.out.println(ZoneId.getAvailableZoneIds());// 打印所有合法的时区idZoneId zone1 = ZoneId.of(&quot;Europe/Berlin&quot;);ZoneId zone2 = ZoneId.of(&quot;Brazil/East&quot;);System.out.println(zone1.getRules());System.out.println(zone2.getRules());// ZoneRules[currentStandardOffset=+01:00]// ZoneRules[currentStandardOffset=-03:00] LocalTimeLocalTime只有时分秒数据，没有日期： 12345678910LocalTime now1 = LocalTime.now(zone1);//zone1时区的时分秒LocalTime now2 = LocalTime.now(zone2);//zone2时区的时分秒System.out.println(now1.isBefore(now2)); // falselong hoursBetween = ChronoUnit.HOURS.between(now1, now2);//两个时区的时差long minutesBetween = ChronoUnit.MINUTES.between(now1, now2);//两个时区的分差System.out.println(hoursBetween); // -3System.out.println(minutesBetween); // -239 12345678910LocalTime late = LocalTime.of(23, 59, 59);//of构造方法System.out.println(late); // 23:59:59DateTimeFormatter germanFormatter = DateTimeFormatter .ofLocalizedTime(FormatStyle.SHORT) .withLocale(Locale.GERMAN);LocalTime leetTime = LocalTime.parse("13:37", germanFormatter);//字符解析构造方法System.out.println(leetTime); // 13:37 LocalDateLocalDate只包含日期，不包括时分秒： 1234567LocalDate today = LocalDate.now();LocalDate tomorrow = today.plus(1, ChronoUnit.DAYS);LocalDate yesterday = tomorrow.minusDays(2);LocalDate independenceDay = LocalDate.of(2014, Month.JULY, 4);DayOfWeek dayOfWeek = independenceDay.getDayOfWeek();System.out.println(dayOfWeek); // FRIDAY 1234567DateTimeFormatter germanFormatter = DateTimeFormatter .ofLocalizedDate(FormatStyle.MEDIUM) .withLocale(Locale.GERMAN);LocalDate xmas = LocalDate.parse("24.12.2014", germanFormatter);System.out.println(xmas); // 2014-12-24 LocalDateTimeLocalDateTime包含日期和时分秒: 12345678910LocalDateTime sylvester = LocalDateTime.of(2014, Month.DECEMBER, 31, 23, 59, 59);DayOfWeek dayOfWeek = sylvester.getDayOfWeek();System.out.println(dayOfWeek); // WEDNESDAYMonth month = sylvester.getMonth();System.out.println(month); // DECEMBERlong minuteOfDay = sylvester.getLong(ChronoField.MINUTE_OF_DAY);System.out.println(minuteOfDay); // 1439 123456Instant instant = sylvester .atZone(ZoneId.systemDefault()) .toInstant();Date legacyDate = Date.from(instant);System.out.println(legacyDate); // Wed Dec 31 23:59:59 CET 2014 1234567DateTimeFormatter formatter = DateTimeFormatter .ofPattern("MMM dd, yyyy - HH:mm");//这个类是线程安全的LocalDateTime parsed = LocalDateTime.parse("Nov 03, 2014 - 07:13", formatter);String string = formatter.format(parsed);System.out.println(string); // Nov 03, 2014 - 07:13 可重复注解java 8 后可以通过@Repeatable来声明注解的重复性，如: 12345678@interface Hints &#123; Hint[] value();&#125;@Repeatable(Hints.class)@interface Hint &#123; String value();&#125; 12@Hints(&#123;@Hint("hint1"), @Hint("hint2")&#125;)//以前的写法class Person &#123;&#125; 123@Hint("hint1")@Hint("hint2")//新的写法，编译器会自动转换成上面那种写法class Person &#123;&#125; 12345678Hint hint = Person.class.getAnnotation(Hint.class);//这种方式是获取不到重复的Hint注解的System.out.println(hint); // nullHints hints1 = Person.class.getAnnotation(Hints.class);//编译器隐式创建了Hints注解System.out.println(hints1.value().length); // 2Hint[] hints2 = Person.class.getAnnotationsByType(Hint.class);//获取Hint数组System.out.println(hints2.length); // 2 此外JAVA8中的annotations新增了两个目标： 12@Target(&#123;ElementType.TYPE_PARAMETER, ElementType.TYPE_USE&#125;)@interface MyAnnotation &#123;&#125; 以上就是java 8新添加的特性及使用说明，有问题欢迎留言。]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JdbcTemplate简化数据库操作]]></title>
    <url>%2F2019%2F01%2F28%2FJdbcTemplate%E7%AE%80%E5%8C%96%E6%95%B0%E6%8D%AE%E5%BA%93%E6%93%8D%E4%BD%9C%2F</url>
    <content type="text"><![CDATA[普片来说，后端服务系统，大部分都是需要和数据库打交道的。对于简单的系统，可能数据库操作的代码编写时间就占据了整个系统开发时间的40%。频繁的数据表到对象的映射编码，或对象到数据表映射编码，大多数都是繁琐、低效的低等工作。 所以市面上都有这些问题的解决方案，如：hibernate、mybatis和JdbcTemplate等，hibernate效率低、笨重，mybatis的编写风格不太喜欢，所以笔者要说的是轻量、简洁的JdbcTemplate。 数据库操作大多数为增删改查操作，下面来讲解这些操作的编写和其中的坑。 insert个人习惯，每个表都用long类型记录创建时间和修改时间，所以insert操作基本可以用一下模板来写： 123456789101112131415161718public boolean insert(Po po) &#123; try &#123; long curTime = System.currentTimeMillis(); po.setCreateTime(curTime); po.setUpdateTime(curTime); SimpleJdbcInsert insert = new SimpleJdbcInsert(jdbcTemplate).withTableName(tableName); SqlParameterSource parameterSource = new BeanPropertySqlParameterSource(po); return insert.execute(parameterSource) &gt; 0; &#125; catch (DuplicateKeyException e) &#123; // TODO 重复插入的的处理，可以返回true，也可以抛异常，按业务需求来定 return true; &#125; catch (Exception e) &#123; // TODO 打印日志并抛异常即可 throw new RuntimeException(); &#125; &#125; insert.execute方法，无论是因为是主键重复，还是unique key重复，都会抛DuplicateKeyException异常，所以只需要catch这个异常，就可以处理重复插入的处理。使用insert.executeAndReturnKey来代替insert.execute方法，可以获取数据库自增的id。 重点来说一下BeanPropertySqlParameterSource，它会自动将对象的属性映射到数据表的字段中，底层原理并不是简单的属性转换，而是对象和表字段匹配。 12345678910111213141516171819202122232425262728293031323334353637public List&lt;Object&gt; matchInParameterValuesWithInsertColumns(SqlParameterSource parameterSource) &#123; List&lt;Object&gt; values = new ArrayList&lt;&gt;(); //去掉大小写敏感获得的map Map&lt;String, String&gt; caseInsensitiveParameterNames = SqlParameterSourceUtils.extractCaseInsensitiveParameterNames(parameterSource); for (String column : this.tableColumns) &#123; //属性名直接匹配字段名 if (parameterSource.hasValue(column)) &#123; values.add(SqlParameterSourceUtils.getTypedValue(parameterSource, column)); &#125; else &#123; //属性名和字段全小写名匹配 String lowerCaseName = column.toLowerCase(); if (parameterSource.hasValue(lowerCaseName)) &#123; values.add(SqlParameterSourceUtils.getTypedValue(parameterSource, lowerCaseName)); &#125; else &#123; //属性名匹配字段名的下划线格式 String propertyName = JdbcUtils.convertUnderscoreNameToPropertyName(column); if (parameterSource.hasValue(propertyName)) &#123; values.add(SqlParameterSourceUtils.getTypedValue(parameterSource, propertyName)); &#125; else &#123; //属性小写名和字段小写名匹配 if (caseInsensitiveParameterNames.containsKey(lowerCaseName)) &#123; values.add(SqlParameterSourceUtils.getTypedValue( parameterSource, caseInsensitiveParameterNames.get(lowerCaseName))); &#125; else &#123; //没有匹配到 values.add(null); &#125; &#125; &#125; &#125; &#125; return values;&#125; 在实验中，Po的createTime属性对应的数据表的create_time，我修改为createtime，也是没有问题的。另外，如果对象中多一个属性，并不会导致没匹配到而报错。 query数据查询可能是数据操作中用的最多的，可以用以下模板来写： 对象查询：123456789101112131415public Po query(long id) &#123; String sql = "SELECT * from " + tableName + " where id=?"; try &#123; Po po = jdbcTemplate.queryForObject(sql, new BeanPropertyRowMapper&lt;Po&gt;(Po.class), id); return po; &#125; catch (EmptyResultDataAccessException e) &#123; //TODO 查询数据库没记录的情况，可以返回空，也可以抛异常 &#125; return null;&#125; queryForObject方法有一个新人经常会踩到的坑，就是如果记录没找到，不是返回null，而是抛异常EmptyResultDataAccessException，所以这个需要自己的业务需求来对这个异常做对应的处理。 重点说明一下BeanPropertyRowMapper，它会自动将数据表字段映射到对象的属性中。底层实现中使用的是对象的setter方法设置，所以如果没写setter方法是映射不过去的。BeanPropertyRowMapper会初始化属性到字段的映射，如下： 12345678910111213141516171819protected void initialize(Class&lt;T&gt; mappedClass) &#123; this.mappedClass = mappedClass; this.mappedFields = new HashMap&lt;&gt;(); this.mappedProperties = new HashSet&lt;&gt;(); //获取setter方法的属性 PropertyDescriptor[] pds = BeanUtils.getPropertyDescriptors(mappedClass); for (PropertyDescriptor pd : pds) &#123; if (pd.getWriteMethod() != null) &#123; //属性名小写映射 this.mappedFields.put(lowerCaseName(pd.getName()), pd); //下划线映射 String underscoredName = underscoreName(pd.getName()); if (!lowerCaseName(pd.getName()).equals(underscoredName)) &#123; this.mappedFields.put(underscoredName, pd); &#125; this.mappedProperties.add(pd.getName()); &#125; &#125;&#125; mapRow方法会将字段内容写入到对象属性中，这个方法写了如下内容： 123String column = JdbcUtils.lookupColumnName(rsmd, index);String field = lowerCaseName(StringUtils.delete(column, " "));PropertyDescriptor pd = (this.mappedFields != null ? this.mappedFields.get(field) : null); 通过之前初始化的mappedFields，就可以将字段关联到属性了。 基本类型：如果查询的是基本类型，则将查询操作修改为下面的方法即可： 1Long res = jdbcTemplate.queryForObject(sql, Long.class, id); 列表查询： 1List&lt;Long&gt; res = jdbcTemplate.queryForList(sql, Long.class, id); update更新操作就简单了，笔者一般会使用updateTime作为乐观锁，所以可以使用以下模板： 1234567public boolean update(long id, int num, long updateTime) &#123; String sql = "UPDATE " + tableName + " set num=num+?,update_time=unix_timestamp(now())*1000 where id=? and update_time=?"; return jdbcTemplate.update(sql, num, id, updateTime) &gt; 0;&#125; 如果更新比较多属性，甚至是整个对象都更新的，那么可以使用NamedParameterJdbcTemplate来操作，如下： 1234String sql = "UPDATE " + tableName + " set num=num+ :num,update_time=unix_timestamp(now())*1000 where id= :id and update_time= :update_time";SqlParameterSource parameterSource = new BeanPropertySqlParameterSource(po);return namedParameterJdbcTemplate.update(sql, parameterSource) &gt; 0; delete删除操作与更新类似，模板如下： 1234567public boolean delete(long id) &#123; String sql = "DELETE FROM " + tableName + " WHERE id = ?"; return jdbcTemplate.update(sql, id) &gt; 0;&#125; 以上，就可以高效、高性能，且可以自己灵活掌控SQL的编写代码，如果您有更好的做法，也可以在下方留言。]]></content>
      <categories>
        <category>笔记</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>jdbc</tag>
        <tag>spring</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hexo中LocalSearch一直转圈问题处理]]></title>
    <url>%2F2019%2F01%2F27%2FHexo%E4%B8%ADLocalSearch%E4%B8%80%E7%9B%B4%E8%BD%AC%E5%9C%88%E9%97%AE%E9%A2%98%E5%A4%84%E7%90%86%2F</url>
    <content type="text"><![CDATA[最近使用hexo的localsearch，发现点击搜索一直load不出来，一开始还以为是网络问题，试多了几次发现还是一样，我的文章不多，localsearch的性能不至于这样吧。 网上搜索发现是非法字符的问题，用sublime打开发现多了以下红框的字符： 我们知道localsearch会最终生成一份search.xml文件，用sublime打开查看发现： XML中多了这些字符，就会解析出错，所以就一直load不出来。 删掉这些字符就可以解决问题了，如下：]]></content>
      <categories>
        <category>问题总结</category>
      </categories>
      <tags>
        <tag>hexo</tag>
        <tag>localsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[免费SSL证书申请]]></title>
    <url>%2F2019%2F01%2F27%2F%E5%85%8D%E8%B4%B9SSL%E8%AF%81%E4%B9%A6%E7%94%B3%E8%AF%B7%2F</url>
    <content type="text"><![CDATA[什么是 Let’s EncryptLet’s Encrypt 是国外一个公共的免费 SSL 项目，由 Linux 基金会托管。它的来头不小，由 Mozilla、思科、Akamai、IdenTrust 和 EFF 等组织发起，目的就是向网站自动签发和管理免费证书。以便加速互联网由 HTTP 过渡到 HTTPS，目前 Facebook 等大公司开始加入赞助行列。 Let’s Encrypt 已经得了 IdenTrust 的交叉签名，这意味着其证书现在已经可以被 Mozilla、Google、Microsoft 和 Apple 等主流的浏览器所信任。用户只需要在 Web 服务器证书链中配置交叉签名，浏览器客户端会自动处理好其它的一切，Let’s Encrypt 安装简单，使用非常方便。 2018 年 3 月 14 日，Let’s Encrypt 对外宣布 ACME v2 已正式支持通配符证书。这就意外味着用户可以在 Let’s Encrypt 上免费申请支持通配符的 SSL 证书。 什么是通配符证书域名通配符证书类似 DNS 解析的泛域名概念，通配符证书就是证书中可以包含一个通配符。主域名签发的通配符证书可以在所有子域名中使用，比如 .edjdhbb.com、www.edjdhbb.com。 申请通配符证书Let’s Encrypt 上的证书申请是通过 ACME 协议来完成的。ACME 协议规范化了证书申请、更新、撤销等流程，实现了 Let’s Encrypt CA 自动化操作。解决了传统的 CA 机构是人工手动处理证书申请、证书更新、证书撤销的效率和成本问题。 ACME v2 是 ACME 协议的更新版本，通配符证书只能通过 ACME v2 获得。要使用 ACME v2 协议申请通配符证书，只需一个支持该协议的客户端就可以了，官方推荐的客户端是 Certbot。 获取 Certbot 客户端12345# 下载 Certbot 客户端$ wget https://dl.eff.org/certbot-auto# 设为可执行权限$ chmod a+x certbot-auto 注：Certbot 从 0.22.0 版本开始支持 ACME v2，如果你之前已安装旧版本客户端程序需更新到新版本。 更详细的安装可参考官方文档：https://certbot.eff.org/ 申请通配符证书客户在申请 Let’s Encrypt 证书的时候，需要校验域名的所有权，证明操作者有权利为该域名申请证书，目前支持三种验证方式： dns-01：给域名添加一个 DNS TXT 记录。 http-01：在域名对应的 Web 服务器下放置一个 HTTP well-known URL 资源文件。 tls-sni-01：在域名对应的 Web 服务器下放置一个 HTTPS well-known URL 资源文件。 使用 Certbot 客户端申请证书方法非常的简单，只需如下一行命令就搞定了。 1$ ./certbot-auto certonly -d "*.xxx.com" --manual --preferred-challenges dns-01 --server https://acme-v02.api.letsencrypt.org/directory 1.申请通配符证书，只能使用 dns-01 的方式。2.xxx.com 请根据自己的域名自行更改。 相关参数说明： 12341. certonly 表示插件，Certbot 有很多插件。不同的插件都可以申请证书，用户可以根据需要自行选择。2. -d 为哪些主机申请证书。如果是通配符，输入 *.xxx.com (根据实际情况替换为你自己的域名)。3. --preferred-challenges dns-01，使用 DNS 方式校验域名所有权。4. --server，Let&apos;s Encrypt ACME v2 版本使用的服务器不同于 v1 版本，需要显示指定。 执行完这一步之后，就是命令行的输出，请根据提示输入相应内容： 执行到上图最后一步时，先暂时不要回车。申请通配符证书是要经过 DNS 认证的，接下来需要按照提示在域名后台添加对应的 DNS TXT 记录。添加完成后，先输入以下命令确认 TXT 记录是否生效： 12345678910$ dig -t txt _acme-challenge.xxx.com @8.8.8.8 ...;; OPT PSEUDOSECTION:; EDNS: version: 0, flags:; udp: 512;; QUESTION SECTION:;_acme-challenge.xxx.com. IN TXT;; ANSWER SECTION:_acme-challenge.xxx.com. 599 IN TXT "xxUHdwoZ6IaU_ab87h67rvbU2yJgdRyRe9zEA3jw"... 确认生效后，回车继续执行，最后会输出如下内容： 12345678910111213IMPORTANT NOTES: - Congratulations! Your certificate and chain have been saved at: /etc/letsencrypt/live/xxx.com/fullchain.pem Your key file has been saved at: /etc/letsencrypt/live/xxx.com/privkey.pem Your cert will expire on 2018-06-12. To obtain a new or tweaked version of this certificate in the future, simply run certbot-auto again. To non-interactively renew *all* of your certificates, run "certbot-auto renew" - If you like Certbot, please consider supporting our work by: Donating to ISRG / Let's Encrypt: https://letsencrypt.org/donate Donating to EFF: https://eff.org/donate-le 到了这一步后，恭喜您，证书申请成功。 证书和密钥保存在下列目录： 123456$ tree /etc/letsencrypt/live/xxx.com/.├── cert.pem├── chain.pem├── fullchain.pem└── privkey.pem 校验证书信息，输入如下命令： 1234567891011$ openssl x509 -in /etc/letsencrypt/live/xxx.com/cert.pem -noout -text # 可以看到证书包含了 SAN 扩展，该扩展的值就是 *.xxx.com...Authority Information Access: OCSP - URI:http://ocsp.int-x3.letsencrypt.org CA Issuers - URI:http://cert.int-x3.letsencrypt.org/X509v3 Subject Alternative Name: DNS:*.xxx.com... 到此，我们就演示了如何在 Let’s Encrypt 申请免费的通配符证书。 其它相关 证书续期 Let’s encrypt 的免费证书默认有效期为 90 天，到期后如果要续期可以执行： 1$ certbot-auto renew 在 Nginx 中 配置 Let’s Encrypt 证书 Nginx 配置文件片断： 123456789101112server &#123; server_name xxx.com; listen 443 http2 ssl; ssl on; ssl_certificate /etc/cert/xxx.com/fullchain.pem; ssl_certificate_key /etc/cert/xxx.com/privkey.pem; ssl_trusted_certificate /etc/cert/xxx.com/chain.pem; location / &#123; proxy_pass http://127.0.0.1:6666; &#125;&#125; 参考文档：Let’s Encrypt 免费通配符 SSL 证书申请教程]]></content>
      <categories>
        <category>笔记</category>
      </categories>
      <tags>
        <tag>SSL证书</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[markdown基本语法]]></title>
    <url>%2F2019%2F01%2F26%2Fmarkdown%E5%9F%BA%E6%9C%AC%E8%AF%AD%E6%B3%95%2F</url>
    <content type="text"><![CDATA[Markdown是什么Markdown是一种轻量级标记语言，它以纯文本形式编写文档，具有易读、易写、易更改的特性，并最终以HTML格式发布，当然你也可以导出为PDF、Word(.docx)、wiki等文件格式。 为什么选择Markdown 它是易读（看起来舒服）、易写（语法简单）、易更改纯文本，学习成本低 比起使用word更简单快速 支持跨平台使用，越来越多的网站也支持了Markdown。 更清晰的组织文档的结构 可以通过HTML和CSS扩展渲染多种样式，实现一键排版 很多markdown编辑器都可以一键导出word、pdf、html等多种文本格式 总的来说就是，markdown能让你无需花大的心思精力在排版和样式上，你专心做的就是写作的内容。 markdown编辑器 平台 编辑器 优点 Web、Android、ios 简书 简单易用可发布文章 Web、Win 小书匠 大量的扩展功能 Web、桌面全平台 Editor.md 开源 Chrome扩展 马克飞象 适合印象笔记重度用户 桌面全平台 Typora 所写即所得，功能强大外观简约 Win/Linux Moe 模仿Moe，颜值赛高 Linux Retext 即时预览 OSX Mou osx下最杰出的编辑器 桌面全平台 Atom、VS code 支持Markdown的两个代码编辑器 Android MarkdownX 手机即可编辑预览 注：笔者用的是Typora 基本语法标题两种形式：1）使用=和-标记一级和二级标题。 1234一级标题===二级标题--- 效果： 一级标题二级标题 2）使用#，可表示1-6级标题。 123456# 一级标题## 二级标题### 三级标题#### 四级标题##### 五级标题###### 六级标题 效果： 一级标题二级标题三级标题四级标题五级标题六级标题 列表使用·、+、或-标记无序列表，如： 123- 第一项- 第二项- 第三项 注意：标记后面最少有一个空格。若不在引用区块中，必须和前方段落之间存在空行。 效果： 第一项 第二项 第三项 有序列表的标记方式是将上述的符号换成数字,并辅以.，如： 1231. 第一项2. 第二项3. 第三项 效果： 第一项 第二项 第三项 如果你想要建立子列表或者列表描述的话，那就用两个空格再输入内容。 语法： 12345+ 父列表1 + 子列表1 + 子列表2+ 父列表2 这里是列表的描述内容 效果： 父列表1 子列表1 子列表2 父列表2 这里是列表的描述内容 有序列表同理： 语法： 1234567891. 文本12. 文本2 - 子文本1 - 子文本23. 文本3 1. 子文本3 2. 子文本44. 文本4 列表的描述性语言 文本1 文本2 子文本1 子文本2 文本3 子文本3 子文本4 文本4 列表的描述性语言 区块引用在段落的每行或者只在第一行使用符号&gt;,还可使用多个嵌套引用，如： 12&gt; 区块引用&gt;&gt; 嵌套引用 效果： 区块引用 嵌套引用 代码引用如果是一行或者只是一段文字里面引用部分代码，可以使用符号`括住，如： 1啊，`java`可以做很多事情。 效果： 啊，java可以做很多事情。 如果是引用一段代码，可以使用符号```括住，如：```public interface RouteDefinitionLocator { Flux getRouteDefinitions();}``` 效果： 123public interface RouteDefinitionLocator &#123; Flux&lt;RouteDefinition&gt; getRouteDefinitions();&#125; 你也可以在```后面标识语言来高亮代码，如：```javapublic interface RouteDefinitionLocator { Flux getRouteDefinitions();}``` 效果： 123public interface RouteDefinitionLocator &#123; Flux&lt;RouteDefinition&gt; getRouteDefinitions();&#125; 当然通过4个空格或tab也可以达到同样的效果，只是不能标识语言。 粗体斜体在强调内容两侧分别加上*或者_，如： 12**粗体**，**粗体***斜体*，*斜体* 效果： 粗体，粗体 斜体，斜体 分割线三个或更多-_*，必须单独一行，可含空格，如： 123---___*** 效果： 超链接超链接可以由两种形式生成：行内式、参考式和自动式。 行内式： 1[二当家的黑板报](https://www.edjdhbb.com) 效果： 二当家的黑板报 参考式： 12[二当家的黑板报][1][1]:https://www.edjdhbb.com &quot;可选标题&quot; 效果： 二当家的黑板报 自动式： 1&lt;https://www.edjdhbb.com&gt; 效果： https://www.edjdhbb.com 图片引入图片引入很简单，和超链接格式差不多，只不过在前面增加一个符号!，如： 1![二当家的头像](https://wx3.sinaimg.cn/large/006tNbRwgy1fy7jd3rqpqj308k08kq30.jpg) 效果： 反斜杠\相当于反转义作用，使符号成为普通符号。 表格冒号表示左对齐、右对齐或居中，如下： 123| 博客 | 平台 | 链接 | 推荐程度 || -------- | :----------- | ----: | :----: || 二当家的黑板报 | 自有 | https://www.edjdhbb.com | 满分 | 效果： 博客 平台 链接 推荐程度 二当家的黑板报 自有 https://www.edjdhbb.com 满分 tipsmarkdown语法简单好看，很多平台都支持了markdown语法的，如：GitHub、简书、Stack Overflow、CSDN等，一些不支持的平台，如：微信公众号、知乎等，则可以通过一些在线工具(markdown转公众号)，一键转换过去。]]></content>
      <categories>
        <category>笔记</category>
      </categories>
      <tags>
        <tag>markdown</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[if __name__ == '__main__'的作用]]></title>
    <url>%2F2019%2F01%2F25%2Fif-name-main-%E7%9A%84%E4%BD%9C%E7%94%A8%2F</url>
    <content type="text"><![CDATA[程序入口 对于很多编程语言来说，程序都必须要有一个入口，比如C，C++，以及完全面向对象的编程语言Java，C#等。如果你接触过这些语言，对于程序入口这个概念应该很好理解，C，C++都需要有一个main函数作为程序的入口，也就是程序的运行会从main函数开始。同样，Java，C#必须要有一个包含Main方法的主类，作为程序入口。 而Python则不同，它属于脚本语言，不像编译型语言那样先将程序编译成二进制再运行，而是动态的逐行解释运行。也就是从脚本第一行开始运行，没有统一的入口。 在Python，我们经常会编写 1if __name__ == &apos;__main__&apos; 这么一段代码，这段代码该怎么来理解？ 这段代码的功能理解如下： 一个python的文件有两种使用的方法： 作用一，直接作为脚本执行。 作用二，import到其他的python脚本中被调用（模块重用）执行。 if __name__ == &#39;__main__&#39;: 的作用就是控制这两种情况执行代码的过程，在if __name__ == &#39;__main__&#39;:下的代码只有在第一种情况下（即文件作为脚本直接执行）才会被执行，而import到其他脚本中是不会被执行的。 运行原理 每个python模块（python文件）都包含内置的变量__name__，当运行模块被执行的时候，__name__等于文件名（包含了后缀.py）。如果import到其他模块中，则__name__等于模块名称（不包含后缀.py）。而__main__等于当前执行文件的名称（包含了后缀.py）。所以当模块被直接执行时，__name__ == &#39;__main__&#39;结果为真；而当模块被import到其他模块中时，__name__ == &#39;__main__&#39;结果为假，就是不调用对应的方法。 简而言之就是：__name__ 是当前模块名，当模块被直接运行时模块名为__main__ 。当模块被直接运行时，代码将被运行，当模块是被导入时，代码不被运行。]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[jvm监控工具]]></title>
    <url>%2F2019%2F01%2F25%2Fjvm%E7%9B%91%E6%8E%A7%E5%B7%A5%E5%85%B7%2F</url>
    <content type="text"><![CDATA[1. jps（JVM Process Status Tools）jps是参照Unix系统的取名规则命名的，而他的功能和ps的功能类似，可以列举正在运行的饿虚拟机进程并显示虚拟机执行的主类以及这些进程的唯一ID（LVMID，对应本机来说和PID相同），他的用法如下： jps [option] [hostid] 其中hostid默认为本机，而option选项包含以下选项 Option Function -q 只输出LVMID -m 输出JVM启动时传给主类的方法 -l 输出主类的全名，如果是Jar则输出jar的路径 -v 输出JVM的启动参数 2. jstat（JVM Statistics Monitoring Tools）jstat主要用于监控虚拟机的各种运行状态信息，如类的装载、内存、垃圾回收、JIT编译器等，在没有GUI的服务器上，这款工具是首选的一款监控工具。其用法如下： jstat [option vmid [interval [s|ms] [vount] ] ] 参数interval和count分别表示查询间隔和查询次数，如每1毫秒查询一次进程20445的垃圾回收情况，监控20次，命令如下所示： jstat -gc 20445 1 20 相关的输出参数介绍可参照官方的说明（注：网址链接请点击此处） 选项option代表用户需要查询的虚拟机的信息，主要分为3类：类装载、垃圾回收和运行期的编译情况，具体如下表所示： Option Function -class 监视类的装载、卸载数量以及类的装载总空间和耗费时间等 -gc 监视Java堆，包含eden、2个survivor区、old区和永久带区域的容量、已用空间、GC时间合计等信息 -gccapcity 监视内容与-gc相同，但输出主要关注Java区域用到的最大和最小空间 -gcutil 监视内容与-gc相同，但输出主要关注已使用空间占总空间的百分比 -gccause 与-gcutil输出信息相同，额外输出导致上次GC产生的原因 -gcnew 监控新生代的GC情况 -gcnewcapacity 与-gcnew监控信息相同，输出主要关注使用到的最大和最小空间 -gcold 监控老生代的GC情况 -gcoldcapacity 与-gcold监控信息相同，输出主要关注使用到的最大和最小空间 -gcpermcapacity 输出永久带用到的最大和最小空间 -compiler 输出JIT编译器编译过的方法、耗时信息 -printcompilation 输出已经被JIT编译的方法 3. jinfo（JVM configuration Info for Java）Jinfo的作用是实时查看虚拟机的各项参数信息jps –v可以查看虚拟机在启动时被显式指定的参数信息，但是如果你想知道默认的一些参数信息呢？除了去查询对应的资料以外，jinfo就显得很重要了。jinfo的用法如下： Jinfo [option] pid 如 jinfo –sysprops {pid} 4. jmap（JVM Memory Map for Java）jmap用于生成堆快照（heapdump）。当然我们有很多方法可以取到对应的dump信息，如我们通过JVM启动时加入启动参数 –XX:HeapDumpOnOutOfMemoryError参数，可以让JVM在出现内存溢出错误的时候自动生成dump文件，亦可以通过-XX:HeapDumpOnCtrlBreak参数，在运行时使用ctrl+break按键生成dump文件，当然我们也可以使用kill -3 pid的方式去恐吓JVM生成dump文件。jmap的作用不仅仅是为了获取dump文件，还可以用于查询finalize执行队列、Java堆和永久带的详细信息，如空间使用率、垃圾回收器等。其运行格式如下： jmap [option] vmip Option的信息如下表所示 Option Function -dump 生成对应的dump信息，用法为-dump:[live,]format=b,file={fileName} -finalizerinfo 显示在F-Queue中等待的Finalizer方法的对象（只在linux下生效） -heap 显示堆的详细信息、垃圾回收器信息、参数配置、分代详情等 -histo 显示堆栈中的对象的统计信息，包含类、实例数量和合计容量 -permstat 以ClassLoder为统计口径显示永久带的内存状态 -F 当虚拟机对-dump无响应时可使用这个选项强制生成dump快照 示例：jmap -dump:format=b,file=heap.dump 20445 5. jhat（JVM Heap Analysis Tool）jhat是用来分析dump文件的一个微型的HTTP/HTML服务器，它能将生成的dump文件生成在线的HTML文件，让我们可以通过浏览器进行查阅，然而实际中我们很少使用这个工具，因为一般服务器上设置的堆、栈内存都比较大，生成的dump也比较大，直接用jhat容易造成内存溢出，而是我们大部分会将对应的文件拷贝下来，通过其他可视化的工具进行分析。启用法如下： jhat {dump_file} 执行命令后，我们看到系统开始读取这段dump信息，当系统提示Server is ready的时候，用户可以通过在浏览器键入 http://ip:7000 进行查询 6. jstack（JVM Stack Trace for java）jstack用于JVM当前时刻的线程快照，又称threaddump文件，它是JVM当前每一条线程正在执行的堆栈信息的集合。生成线程快照的主要目的是为了定位线程出现长时间停顿的原因，如线程死锁、死循环、请求外部时长过长导致线程停顿的原因。通过jstack我们就可以知道哪些进程在后台做些什么？在等待什么资源等！其运行格式如下： jstack [option] vmid 相关的option和function如下表所示 Option Function -F 当正常输出的请求不响应时强制输出线程堆栈 -l 除堆栈信息外，显示关于锁的附加信息 -m 显示native方法的堆栈信息 示例：jstack -l 20445 7. 导出GC日志可以通过在java命令种加入参数来指定对应的gc类型，打印gc日志信息并输出至文件等策略。 注： GC的日志是以替换的方式(&gt;)写入的，而不是追加(&gt;&gt;)，如果下次写入到同一个文件中的话，以前的GC内容会被清空。 对应的参数列表 -XX:+PrintGC 输出GC日志 -XX:+PrintGCDetails 输出GC的详细日志 -XX:+PrintGCTimeStamps 输出GC的时间戳（以基准时间的形式） -XX:+PrintGCDateStamps 输出GC的时间戳（以日期的形式，如 2013-05-04T21:53:59.234+0800） -XX:+PrintHeapAtGC 在进行GC的前后打印出堆的信息 -Xloggc:../logs/gc.log 日志文件的输出路径 这里使用如下的参数来进行日志的打印： -XX:+PrintGCDateStamps -XX:+PrintGCDetails -Xloggc:./gclog 可以观察到gclog文件打印出一下日志： 1232019-02-01T16:10:27.433+0800: 7.910: [GC (Allocation Failure) [PSYoungGen: 325632K-&gt;2592K(331776K)] 356991K-&gt;33959K(398848K), 0.0070445 secs] [Times: user=0.02 sys=0.00, real=0.00 secs]2019-02-01T16:10:28.000+0800: 8.477: [GC (Allocation Failure) [PSYoungGen: 328224K-&gt;5344K(330752K)] 359591K-&gt;36720K(397824K), 0.0090341 secs] [Times: user=0.03 sys=0.00, real=0.01 secs]2019-02-01T16:10:28.861+0800: 9.339: [GC (Allocation Failure) [PSYoungGen: 329952K-&gt;6640K(331264K)] 361328K-&gt;39183K(398336K), 0.0110267 secs] [Times: user=0.04 sys=0.00, real=0.02 secs] 日志具体含义是，PSYoungGen的回收前的大小 -&gt; 回收后的大小(总大小)，整个堆回收前的大小 -&gt; 整个堆回收后的大小(总大小)，回收时间，[用户耗时，系统耗时，真实耗时] 也可以使用gc日志离线分析工具，如：sun的gchisto、GCLogViewer、HPjmeter、gcviewer等。]]></content>
      <categories>
        <category>笔记</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>linux</tag>
        <tag>jvm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[命令压缩解压]]></title>
    <url>%2F2019%2F01%2F25%2F%E5%91%BD%E4%BB%A4%E5%8E%8B%E7%BC%A9%E8%A7%A3%E5%8E%8B%2F</url>
    <content type="text"><![CDATA[*.tar *解包：tar xvf FileName.tar打包：tar cvf FileName.tar DirName（注：tar是打包，不是压缩！） .gz解压1：gunzip FileName.gz解压2：gzip -d FileName.gz压缩：gzip FileName .tar.gz 和 .tgz解压：tar zxvf FileName.tar.gz压缩：tar zcvf FileName.tar.gz DirName .bz2解压1：bzip2 -d FileName.bz2解压2：bunzip2 FileName.bz2压缩： bzip2 -z FileName .tar.bz2解压：tar jxvf FileName.tar.bz2压缩：tar jcvf FileName.tar.bz2 DirName .bz解压1：bzip2 -d FileName.bz解压2：bunzip2 FileName.bz压缩：未知 .tar.bz解压：tar jxvf FileName.tar.bz压缩：未知 .Z解压：uncompress FileName.Z压缩：compress FileName .tar.Z解压：tar Zxvf FileName.tar.Z压缩：tar Zcvf FileName.tar.Z DirName .zip解压：unzip FileName.zip压缩：zip FileName.zip DirName .rar解压：rar x FileName.rar压缩：rar a FileName.rar DirName .lha解压：lha -e FileName.lha压缩：lha -a FileName.lha FileName .rpm解包：rpm2cpio FileName.rpm | cpio -div .deb解包：ar p FileName.deb data.tar.gz | tar zxf - .tar .tgz .tar.gz .tar.Z .tar.bz .tar.bz2 .zip .cpio .rpm .deb .slp .arj .rar .ace .lha .lzh .lzx .lzs .arc .sda .sfx .lnx .zoo .cab .kar .cpt .pit .sit .sea解压：sEx x FileName.*压缩：sEx a FileName.* FileNamesEx只是调用相关程序，本身并无压缩、解压功能，请注意！ *gzip 命令详解 *减少文件大小有两个明显的好处，一是可以减少存储空间，二是通过网络传输文件时，可以减少传输的时间。gzip 是在 Linux 系统中经常使用的一个对文件进行压缩和解压缩的命令，既方便又好用。语法：gzip [选项] 压缩（解压缩）的文件名该命令的各选项含义如下： -c 将输出写到标准输出上，并保留原有文件。 -d 将压缩文件解压。 -l 对每个压缩文件，显示下列字段： 压缩文件的大小；未压缩文件的大小；压缩比；未压缩文件的名字 -r 递归式地查找指定目录并压缩其中的所有文件或者是解压缩。 -t 测试，检查压缩文件是否完整。 -v 对每一个压缩和解压的文件，显示文件名和压缩比。 -num 用指定的数字 num 调整压缩的速度， -1 或 –fast 表示最快压缩方法（低压缩比），-9 或–best表示最慢压缩方法（高压缩比）。系统缺省值为 6。 指令实例：gzip *% 把当前目录下的每个文件压缩成 .gz 文件。gzip -dv *% 把当前目录下每个压缩的文件解压，并列出详细的信息。gzip -l *% 详细显示例1中每个压缩的文件的信息，并不解压。gzip usr.tar% 压缩 tar 备份文件 usr.tar，此时压缩文件的扩展名为.tar.gz。]]></content>
      <categories>
        <category>笔记</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>压缩解压</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mysql基本修改语句]]></title>
    <url>%2F2019%2F01%2F25%2Fmysql%E5%9F%BA%E6%9C%AC%E4%BF%AE%E6%94%B9%E8%AF%AD%E5%8F%A5%2F</url>
    <content type="text"><![CDATA[添加唯一索引： 12ALTER TABLE 表名 ADD UNIQUE INDEX 索引名 (字段)ALTER TABLE 表名 ADD UNIQUE INDEX 索引名 (字段, 字段) 添加索引： 12ALTER TABLE 表名 ADD INDEX 索引名 (字段)ALTER TABLE 表名 ADD INDEX 索引名 (字段, 字段) 删除索引： 1ALTER TABLE 表名 DROP INDEX 索引名 修改表名： 1ALTER TABLE 旧表名 RENAME 新表名 删除表： 1DROP TABLE 表名 添加一个字段： 12alter table user add COLUMN new1 VARCHAR(20) DEFAULT NULL; //增加一个字段，默认为空alter table user add COLUMN new2 VARCHAR(20) NOT NULL; //增加一个字段，默认不能为空 删除一个字段 1alter table user DROP COLUMN new2; //删除一个字段 修改一个字段 12alter table user MODIFY new1 VARCHAR(10); //修改一个字段的类型alter table user CHANGE new1 new4 int; //修改一个字段的名称，此时一定要重新指定该字段的类型]]></content>
      <categories>
        <category>笔记</category>
      </categories>
      <tags>
        <tag>mysql</tag>
        <tag>数据库</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[对比java 8，java 11到底有多快]]></title>
    <url>%2F2019%2F01%2F22%2F%E5%AF%B9%E6%AF%94java-8-java-11%E5%88%B0%E5%BA%95%E6%9C%89%E5%A4%9A%E5%BF%AB%2F</url>
    <content type="text"><![CDATA[转自：开源中国（属于又想骗我升级系列文章，哈哈） OptaPlanner 官网发布了一个 Java 11 GC 性能基准测试报告。 当前使用量最大的 Java 版本是 8，所以测试者用 Java 8 与 Java 11 进行对比测试。GC 是影响 Java 性能的关键因素，所以测试自然也基于 GC，在 G1 GC 和并行 GC 下分别进行了测试，结果如下： Java 8 vs. Java 11 使用 G1 GC G1 GC 下每秒分值： Java 11 在几乎所有测试数据集上都有速度上的提升。平均而言，仅通过切换到 Java 11 就有 16％ 的改进，这种改进可能是因为 Java 10 中引入了 JEP 307: Parallel Full GC for G1。 使用并行 GC 并行 GC 下每秒分值： 使用并行 GC，结果不如 G1，某些数据集上有所改进，但其它数据集保持不变甚至出现性能下降。平均而言，Java 11 的性能提升了 4％ 以上。 测试者还在 Java 11 上对并行 GC 与 G1 GC 进行对比： Java 11 上并行 GC vs. G1 GC 结果表明 G1 GC 整体上不如并行 GC。 OptaPlanner 表示，从 Java 8 到 Java 11，G1 GC 的平均速度改进为 16.1％，并行 GC 为 4.5％。此外虽然并行 GC 面向吞吐量，而 G1 则侧重于低延迟 GC，但是 Java 11 中带来的 G1 显著改进，使得将两者进行直接比较是有意义的。此外，基于基准测试中的大多数数据集来看，并行 GC 还是更适合 OptaPlanner 的，因为吞吐量对于解决 OptaPlanner 的优化问题更为重要。 详细测试基准与过程查看原文： https://www.optaplanner.org/blog/2019/01/17/HowMuchFasterIsJava11.html]]></content>
      <categories>
        <category>行业资讯</category>
      </categories>
      <tags>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Microservice Gateway Comparison]]></title>
    <url>%2F2019%2F01%2F15%2FMicroservice-Gateway-Comparison%2F</url>
    <content type="text"><![CDATA[Recently, due to the needs of new projects, the technology stack selected by our back-end is spring cloud micro-service, which needs to be selected for the gateway, so we have the performance test analysis of this variety of gateways. Testing Machine Using a 4-core 4G memory virtual machine, the system is Ubuntu18 Wrk testThe following are 10 threads, 200 concurrent, lasting 30 seconds, test results for different gateways. (wrk uses the http1.1 version) Direct connection Nginx zuul（After warming up），Timeout situation from time to time gateway（After warming up） linkerd（After warming up） Test result analysis When there is no gateway and directly connect to the service, 22,433 requests can be processed per second, and the average delay of each thread is 13.25ms. When using the Nginx proxy, it can process 12507 requests per second, which is about 56% of the direct connection, with an average delay of 75.43ms. After using the warm-up zuul, it can process 6517 requests per second, which is about 29% of the direct connection, with an average delay of 105.06ms; multiple tests, occasionally there will be 3 or 4 timeout requests. Using the warm-up gateway, it can process 9228 requests per second, which is about 41% of the direct connection, and the average delay is 21.83ms. Using the linkerd after warm-up, it can process 9999 requests per second, which is about 44% of the direct connection, and the average delay is 20.35ms. Ab test for http1.0The following uses ab to do 200 concurrency, 50,000 request tests: Direct connection Nginx zuul(After warming up) gateway（After warming up） linkerd(After warming up) Test result analysisThere is a post on the Internet with ab test, the test to the gateway performance is very poor, the official has been checked is ab used http1.0, and the bottom layer reactor-netty does not support http1.0, this problem has been fixed. https://cloud.tencent.com/developer/article/1083254 https://github.com/reactor/reactor-netty/issues/21 Direct connection, 50,000 requests, total time 2.748 seconds, processing 18,195 requests per second, an average of 11ms a request Nginx, the total time is 5.021 seconds, processing 9,0 requests per second, about 55% of the direct connection, an average of 20ms a request After the warm-up zuul, the total time is 7.862 seconds, processing 6360 requests per second, about 35% of the direct connection, an average of 31ms a request After warming up the gateway, the total time is 6.762 seconds, processing 7394 requests per second, about 41% of the direct connection, an average of 27ms a request After the warm-up linkerd, the total time is 6.972 seconds, processing 7171 requests per second, about 39% of the direct connection, an average of 28ms a request Simple summary According to the test situation, the performance of Nginx in a variety of gateways is undoubtedly the best, whether it is Taobao’s Tengine feature enhancement, or OpenResty’s module enhancements, are optimized for Nginx. However, from the perspective of positioning, I tend to position it as a traditional tool that only carries request forwarding, which is not so friendly to the technology stack we developed. In addition, OpenResty has a lua-resty-mysql module that can be used as a gateway to mysql. We have never used it and don’t know how it performs. Both zuul and gateway can seamlessly connect to the spring cloud. Combined with service discovery, the service layer can shield machine resources (ip or internal and external network domain names, etc.), and the service has no dependence on the machine. Their positioning is a gateway that is integrated into the microservices and carries the enhanced functionality of the microservices. For the second development, these two are easier for us to develop. Compared with zuul and gateway, according to the test situation, the performance of the gateway is still much better than zuul. In addition, the gateway also has a lot of features that zuul does not have, such as support for http2, websocket, according to domain name forwarding. For the high performance requirements of service forwarding, rather than database forwarding, the performance and Nginx are not much different. Linkerd is the concept of service grid. The principle of linkerd is system proxy request, which is non-intrusive to the service and has a mature monitoring management interface. The official test of gateway is that gateway performance is much better than linkerd, but testing on my virtual machine is similar. Linkerd is used in conjunction with docker and k8s, and the abstraction of machine resources goes one step further. Linkerd is more like TCP/IP between applications or microservices. Network monitoring, current limiting, and blowing are transparent to the service layer. Above, for each gateway, individuals feel that they are not the same dimension, but can choose the corresponding gateway according to their own needs, technology, and so on. original text]]></content>
      <categories>
        <category>技术测评</category>
      </categories>
      <tags>
        <tag>gateway</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Is your httpclient really used correctly?]]></title>
    <url>%2F2019%2F01%2F15%2FIs-your-httpclient-really-used-correctly%2F</url>
    <content type="text"><![CDATA[When I used java to develop a toolkit like scrapy, I used httpclient to make a network request. I encountered the problem of requesting an infinite card death. I took out the solution today and avoided other people stepping on the pit. Questions are as follows：1RequestConfig.custom().setSocketTimeout(SO_TIME_OUT).setConnectTimeout(CONNECTION_TIME_OUT).setConnectionRequestTimeout(CONNECTION_REQUEST_TIME_OUT) In the regular timeout configuration such as socketTimeout, connectTimeout and connectionRequestTimeout, in the case of large concurrency, some requests will be stuck in the java.net.SocketInputStream.socketRead0 method from time to time. The information dumped is as follows: 1234567891011121314151617181920212223"pool-2-thread-87" #202 prio=5 os_prio=0 tid=0x00007f52603a8000 nid=0x6672 runnable [0x00007f51888c6000] java.lang.Thread.State: RUNNABLE at java.net.SocketInputStream.socketRead0(Native Method) at java.net.SocketInputStream.socketRead(SocketInputStream.java:116) at java.net.SocketInputStream.read(SocketInputStream.java:171) at java.net.SocketInputStream.read(SocketInputStream.java:141) at org.apache.http.impl.io.SessionInputBufferImpl.streamRead(SessionInputBufferImpl.java:137) at org.apache.http.impl.io.SessionInputBufferImpl.fillBuffer(SessionInputBufferImpl.java:153) at org.apache.http.impl.io.SessionInputBufferImpl.readLine(SessionInputBufferImpl.java:282) at org.apache.http.impl.conn.DefaultHttpResponseParser.parseHead(DefaultHttpResponseParser.java:138) at org.apache.http.impl.conn.DefaultHttpResponseParser.parseHead(DefaultHttpResponseParser.java:56) at org.apache.http.impl.io.AbstractMessageParser.parse(AbstractMessageParser.java:259) at org.apache.http.impl.DefaultBHttpClientConnection.receiveResponseHeader(DefaultBHttpClientConnection.java:163) at org.apache.http.protocol.HttpRequestExecutor.doReceiveResponse(HttpRequestExecutor.java:273) at org.apache.http.protocol.HttpRequestExecutor.execute(HttpRequestExecutor.java:125) at org.apache.http.impl.execchain.MainClientExec.createTunnelToTarget(MainClientExec.java:486) at org.apache.http.impl.execchain.MainClientExec.establishRoute(MainClientExec.java:411) at org.apache.http.impl.execchain.MainClientExec.execute(MainClientExec.java:237) at org.apache.http.impl.execchain.ProtocolExec.execute(ProtocolExec.java:185) at org.apache.http.impl.execchain.RetryExec.execute(RetryExec.java:89) at org.apache.http.impl.execchain.RedirectExec.execute(RedirectExec.java:110) at org.apache.http.impl.client.InternalHttpClient.doExecute(InternalHttpClient.java:185) at org.apache.http.impl.client.CloseableHttpClient.execute(CloseableHttpClient.java:83) Check if you are stepping on the pitYou can run the command: jstack -l ${pid} | grep java.net.SocketInputStream.socketRead0If multiple dump messages have the same thread stack, you can determine that this problem also exists for your service. solution If it is https, and the version of httpclient is version 4.3.5, upgrade the httpclient version. This is a bug of httpclient. For details, see https://issues.apache.org/jira/browse/HTTPCLIENT-1589, this is the first possible solution. If the first one cannot be solved, it is basically this possibility. I am using httpclient proxy request, after numerous debugs, I found the proxy request in the TUNNEL_TARGET step, the connection used by the DefaultBHttpClientConnection binding, using the SocketConfig configuration, not RequestConfig, so without setting SocketConfig The socket’s ocketRead0 method is infinitely waiting, causing the thread to be stuck. Add the following configuration: 1connectionManager.setSocketConfig(SocketConfig.custom().setSoTimeout(SO_TIME_OUT).build()); In addition, any use of socket, you may need to pay attention to whether the socket timeout is set, otherwise it will always appear socketRead0. original text]]></content>
      <categories>
        <category>问题总结</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>httpclient</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[阿里开源的分布式事务解决方案-Fescar]]></title>
    <url>%2F2019%2F01%2F14%2F%E9%98%BF%E9%87%8C%E5%BC%80%E6%BA%90%E7%9A%84%E5%88%86%E5%B8%83%E5%BC%8F%E4%BA%8B%E5%8A%A1%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88-Fescar%2F</url>
    <content type="text"><![CDATA[Fescar 是 阿里巴巴 开源的 分布式事务中间件，以 高效 并且对业务 0 侵入 的方式，解决 微服务 场景下面临的分布式事务问题。 1. 什么是微服务化带来的分布式事务问题？首先，设想一个传统的单体应用（Monolithic App），通过 3 个 Module，在同一个数据源上更新数据来完成一项业务。 很自然的，整个业务过程的数据一致性由本地事务来保证。 随着业务需求和架构的变化，单体应用被拆分为微服务：原来的 3 个 Module 被拆分为 3 个独立的服务，分别使用独立的数据源（Pattern: Database per service）。业务过程将由 3 个服务的调用来完成。 此时，每一个服务内部的数据一致性仍由本地事务来保证。而整个业务层面的全局数据一致性要如何保障呢？这就是微服务架构下面临的，典型的分布式事务需求：我们需要一个分布式事务的解决方案保障业务全局的数据一致性。 2. Fescar 的发展历程阿里是国内最早一批进行应用分布式（微服务化）改造的企业，所以很早就遇到微服务架构下的分布式事务问题。 2014 年，阿里中间件团队发布 TXC（Taobao Transaction Constructor），为集团内应用提供分布式事务服务。 2016 年，TXC 经过产品化改造，以 GTS（Global Transaction Service） 的身份登陆阿里云，成为当时业界唯一一款云上分布式事务产品，在阿云里的公有云、专有云解决方案中，开始服务于众多外部客户。 2019 年起，基于 TXC 和 GTS 的技术积累，阿里中间件团队发起了开源项目 Fescar（Fast &amp; EaSy Commit And Rollback, FESCAR），和社区一起建设这个分布式事务解决方案。 TXC/GTS/Fescar 一脉相承，为解决微服务架构下的分布式事务问题交出了一份与众不同的答卷。 2.1 设计初衷高速增长的互联网时代，快速试错 的能力对业务来说是至关重要的： 一方面，不应该因为技术架构上的微服务化和分布式事务支持的引入，给业务层面带来额外的研发负担。 另一方面，引入分布式事务支持的业务应该基本保持在同一量级上的性能表现，不能因为事务机制显著拖慢业务。 基于这两点，我们设计之初的最重要的考量就在于： 对业务无侵入： 这里的 侵入 是指，因为分布式事务这个技术问题的制约，要求应用在业务层面进行设计和改造。这种设计和改造往往会给应用带来很高的研发和维护成本。我们希望把分布式事务问题在 中间件 这个层次解决掉，不要求应用在业务层面做额外的工作。 高性能： 引入分布式事务的保障，必然会有额外的开销，引起性能的下降。我们希望把分布式事务引入的性能损耗降到非常低的水平，让应用不因为分布式事务的引入导致业务的可用性受影响。 2.2 既有的解决方案为什么不满足？既有的分布式事务解决方案按照对业务侵入性分为两类，即：对业务无侵入的和对业务有侵入的。 业务无侵入的方案既有的主流分布式事务解决方案中，对业务无侵入的只有基于 XA 的方案，但应用 XA 方案存在 3 个方面的问题： 要求数据库提供对 XA 的支持。如果遇到不支持 XA（或支持得不好，比如 MySQL 5.7 以前的版本）的数据库，则不能使用。 受协议本身的约束，事务资源的锁定周期长。长周期的资源锁定从业务层面来看，往往是不必要的，而因为事务资源的管理器是数据库本身，应用层无法插手。这样形成的局面就是，基于 XA 的应用往往性能会比较差，而且很难优化。 已经落地的基于 XA 的分布式解决方案，都依托于重量级的应用服务器（Tuxedo/WebLogic/WebSphere 等)，这是不适用于微服务架构的。 侵入业务的方案实际上，最初分布式事务只有 XA 这个唯一方案。XA 是完备的，但在实践过程中，由于种种原因（包含但不限于上面提到的 3 点）往往不得不放弃，转而从业务层面着手来解决分布式事务问题。比如： 基于可靠消息的最终一致性方案 TCC Saga 都属于这一类。这些方案的具体机制在这里不做展开，网上这方面的论述文章非常多。总之，这些方案都要求在应用的业务层面把分布式事务技术约束考虑到设计中，通常每一个服务都需要设计实现正向和反向的幂等接口。这样的设计约束，往往会导致很高的研发和维护成本。 2.3 理想的方案应该是什么样子？不可否认，侵入业务的分布式事务方案都经过大量实践验证，能有效解决问题，在各行种业的业务应用系统中起着重要作用。但回到原点来思考，这些方案的采用实际上都是 迫于无奈。设想，如果基于 XA 的方案能够不那么 重，并且能保证业务的性能需求，相信不会有人愿意把分布式事务问题拿到业务层面来解决。 一个理想的分布式事务解决方案应该：像使用 本地事务 一样简单，业务逻辑只关注业务层面的需求，不需要考虑事务机制上的约束。 3. 原理和设计我们要设计一个对业务无侵入的方案，所以从业务无侵入的 XA 方案来思考： 是否可以在 XA 的基础上演进，解决掉 XA 方案面临的问题呢？ 3.1 如何定义一个分布式事务？首先，很自然的，我们可以把一个分布式事务理解成一个包含了若干 分支事务 的 全局事务。全局事务 的职责是协调其下管辖的 分支事务 达成一致，要么一起成功提交，要么一起失败回滚。此外，通常 分支事务 本身就是一个满足 ACID 的 本地事务。这是我们对分布式事务结构的基本认识，与 XA 是一致的。 其次，与 XA 的模型类似，我们定义 3 个组件来协议分布式事务的处理过程。 Transaction Coordinator (TC)： 事务协调器，维护全局事务的运行状态，负责协调并驱动全局事务的提交或回滚。 Transaction Manager (TM)： 控制全局事务的边界，负责开启一个全局事务，并最终发起全局提交或全局回滚的决议。 Resource Manager (RM)： 控制分支事务，负责分支注册、状态汇报，并接收事务协调器的指令，驱动分支（本地）事务的提交和回滚。 一个典型的分布式事务过程： TM 向 TC 申请开启一个全局事务，全局事务创建成功并生成一个全局唯一的 XID。 XID 在微服务调用链路的上下文中传播。 RM 向 TC 注册分支事务，将其纳入 XID 对应全局事务的管辖。 TM 向 TC 发起针对 XID 的全局提交或回滚决议。 TC 调度 XID 下管辖的全部分支事务完成提交或回滚请求。 至此，Fescar 的协议机制总体上看与 XA 是一致的。 3.2 与 XA 的差别在什么地方？架构层次 XA 方案的 RM 实际上是在数据库层，RM 本质上就是数据库自身（通过提供支持 XA 的驱动程序来供应用使用）。 而 Fescar 的 RM 是以二方包的形式作为中间件层部署在应用程序这一侧的，不依赖与数据库本身对协议的支持，当然也不需要数据库支持 XA 协议。这点对于微服务化的架构来说是非常重要的：应用层不需要为本地事务和分布式事务两类不同场景来适配两套不同的数据库驱动。 这个设计，剥离了分布式事务方案对数据库在 协议支持 上的要求。 两阶段提交先来看一下 XA 的 2PC 过程。 无论 Phase2 的决议是 commit 还是 rollback，事务性资源的锁都要保持到 Phase2 完成才释放。 设想一个正常运行的业务，大概率是 90% 以上的事务最终应该是成功提交的，我们是否可以在 Phase1 就将本地事务提交呢？这样 90% 以上的情况下，可以省去 Phase2 持锁的时间，整体提高效率。 这个设计，在绝大多数场景减少了事务持锁时间，从而提高了事务的并发度。 当然，你肯定会问：Phase1 即提交的情况下，Phase2 如何回滚呢？ 3.3 分支事务如何提交和回滚？首先，应用需要使用 Fescar 的 JDBC 数据源代理，也就是 Fescar 的 RM。 Phase1： Fescar 的 JDBC 数据源代理通过对业务 SQL 的解析，把业务数据在更新前后的数据镜像组织成回滚日志，利用 本地事务 的 ACID 特性，将业务数据的更新和回滚日志的写入在同一个 本地事务 中提交。 这样，可以保证：任何提交的业务数据的更新一定有相应的回滚日志存在。 基于这样的机制，分支的本地事务便可以在全局事务的 Phase1 提交，马上释放本地事务锁定的资源。 Phase2： 如果决议是全局提交，此时分支事务此时已经完成提交，不需要同步协调处理（只需要异步清理回滚日志），Phase2 可以非常快速地完成。 如果决议是全局回滚，RM 收到协调器发来的回滚请求，通过 XID 和 Branch ID 找到相应的回滚日志记录，通过回滚记录生成反向的更新 SQL 并执行，以完成分支的回滚。 3.4 事务传播机制XID 是一个全局事务的唯一标识，事务传播机制要做的就是把 XID 在服务调用链路中传递下去，并绑定到服务的事务上下文中，这样，服务链路中的数据库更新操作，就都会向该 XID 代表的全局事务注册分支，纳入同一个全局事务的管辖。 基于这个机制，Fescar 是可以支持任何微服务 RPC 框架的。只要在特定框架中找到可以透明传播 XID 的机制即可，比如，Dubbo 的 Filter + RpcContext。 对应到 Java EE 规范和 Spring 定义的事务传播属性，Fescar 的支持如下： PROPAGATION_REQUIRED： 默认支持 PROPAGATION_SUPPORTS： 默认支持 PROPAGATION_MANDATORY：应用通过 API 来实现 PROPAGATION_REQUIRES_NEW：应用通过 API 来实现 PROPAGATION_NOT_SUPPORTED：应用通过 API 来实现 PROPAGATION_NEVER：应用通过 API 来实现 PROPAGATION_REQUIRED_NESTED：不支持 3.5 隔离性全局事务的隔离性是建立在分支事务的本地隔离级别基础之上的。 在数据库本地隔离级别 读已提交 或以上的前提下，Fescar 设计了由事务协调器维护的 全局写排他锁，来保证事务间的 写隔离，将全局事务默认定义在 读未提交 的隔离级别上。 我们对隔离级别的共识是：绝大部分应用在 读已提交 的隔离级别下工作是没有问题的。而实际上，这当中又有绝大多数的应用场景，实际上工作在 读未提交 的隔离级别下同样没有问题。 在极端场景下，应用如果需要达到全局的 读已提交，Fescar 也提供了相应的机制来达到目的。默认，Fescar 是工作在 读无提交 的隔离级别下，保证绝大多数场景的高效性。 事务的 ACID 属性在 Fescar 中的体现是一个比较复杂的话题，我们会有专门的文章来深入分析，这里不做进一步展开。 4. 适用场景分析前文所述的 Fescar 的核心原理中有一个 重要前提：分支事务中涉及的资源，必须 是支持 ACID 事务的 关系型数据库。分支的提交和回滚机制，都依赖于本地事务的保障。所以，如果应用使用的数据库是不支持事务的，或根本不是关系型数据库，就不适用。 另外，目前 Fescar 的实现还存在一些局限，比如：事务隔离级别最高支持到 读已提交 的水平，SQL 的解析还不能涵盖全部的语法等。 为了覆盖 Fescar 原生机制暂时不能支持应用场景，我们定义了另外一种工作模式。 上面介绍的 Fescar 原生工作模式称为 AT（Automatic Transaction）模式，这种模式是对业务无侵入的。与之相应的另外一种工作模式称为 MT（Manual Transaction）模式，这种模式下，分支事务需要应用自己来定义业务本身及提交和回滚的逻辑。 4.1 分支的基本行为模式作为全局事务一部分的分支事务，除本身的业务逻辑外，都包含 4 个与协调器交互的行为： 分支注册： 在分支事务的数据操作进行之前，需要向协调器注册，把即将进行的分支事务数据操作，纳入一个已经开启的全局事务的管理中去，在分支注册成功后，才可以进行数据操作。 状态上报： 在分支事务的数据操作完成后，需要向事务协调器上报其执行结果。 分支提交：响应协调器发出的分支事务提交的请求，完成分支提交。 分支回滚：响应协调器发出的分支事务回滚的请求，完成分支回滚。 4.2 AT 模式分支的行为模式业务逻辑不需要关注事务机制，分支与全局事务的交互过程自动进行。 4.3 MT 模式分支的行为模式业务逻辑需要被分解为 Prepare/Commit/Rollback 3 部分，形成一个 MT 分支，加入全局事务。 MT 模式一方面是 AT 模式的补充。另外，更重要的价值在于，通过 MT 模式可以把众多非事务性资源纳入全局事务的管理中。 4.4 混合模式因为 AT 和 MT 模式的分支从根本上行为模式是一致的，所以可以完全兼容，即，一个全局事务中，可以同时存在 AT 和 MT 的分支。这样就可以达到全面覆盖业务场景的目的：AT 模式可以支持的，使用 AT 模式；AT 模式暂时支持不了的，用 MT 模式来替代。另外，自然的，MT 模式管理的非事务性资源也可以和支持事务的关系型数据库资源一起，纳入同一个分布式事务的管理中。 4.5 应用场景的远景回到我们设计的初衷：一个理想的分布式事务解决方案是不应该侵入业务的。MT 模式是在 AT 模式暂时不能完全覆盖所有场景的情况下，一个比较自然的补充方案。我们希望通过 AT 模式的不断演进增强，逐步扩大所支持的场景，MT 模式逐步收敛。未来，我们会纳入对 XA 的原生支持，用 XA 这种无侵入的方式来覆盖 AT 模式无法触达的场景。 更多信息请查看阿里的GitHub：https://github.com/alibaba/fescar]]></content>
      <categories>
        <category>中间件</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>数据库</tag>
        <tag>分布式解决方案</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[spring cloud gateway系列教程4—其他配置]]></title>
    <url>%2F2019%2F01%2F09%2Fspring-cloud-gateway%E7%B3%BB%E5%88%97%E6%95%99%E7%A8%8B4%E2%80%94%E5%85%B6%E4%BB%96%E9%85%8D%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[spring cloud gateway系列教程目录 spring cloud gateway系列教程1—Route Predicate spring cloud gateway系列教程2——GatewayFilter_上篇 spring cloud gateway系列教程2——GatewayFilter_下篇 spring cloud gateway系列教程3—Global Filters spring cloud gateway系列教程4—其他配置 1. TLS / SSL Spring Cloud Gateway使用HTTPS，是和普通的Spring boot服务配置是一样的，比如： application.yml. 1234567server: ssl: enabled: true key-alias: scg key-store-password: scg1234 key-store: classpath:scg-keystore.p12 key-store-type: PKCS12 Spring Cloud Gateway都可以路由转给给http和HTTPS的下游后端服务，如果是路由去HTTPS后端服务，gateway像下面一样配置信任所有下游服务： application.yml. 123456spring: cloud: gateway: httpclient: ssl: useInsecureTrustManager: true 当然这种配置，线上生成环境还是不太适合的，所以gateway可以配置自己的信任的证书列表: application.yml. 12345678spring: cloud: gateway: httpclient: ssl: trustedX509Certificates: - cert1.pem - cert2.pem Spring Cloud Gateway如果没有配置信任证书列表，则会拿系统默认的证书库（可以通过system property的javax.net.ssl.trustStore属性来修改系统默认证书库）。 TLS Handshake当是用HTTPS来通讯时，http客户端就需要初始化TLS握手连接了，所以就需要配置握手连接时的超时配置： application.yml. 12345678spring: cloud: gateway: httpclient: ssl: handshake-timeout-millis: 10000 close-notify-flush-timeout-millis: 3000 close-notify-read-timeout-millis: 0 2. ConfigurationSpring Cloud Gateway是通过一系列的RouteDefinitionLocator接口配置的，接口如下： RouteDefinitionLocator.java. 123public interface RouteDefinitionLocator &#123; Flux&lt;RouteDefinition&gt; getRouteDefinitions();&#125; 默认情况下，PropertiesRouteDefinitionLocator会通过Spring Boot的@ConfigurationProperties机制来加载路由配置，比如下面的例子（一个使用了完整的配置，一个使用了快捷配置，前几章也大量的用了这些配置）： application.yml. 1234567891011121314spring: cloud: gateway: routes: - id: setstatus_route uri: http://example.org filters: - name: SetStatus args: status: 401 - id: setstatusshortcut_route uri: http://www.google.com filters: - SetStatus=401 通常情况下，properties的配置就已经够用的了，但也有一些人的需求是从外部源来加载配置文件，比如数据库等，所以官方也承诺未来的版本会基于Spring Data Repositories实现Redis, MongoDB和Cassandra版本的RouteDefinitionLocator。 2.1 Fluent Java Routes API除了上面的配置文件配置外，也可以通过RouteLocatorBuilder的流式API来进行java实现配置。 GatewaySampleApplication.java. 123456789101112131415161718192021222324// static imports from GatewayFilters and RoutePredicates@Beanpublic RouteLocator customRouteLocator(RouteLocatorBuilder builder, ThrottleGatewayFilterFactory throttle) &#123; return builder.routes() .route(r -&gt; r.host("**.abc.org").and().path("/image/png") .filters(f -&gt; f.addResponseHeader("X-TestHeader", "foobar")) .uri("http://httpbin.org:80") ) .route(r -&gt; r.path("/image/webp") .filters(f -&gt; f.addResponseHeader("X-AnotherHeader", "baz")) .uri("http://httpbin.org:80") ) .route(r -&gt; r.order(-1) .host("**.throttle.org").and().path("/get") .filters(f -&gt; f.filter(throttle.apply(1, 1, 10, TimeUnit.SECONDS))) .uri("http://httpbin.org:80") ) .build();&#125; 这种用法就可以通过实行Predicate&lt;ServerWebExchange&gt;接口来定义更复杂的匹配规则，也可以用and()、or()和negate()来组合不同的匹配规则，灵活性会更大一点。 2.2 DiscoveryClient Route Definition Locator通过服务发现客户端DiscoveryClient，gateway可以基于注册了的服务自动创建路由。只需要配置spring.cloud.gateway.discovery.locator.enabled=true，以及引入DiscoveryClient的maven依赖即可，如：Netflix Eureka, Consul or Zookeeper。 Configuring Predicates and Filters For DiscoveryClient Routes默认情况下gateway中的GatewayDiscoveryClientAutoConfiguration以及定义了一个predicate和filter的了。默认的predicate是配置了/serviceId/**路径的path predicate，当然serviceId是DiscoveryClient里面的服务id。默认的filter是配置了匹配参数/serviceId/(?&lt;remaining&gt;.*)和替换参数/${remaining}的rewrite path filter，目的是将serviceId从path中去除掉，因为下游是不需要的。 你也可以自定义DiscoveryClient路由的predicate和filter，只需要设置spring.cloud.gateway.discovery.locator.predicates[x]和spring.cloud.gateway.discovery.locator.filters[y]即可，如下：application.properties. 123456789spring.cloud.gateway.discovery.locator.predicates[0].name: Pathspring.cloud.gateway.discovery.locator.predicates[0].args[pattern]: "'/'+serviceId+'/**'"spring.cloud.gateway.discovery.locator.predicates[1].name: Hostspring.cloud.gateway.discovery.locator.predicates[1].args[pattern]: "'**.foo.com'"spring.cloud.gateway.discovery.locator.filters[0].name: Hystrixspring.cloud.gateway.discovery.locator.filters[0].args[name]: serviceIdspring.cloud.gateway.discovery.locator.filters[1].name: RewritePathspring.cloud.gateway.discovery.locator.filters[1].args[regexp]: "'/' + serviceId + '/(?&lt;remaining&gt;.*)'"spring.cloud.gateway.discovery.locator.filters[1].args[replacement]: "'/$&#123;remaining&#125;'" 3. Reactor Netty Access Logsspring cloud gateway是没有打印access log的，但是底层的Reactor Netty是有的，在应用启动命名中增加设置-Dreactor.netty.http.server.accessLogEnabled=true来开启。注：因为Reactor Netty不是基于spring boot的，所以它并不会去spring boot的配置中获取上面的配置，所以只能在Java System Property中获取。 可以在常用的日志系统中配置日志的打印文件和格式，如logback的配置： logback.xml. 12345678910111213&lt;appender name="accessLog" class="ch.qos.logback.core.FileAppender"&gt; &lt;file&gt;access_log.log&lt;/file&gt; &lt;encoder&gt; &lt;pattern&gt;%msg%n&lt;/pattern&gt; &lt;/encoder&gt;&lt;/appender&gt;&lt;appender name="async" class="ch.qos.logback.classic.AsyncAppender"&gt; &lt;appender-ref ref="accessLog" /&gt;&lt;/appender&gt;&lt;logger name="reactor.netty.http.server.AccessLog" level="INFO" additivity="false"&gt; &lt;appender-ref ref="async"/&gt;&lt;/logger&gt; 4. CORS Configurationgateway是支持CORS的配置，可以通过不同的URL规则匹配不同的CORS策略： application.yml. 123456789spring: cloud: gateway: globalcors: corsConfigurations: '[/**]': allowedOrigins: "http://docs.spring.io" allowedMethods: - GET 有不熟悉CORS的，可以看一下这篇介绍。 5. Actuator APISpring Cloud Gateway也可以配置actuator来监控和操作一些功能点，增加下面的配置即可： application.properties. 12management.endpoint.gateway.enabled=true # default valuemanagement.endpoints.web.exposure.include=gateway 5.1 查看filter信息5.1.1 Global Filters使用GET请求gateway地址/actuator/gateway/globalfilters，就可以获取类似于下面的返回： 12345678910&#123; "org.springframework.cloud.gateway.filter.LoadBalancerClientFilter@77856cc5": 10100, "org.springframework.cloud.gateway.filter.RouteToRequestUrlFilter@4f6fd101": 10000, "org.springframework.cloud.gateway.filter.NettyWriteResponseFilter@32d22650": -1, "org.springframework.cloud.gateway.filter.ForwardRoutingFilter@106459d9": 2147483647, "org.springframework.cloud.gateway.filter.NettyRoutingFilter@1fbd5e0": 2147483647, "org.springframework.cloud.gateway.filter.ForwardPathFilter@33a71d23": 0, "org.springframework.cloud.gateway.filter.AdaptCachedBodyGlobalFilter@135064ea": 2147483637, "org.springframework.cloud.gateway.filter.WebsocketRoutingFilter@23c05889": 2147483646&#125; 返回信息包含了gateway的使用中的global filters实例，包含了实例的toString()和order的keyvalue信息。 5.1.2 Route Filters使用GET请求gateway地址/actuator/gateway/routefilters，就可以获取类似于下面的返回： 12345&#123; "[AddRequestHeaderGatewayFilterFactory@570ed9c configClass = AbstractNameValueGatewayFilterFactory.NameValueConfig]": null, "[SecureHeadersGatewayFilterFactory@fceab5d configClass = Object]": null, "[SaveSessionGatewayFilterFactory@4449b273 configClass = Object]": null&#125; 返回信息里面包含了gateway中可以提供使用的GatewayFilter factories 详细信息，其中展示的是GatewayFilterFactory的实例toString()打印，及配置类。后面的null是某些GatewayFilter factory实现问题，本来是用来展示order的，但是GatewayFilter factory没有实现，就返回null了。 5.2 路由缓存刷新使用POST请求gateway地址/actuator/gateway/refresh，并返回http状态码为200，标识刷新路由缓存成功。 5.3 查看路由定义信息使用GET请求gateway地址/actuator/gateway/routes，获取类似下面的返回： 123456789101112131415161718[&#123; "route_id": "first_route", "route_object": &#123; "predicate": "org.springframework.cloud.gateway.handler.predicate.PathRoutePredicateFactory$$Lambda$432/1736826640@1e9d7e7d", "filters": [ "OrderedGatewayFilter&#123;delegate=org.springframework.cloud.gateway.filter.factory.PreserveHostHeaderGatewayFilterFactory$$Lambda$436/674480275@6631ef72, order=0&#125;" ] &#125;, "order": 0&#125;,&#123; "route_id": "second_route", "route_object": &#123; "predicate": "org.springframework.cloud.gateway.handler.predicate.PathRoutePredicateFactory$$Lambda$432/1736826640@cd8d298", "filters": [] &#125;, "order": 0&#125;] 上面对象的定义如下表： key value类型 value描述 route_id String 路由id. route_object.predicate Object Route Predicate route_object.filters Array GatewayFilter order Number 路由顺序 ##### 5.3.1 查看单个路由信息 如果是想只获取单个路由信息，则使用GET请求地址/actuator/gateway/routes/{id}即可。 5.3.2 创建和删除路由创建路由，使用POST请求，并附带类似下面的json body，到/gateway/routes/{id_route_to_create}即可。 12345678&#123; "route_id": "second_route", "route_object": &#123; "predicate": "org.springframework.cloud.gateway.handler.predicate.PathRoutePredicateFactory$$Lambda$432/1736826640@cd8d298", "filters": [] &#125;, "order": 0&#125; 删除路由，使用DELETE请求地址/gateway/routes/{id_route_to_delete}即可。 5.4 Actuator API汇总 ID HTTP Method Description globalfilters GET 展示global filters信息 routefilters GET 展示GatewayFilter factories信息 refresh POST 刷新路由缓存 routes GET 展示路由定义信息 routes/{id} GET 展示单个路由信息 routes/{id} POST 添加新的路由 routes/{id} DELETE 移除路由 开发指南自定义GatewayFilter Factories如果想自定义实现GatewayFilterFactory，可以继承AbstractGatewayFilterFactory抽象类。 比如如果想请求前做一些事情，可以类似于下面的实现： *PreGatewayFilterFactory.java. * 12345678910111213141516171819202122public class PreGatewayFilterFactory extends AbstractGatewayFilterFactory&lt;PreGatewayFilterFactory.Config&gt; &#123; public PreGatewayFilterFactory() &#123; super(Config.class); &#125; @Override public GatewayFilter apply(Config config) &#123; // 从config对象中获取配置 return (exchange, chain) -&gt; &#123; // 在这里做请求前的事情 ServerHttpRequest.Builder builder = exchange.getRequest().mutate(); //重新构造新的request return chain.filter(exchange.mutate().request(request).build()); &#125;; &#125; public static class Config &#123; // 设置配置 &#125;&#125; 请求后做的是事情，可以如下实现： PostGatewayFilterFactory.java. 12345678910111213141516171819202122public class PostGatewayFilterFactory extends AbstractGatewayFilterFactory&lt;PostGatewayFilterFactory.Config&gt; &#123; public PostGatewayFilterFactory() &#123; super(Config.class); &#125; @Override public GatewayFilter apply(Config config) &#123; // 从config对象中获取配置 return (exchange, chain) -&gt; &#123; return chain.filter(exchange).then(Mono.fromRunnable(() -&gt; &#123; ServerHttpResponse response = exchange.getResponse(); // 在这里做请求后的操作实现 &#125;)); &#125;; &#125; public static class Config &#123; // 设置配置 &#125;&#125; 以上就是spring cloud gateway的其他配置使用讲解，如果想查看其他spring cloud gateway的案例和使用，可以点击查看]]></content>
      <categories>
        <category>spring cloud gateway系列教程</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>spring</tag>
        <tag>gateway</tag>
        <tag>spring cloud gateway</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[spring cloud gateway的POST请求问题解决：Only one connection receive subscriber allowed]]></title>
    <url>%2F2019%2F01%2F08%2Fspring-cloud-gateway%E7%9A%84POST%E8%AF%B7%E6%B1%82%E9%97%AE%E9%A2%98%E8%A7%A3%E5%86%B3%EF%BC%9AOnly-one-connection-receive-subscriber-allowed%2F</url>
    <content type="text"><![CDATA[笔者使用spring cloud gateway时踩了个很多人都遇到的坑，这个也是spring boot和spring cloud gateway的开发者们没有协商好导致的问题(后面会说)。 环境JDK1.8spring boot 2.0.6.RELEASE 问题表现 使用POST请求gateway时，后端微服务拿不到body信息，仿佛就是gateway吞掉了body(其实就是)； gateway开DEBUG模式，查看报如下错误：123456789java.lang.IllegalStateException: Only one connection receive subscriber allowed. at reactor.ipc.netty.channel.FluxReceive.startReceiver(FluxReceive.java:279) at reactor.ipc.netty.channel.FluxReceive.lambda$subscribe$2(FluxReceive.java:129) at io.netty.util.concurrent.AbstractEventExecutor.safeExecute$$$capture(AbstractEventExecutor.java:163) at io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java) at io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:404) at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:446) at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:884) at java.lang.Thread.run(Thread.java:745) 大意就是netty的request body只能读取一次，第二次读取就报这个错误了。 问题原因翻查GitHub终于找到，spring boot在2.0.5版本如果使用了WebFlux就自动配置HiddenHttpMethodFilter过滤器。查看源码发现，这个过滤器的作用是，针对当前的浏览器一般只支持GET和POST表单提交方法，如果想使用其他HTTP方法(如：PUT、DELETE、PATCH)，就只能通过一个隐藏的属性如(_method=PUT)来表示，那么HiddenHttpMethodFilter的作用是将POST请求的_method参数里面的value替换掉http请求的方法。想法是很好的，用一种折中的方法来支持使浏览器支持restful方法。 如果只是使用spring boot，一切都是没有问题的，因为使用的过程中，不需要我们自己解析request body，到controller这一层，这一切就已经完成的了。 但是spring cloud gateway需要，因为它的做法就是拿到原始请求信息(包括request body)，再重新封装一个request路由到下游微服务，所以上面的问题就在于: HiddenHttpMethodFilter读取了一次request body； gateway的封装自己的request时，去读取request body，就报错了。 所以这个是spring cloud gateway和spring boot开发者没协商好，都去读取request body的问题。 问题解决方案 HiddenHttpMethodFilter是spring boot在2.0.5版本自动引入的，将版本降到2.0.4即可。 如果不降版本，也可以自己重写HiddenHttpMethodFilter来覆盖原来的实现，如下：123456789@Beanpublic HiddenHttpMethodFilter hiddenHttpMethodFilter() &#123; return new HiddenHttpMethodFilter() &#123; @Override public Mono&lt;Void&gt; filter(ServerWebExchange exchange, WebFilterChain chain) &#123; return chain.filter(exchange); &#125; &#125;;&#125; filter方法什么都不做，自然就不会读取request body了，你可能会想，那不就不支持HiddenHttpMethodFilter的功能了吗，其实gateway本身就不应该做这种事情，原始请求是怎样的，转发给下游的请求就应该是怎样的。下游服务如果使用的是也是spring boot的服务，那么下游服务自己会做HiddenHttpMethodFilter的功能。 这也是gateway官方开发者目前所提出的解决方案。 更多信息可以查看这个issuesspring boot的2.0.5版本添加了HiddenHttpMethodFilter内容，入口 如果想查看其他spring cloud gateway的案例和使用，可以点击查看 有什么问题，或者有更好的解决方案，也可以在下方留言。]]></content>
      <categories>
        <category>问题总结</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>spring</tag>
        <tag>spring cloud gateway</tag>
        <tag>bug解决</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hexo使用rsync部署发布]]></title>
    <url>%2F2019%2F01%2F06%2Fhexo%E4%BD%BF%E7%94%A8rsync%E9%83%A8%E7%BD%B2%E5%8F%91%E5%B8%83%2F</url>
    <content type="text"><![CDATA[hexo发布hexo提供了很多种部署发布的工具集成，如：git、heroku、Netlify、Rsync、OpenShift、FTPSync和SFTP。 网上讲得比较多，就是git的集成发布了，因为多数人部署hexo都是发布到GitHub page的（免费好用，不过GitHub page屏蔽了百度爬虫，百度搜索引擎是搜索不出来的）。 然而我是部署在自有的服务器上，如果使用git，就需要在服务器上搭建git的服务仓库，只能说很繁琐，所以我自己写了两个简单的部署脚本来处理部署的事情，不过随着文章越来越多和越来越大，每次部署都需要很长的传输时间。 后续研究了一番Rsync，以及与hexo集成使用，终于发现了新大陆。 Rsync的服务部署默认情况Ubuntu已经安装了rsync服务,如果没有安装也可通过apt-get安装.sudo apt-get install rsync hexo集成的rsync使用的是ssh底层，所以rsync的服务配置只需要简单配置一下即可，目的是需要rsync服务起来： 编辑配置文件：sudo vim /etc/rsyncd.conf 12345678uid = nobodygid = nobodyport = 873 #不会使用到这个端口号，防火墙可以屏蔽它use chroot = nomax connections = 4pid file = /var/run/rsyncd.pidlock file = /var/run/rsync.locklog file = /var/log/rsyncd.log 服务启动：sudo rsync --daemon hexo集成安装npm install hexo-deployer-rsync --save hexo配置部署123456789deploy: type: rsync host: 服务器host user: ssh用户名 root: 你的站点根目录 port: 22 #ssh端口号 delete: true verbose: true ignore_errors: false 这里有一个要注意的点是，默认hexo使用的是~/.ssh/id_rsa(.pub)的那对ssh公私钥文件，官方文档也没看到哪里可以指定自己的ssh文件。 这里有两个解决方案，第一个是，在服务器的/.ssh/authorized_keys中添加本地的/.ssh/id_rsa.pub的公钥内容； 第二个是共用底层通道，通过配置~/.ssh/config 123host *ControlMaster autoControlPath ~/.ssh/control/%C 如果同一个服务器的连接，第二次就不需要校验的了，因为它们是共用同一个传输通道的，所以你只需要在打开一个窗口（以iterm2为例）ssh连接上服务器，在另一个本地窗口你的hexo博客目录执行hexo deploy即可。 最后，每次部署就不需要所有文件都部署传输一次了，rsync只会将新的和修改过的部署到服务器上。]]></content>
      <categories>
        <category>笔记</category>
      </categories>
      <tags>
        <tag>hexo</tag>
        <tag>rsync</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[spring cloud gateway系列教程3—Global Filters]]></title>
    <url>%2F2019%2F01%2F06%2Fspring%20cloud%20gateway%E7%B3%BB%E5%88%97%E6%95%99%E7%A8%8B3%E2%80%94Global%20Filters%2F</url>
    <content type="text"><![CDATA[spring cloud gateway系列教程目录 spring cloud gateway系列教程1—Route Predicate spring cloud gateway系列教程2——GatewayFilter_上篇 spring cloud gateway系列教程2——GatewayFilter_下篇 spring cloud gateway系列教程3—Global Filters spring cloud gateway系列教程4—其他配置 Global FiltersGlobalFilter接口方法和GatewayFilter是一样的，GlobalFilter特别之处在于它的作用是全局的。 1. Combined Global Filter and GatewayFilter Ordering当请求到来时，Filtering Web Handler处理器会添加所有GlobalFilter实例和匹配的GatewayFilter实例到过滤器链中，通过对filterbean配置注解@Order，则过滤器链会对这些过滤器实例bean进行排序。 Spring Cloud Gateway将过滤器的逻辑按请求执行点分为”pre”和”post”的一前一后处理，如果是高优先级的过滤器，则在”pre”逻辑中最先执行，在”post”逻辑中最后执行。 ExampleConfiguration.java. 1234567891011121314151617181920212223242526272829303132@Bean@Order(-1)public GlobalFilter a() &#123; return (exchange, chain) -&gt; &#123; log.info("first pre filter"); return chain.filter(exchange).then(Mono.fromRunnable(() -&gt; &#123; log.info("third post filter"); &#125;)); &#125;;&#125;@Bean@Order(0)public GlobalFilter b() &#123; return (exchange, chain) -&gt; &#123; log.info("second pre filter"); return chain.filter(exchange).then(Mono.fromRunnable(() -&gt; &#123; log.info("second post filter"); &#125;)); &#125;;&#125;@Bean@Order(1)public GlobalFilter c() &#123; return (exchange, chain) -&gt; &#123; log.info("third pre filter"); return chain.filter(exchange).then(Mono.fromRunnable(() -&gt; &#123; log.info("first post filter"); &#125;)); &#125;;&#125; 上面例子陆续会打印的是： 123456first pre filtersecond pre filterthird pre filterfirst post filtersecond post filterthird post filter 2. Forward Routing FilterForwardRoutingFilter会查看exchange的属性ServerWebExchangeUtils.GATEWAY_REQUEST_URL_ATTR的URI内容，如果url的scheme是forward，比如：forward://localendpoint，则它会使用Spirng的DispatcherHandler来处理这个请求。 源码实现： 123456789101112131415161718@Override public Mono&lt;Void&gt; filter(ServerWebExchange exchange, GatewayFilterChain chain) &#123; URI requestUrl = exchange.getRequiredAttribute(GATEWAY_REQUEST_URL_ATTR); String scheme = requestUrl.getScheme(); if (isAlreadyRouted(exchange) || !"forward".equals(scheme)) &#123; return chain.filter(exchange); &#125; setAlreadyRouted(exchange); //TODO: translate url? if (log.isTraceEnabled()) &#123; log.trace("Forwarding to URI: "+requestUrl); &#125; return this.dispatcherHandler.handle(exchange); &#125; 3. LoadBalancerClient FilterLoadBalancerClientFilter会查看exchange的属性ServerWebExchangeUtils.GATEWAY_REQUEST_URL_ATTR的URI内容，如果url的scheme是lb，比如：lb://myservice，或者是ServerWebExchangeUtils.GATEWAY_SCHEME_PREFIX_ATTR属性的内容是lb，则它会使用Spring Cloud的LoadBalancerClient来将host转化为实际的host和port，并以此替换属性ServerWebExchangeUtils.GATEWAY_REQUEST_URL_ATTR的内容，原来的URL则会被添加到ServerWebExchangeUtils.GATEWAY_ORIGINAL_REQUEST_URL_ATTR属性的列表中。 源码实现： 123456789101112131415161718192021222324252627282930313233@Override public Mono&lt;Void&gt; filter(ServerWebExchange exchange, GatewayFilterChain chain) &#123; URI url = exchange.getAttribute(GATEWAY_REQUEST_URL_ATTR); String schemePrefix = exchange.getAttribute(GATEWAY_SCHEME_PREFIX_ATTR); if (url == null || (!"lb".equals(url.getScheme()) &amp;&amp; !"lb".equals(schemePrefix))) &#123; return chain.filter(exchange); &#125; //preserve the original url addOriginalRequestUrl(exchange, url); log.trace("LoadBalancerClientFilter url before: " + url); final ServiceInstance instance = loadBalancer.choose(url.getHost()); if (instance == null) &#123; throw new NotFoundException("Unable to find instance for " + url.getHost()); &#125; URI uri = exchange.getRequest().getURI(); // if the `lb:&lt;scheme&gt;` mechanism was used, use `&lt;scheme&gt;` as the default, // if the loadbalancer doesn't provide one. String overrideScheme = null; if (schemePrefix != null) &#123; overrideScheme = url.getScheme(); &#125; URI requestUrl = loadBalancer.reconstructURI(new DelegatingServiceInstance(instance, overrideScheme), uri); log.trace("LoadBalancerClientFilter url chosen: " + requestUrl); exchange.getAttributes().put(GATEWAY_REQUEST_URL_ATTR, requestUrl); return chain.filter(exchange); &#125; application.yml. 12345678spring: cloud: gateway: routes: - id: myRoute uri: lb://service predicates: - Path=/service/** 默认情况下，如果LoadBalancer找不到服务实例，则会返回HTTP状态码503，你也可以通过修改spring.cloud.gateway.loadbalancer.use404=true配置修改为返回状态码404。 4. Netty Routing Filter如果ServerWebExchangeUtils.GATEWAY_REQUEST_URL_ATTR属性中的url的scheme是http或https，则Netty Routing Filter才会执行，并使用Netty作为http请求客户端对下游进行代理请求。请求的响应会放在exchange的ServerWebExchangeUtils.CLIENT_RESPONSE_ATTR属性中，以便后面的filter做进一步的处理。 5. Netty Write Response Filter如果NettyWriteResponseFilter发现exchange的ServerWebExchangeUtils.CLIENT_RESPONSE_ATTR属性中存在Netty的HttpClientResponse类型实例，在所有过滤器都执行完毕后，它会将响应写回到gateway客户端的响应中。 源码实现: 1234567891011121314151617181920212223242526272829303132@Override public Mono&lt;Void&gt; filter(ServerWebExchange exchange, GatewayFilterChain chain) &#123; // NOTICE: nothing in "pre" filter stage as CLIENT_RESPONSE_ATTR is not added // until the WebHandler is run return chain.filter(exchange).then(Mono.defer(() -&gt; &#123; HttpClientResponse clientResponse = exchange.getAttribute(CLIENT_RESPONSE_ATTR); if (clientResponse == null) &#123; return Mono.empty(); &#125; log.trace("NettyWriteResponseFilter start"); ServerHttpResponse response = exchange.getResponse(); NettyDataBufferFactory factory = (NettyDataBufferFactory) response.bufferFactory(); //TODO: what if it's not netty final Flux&lt;NettyDataBuffer&gt; body = clientResponse.receive() .retain() //TODO: needed? .map(factory::wrap); MediaType contentType = response.getHeaders().getContentType(); return (isStreamingMediaType(contentType) ? response.writeAndFlushWith(body.map(Flux::just)) : response.writeWith(body)); &#125;)); &#125; //TODO: use framework if possible //TODO: port to WebClientWriteResponseFilter private boolean isStreamingMediaType(@Nullable MediaType contentType) &#123; return (contentType != null &amp;&amp; this.streamingMediaTypes.stream() .anyMatch(contentType::isCompatibleWith)); &#125; 6. RouteToRequestUrl Filter如果exchange的ServerWebExchangeUtils.GATEWAY_ROUTE_ATTR属性存放了Route对象，则RouteToRequestUrlFilter会根据基于请求的URI创建新的URI，新的URI会更新到ServerWebExchangeUtils.GATEWAY_REQUEST_URL_ATTR属性中。 如果URI有scheme前缀，比如：lb:ws://serviceid，lbscheme截取出来放到ServerWebExchangeUtils.GATEWAY_SCHEME_PREFIX_ATTR属性中，方便后面的filter使用。 源码实现: 123456789101112131415161718192021222324252627282930@Override public Mono&lt;Void&gt; filter(ServerWebExchange exchange, GatewayFilterChain chain) &#123; Route route = exchange.getAttribute(GATEWAY_ROUTE_ATTR); if (route == null) &#123; return chain.filter(exchange); &#125; log.trace("RouteToRequestUrlFilter start"); URI uri = exchange.getRequest().getURI(); boolean encoded = containsEncodedParts(uri); URI routeUri = route.getUri(); if (hasAnotherScheme(routeUri)) &#123; // this is a special url, save scheme to special attribute // replace routeUri with schemeSpecificPart exchange.getAttributes().put(GATEWAY_SCHEME_PREFIX_ATTR, routeUri.getScheme()); routeUri = URI.create(routeUri.getSchemeSpecificPart()); &#125; URI requestUrl = UriComponentsBuilder.fromUri(uri) .uri(routeUri) .build(encoded) .toUri(); exchange.getAttributes().put(GATEWAY_REQUEST_URL_ATTR, requestUrl); return chain.filter(exchange); &#125; /* for testing */ static boolean hasAnotherScheme(URI uri) &#123; return schemePattern.matcher(uri.getSchemeSpecificPart()).matches() &amp;&amp; uri.getHost() == null &amp;&amp; uri.getRawPath() == null; &#125; 7. Websocket Routing Filter如果请求URL的scheme是ws或wss的话，那么Websocket Routing Filter就会使用Spring Web Socket底层来处理对下游的请求转发。 如果Websocket也使用了负载均衡，则需要这样配置：lb:ws://serviceid. 源码实现： 1234567891011121314151617181920212223242526272829@Override public Mono&lt;Void&gt; filter(ServerWebExchange exchange, GatewayFilterChain chain) &#123; changeSchemeIfIsWebSocketUpgrade(exchange); URI requestUrl = exchange.getRequiredAttribute(GATEWAY_REQUEST_URL_ATTR); String scheme = requestUrl.getScheme(); if (isAlreadyRouted(exchange) || (!"ws".equals(scheme) &amp;&amp; !"wss".equals(scheme))) &#123; return chain.filter(exchange); &#125; setAlreadyRouted(exchange); HttpHeaders headers = exchange.getRequest().getHeaders(); HttpHeaders filtered = filterRequest(getHeadersFilters(), exchange); List&lt;String&gt; protocols = headers.get(SEC_WEBSOCKET_PROTOCOL); if (protocols != null) &#123; protocols = headers.get(SEC_WEBSOCKET_PROTOCOL).stream() .flatMap(header -&gt; Arrays.stream(commaDelimitedListToStringArray(header))) .map(String::trim) .collect(Collectors.toList()); &#125; return this.webSocketService.handleRequest(exchange, new ProxyWebSocketHandler(requestUrl, this.webSocketClient, filtered, protocols)); &#125; 8. Making An Exchange As Routed上面一些像ForwardRoutingFilter、Websocket Routing Filter的源码中，都可以清楚看到gateway通过设置gatewayAlreadyRouted标识这个请求是否已经路由转发出去了，无需其他filter重复路由，这样就可以避免重复错误的路由操作，保证了路由的实现灵活性。 ServerWebExchangeUtils.isAlreadyRouted检查是否已被路由，ServerWebExchangeUtils.setAlreadyRouted标记已被路由状态。 这一章介绍了Spring Cloud Gateway官方的Global Filters使用场景，下一章讲gateway其他配置的使用。 如果想查看其他spring cloud gateway的案例和使用，可以点击查看]]></content>
      <categories>
        <category>spring cloud gateway系列教程</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>spring</tag>
        <tag>gateway</tag>
        <tag>spring cloud gateway</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[spring cloud gateway系列教程2——GatewayFilter_下篇]]></title>
    <url>%2F2019%2F01%2F06%2Fspring%20cloud%20gateway%E7%B3%BB%E5%88%97%E6%95%99%E7%A8%8B2%E2%80%94%E2%80%94GatewayFilter_%E4%B8%8B%E7%AF%87%2F</url>
    <content type="text"><![CDATA[spring cloud gateway系列教程目录 spring cloud gateway系列教程1—Route Predicate spring cloud gateway系列教程2——GatewayFilter_上篇 spring cloud gateway系列教程2——GatewayFilter_下篇 spring cloud gateway系列教程3—Global Filters spring cloud gateway系列教程4—其他配置 接上篇 8. RequestRateLimiter GatewayFilter FactoryRequestRateLimiter GatewayFilter Factory使用RateLimiter来决定当前请求是否允许通过，如果不允许，则默认返回状态码HTTP 429 - Too Many Requests。 RequestRateLimiter GatewayFilter可以使用一个可选参数keyResolver来做速率限制。 keyResolver是KeyResolver接口的一个实现bean，在配置里面，通过SpEL表达式#{@myKeyResolver}来管理bean的名字myKeyResolver。 KeyResolver.java. 123public interface KeyResolver &#123; Mono&lt;String&gt; resolve(ServerWebExchange exchange);&#125; KeyResolver接口允许你使用不同的策略来得出限制请求的key，未来，官方也会推出一些KeyResolver的不同实现。 KeyResolver默认实现是PrincipalNameKeyResolver，通过ServerWebExchange中获取Principal，并以Principal.getName()作为限流的key。 如果KeyResolver拿不到key，请求默认都会被限制，你也可以自己配置spring.cloud.gateway.filter.request-rate-limiter.deny-empty-key：是否允许空key，spring.cloud.gateway.filter.request-rate-limiter.empty-key-status-code ：空key时返回的状态码。 RequestRateLimiter不支持捷径配置，如下面的配置是非法的 application.properties. 12# INVALID SHORTCUT CONFIGURATIONspring.cloud.gateway.routes[0].filters[0]=RequestRateLimiter=2, 2, #&#123;@userkeyresolver&#125; 8.1 Redis RateLimiter基于 Stripe的redis实现方案，依赖spring-boot-starter-data-redis-reactiveSpring Boot starter，使用的是令牌桶算法。 redis-rate-limiter.replenishRate配置的是每秒允许通过的请求数，其实就是令牌桶的填充速率。 redis-rate-limiter.burstCapacity配置的是一秒内最大的请求数，其实就是令牌桶的最大容量，如果设置为0，则会阻塞所有请求。 所以可以通过设置相同的replenishRate和burstCapacity来实现匀速的速率控制，通过设置burstCapacity大于replenishRate来允许系统流量瞬间突发，但是对于这种情况，突发周期为burstCapacity / replenishRate秒，如果周期内有两次请求突发的情况，则第二次会有部分请求丢失，返回HTTP 429 - Too Many Requests。 application.yml. 1234567891011spring: cloud: gateway: routes: - id: requestratelimiter_route uri: http://www.google.com filters: - name: RequestRateLimiter args: redis-rate-limiter.replenishRate: 10 redis-rate-limiter.burstCapacity: 20 Config.java. 1234@BeanKeyResolver userKeyResolver() &#123; return exchange -&gt; Mono.just(exchange.getRequest().getQueryParams().getFirst("user"));&#125; 上面定义了每个用户每秒10个请求的速率限制，允许20的突发流量，突发完，下一秒只允许10个请求通过了，KeyResolver定义了通过请求获取请求参数user作为key。 你也可以实现RateLimiter接口自定义自己的请求速率限制器，在配置文件中使用SpEL表达式配置对应的bean的名字即可。 application.yml. 1234567891011spring: cloud: gateway: routes: - id: requestratelimiter_route uri: http://www.google.com filters: - name: RequestRateLimiter args: rate-limiter: "#&#123;@myRateLimiter&#125;" key-resolver: "#&#123;@userKeyResolver&#125;" 9. RedirectTo GatewayFilter FactoryRedirectTo GatewayFilter Factory使用status和url两个参数，其中status必须是300系列的HTTP状态码，url则是跳转的地址，会放在响应的Location的header中（http协议中转跳的header）。 application.yml. 12345678spring: cloud: gateway: routes: - id: prefixpath_route uri: http://www.google.cn filters: - RedirectTo=302, http://www.edjdhbb.com 上面路由会执行302重定向到http://www.edjdhbb.com。 10. RemoveNonProxyHeaders GatewayFilter FactoryRemoveNonProxyHeaders GatewayFilter Factory转发请求是会根据IETF的定义，默认会移除下列的http头信息： Connection Keep-Alive Proxy-Authenticate Proxy-Authorization TE Trailer Transfer-Encoding Upgrade 你也可以通过配置spring.cloud.gateway.filter.remove-non-proxy-headers.headers来更改需要移除的header列表。 11. RemoveRequestHeader GatewayFilter FactoryRemoveRequestHeader GatewayFilter Factory配置header的name，即可以移除请求的header。 application.yml. 12345678spring: cloud: gateway: routes: - id: removerequestheader_route uri: http://www.google.com filters: - RemoveRequestHeader=X-Request-Foo 上面路由在发送请求给下游时，会将请求中的X-Request-Foo头信息去掉。 12. RemoveResponseHeader GatewayFilter FactoryRemoveResponseHeader GatewayFilter Factory通过配置header的name，会在响应返回时移除header。 application.yml. 12345678spring: cloud: gateway: routes: - id: removeresponseheader_route uri: http://www.google.com filters: - RemoveResponseHeader=X-Response-Foo 上面路由会在响应返回给gateway的客户端时，将X-Response-Foo响应头信息去掉。 13. RewritePath GatewayFilter FactoryRewritePath GatewayFilter Factory使用路径regexp和替换路径replacement两个参数做路径重写，两个都可以灵活地使用java的正则表达式。 application.yml. 12345678910spring: cloud: gateway: routes: - id: rewritepath_route uri: http://www.google.com predicates: - Path=/foo/** filters: - RewritePath=/foo/(?&lt;segment&gt;.*), /$\&#123;segment&#125; 对于上面的例子，如果请求的路径是/foo/bar，则gateway会将请求路径改为/bar发送给下游。 注：在YAML 的格式中使用$\来代替$。 14. RewriteResponseHeader GatewayFilter FactoryRewriteResponseHeader GatewayFilter Factory的作用是修改响应返回的header内容，需要配置响应返回的header的name，匹配规则regexp和替换词replacement，也是支持java的正则表达式。 application.yml. 12345678spring: cloud: gateway: routes: - id: rewriteresponseheader_route uri: http://www.google.com filters: - RewriteResponseHeader=X-Response-Foo, , password=[^&amp;]+, password=*** 举个例子，对于上面的filter，如果响应的headerX-Response-Foo的内容是/42？user=ford&amp;password=omg!what&amp;flag=true，这个内容会修改为/42?user=ford&amp;password=***&amp;flag=true。 15. SaveSession GatewayFilter FactorySaveSession GatewayFilter Factory会在请求下游时强制执行WebSession::save方法，用在那种像Spring Session延迟数据存储的，并在请求转发前确保session状态保存情况。 application.yml. 12345678910spring: cloud: gateway: routes: - id: save_session uri: http://www.google.com predicates: - Path=/foo/** filters: - SaveSession 如果你将Spring Secutiry于Spring Session集成使用，并想确保安全信息都传到下游机器，你就需要配置这个filter。 16. SecureHeaders GatewayFilter FactorySecureHeaders GatewayFilter Factory会添加在返回响应中一系列安全作用的header，至于为什么，英文好的可以看一下这篇博客。 默认会添加这些头信息和默认内容： X-Xss-Protection:1; mode=block Strict-Transport-Security:max-age=631138519 X-Frame-Options:DENY X-Content-Type-Options:nosniff Referrer-Policy:no-referrer Content-Security-Policy:default-src &#39;self&#39; https:; font-src &#39;self&#39; https: data:; img-src &#39;self&#39; https: data:; object-src &#39;none&#39;; script-src https:; style-src &#39;self&#39; https: &#39;unsafe-inline&#39; X-Download-Options:noopen X-Permitted-Cross-Domain-Policies:none 如果你想修改这些头信息的默认内容，可以在配置文件中添加下面的配置： 前缀：spring.cloud.gateway.filter.secure-headers 上面的header对应的后缀： xss-protection-header strict-transport-security frame-options content-type-options referrer-policy content-security-policy download-options permitted-cross-domain-policies 前后缀接起来即可，如：spring.cloud.gateway.filter.secure-headers.xss-protection-header 17. SetPath GatewayFilter FactorySetPath GatewayFilter Factory采用路径template参数，通过请求路径的片段的模板化，来达到操作修改路径的母的，运行多个路径片段模板化。 application.yml. 12345678910spring: cloud: gateway: routes: - id: setpath_route uri: http://www.google.com predicates: - Path=/foo/&#123;segment&#125; filters: - SetPath=/&#123;segment&#125; 对于上面的例子，如果路径是/foo/bar，则对于下游的请求路径会修改为/bar。 18. SetResponseHeader GatewayFilter FactorySetResponseHeader GatewayFilter Factory通过设置name和value来替换响应对于的header。 application.yml. 12345678spring: cloud: gateway: routes: - id: setresponseheader_route uri: http://www.google.com filters: - SetResponseHeader=X-Response-Foo, Bar 对于上面的例子，如果下游的返回带有头信息为X-Response-Foo:1234，则会gateway会替换为X-Response-Foo:Bar，在返回给客户端。 19. SetStatus GatewayFilter FactorySetStatus GatewayFilter Factory通过配置有效的Spring HttpStatus枚举参数，可以是类似于404的这些数字，也可以是枚举的name字符串，来修改响应的返回码。 application.yml. 123456789101112spring: cloud: gateway: routes: - id: setstatusstring_route uri: http://www.google.com filters: - SetStatus=BAD_REQUEST - id: setstatusint_route uri: http://www.google.com filters: - SetStatus=401 上面例子中，两种路由都会将响应的状态码设置为401。 20. StripPrefix GatewayFilter FactoryStripPrefix GatewayFilter Factory通过配置parts来表示截断路径前缀的数量。 application.yml. 12345678910spring: cloud: gateway: routes: - id: nameRoot uri: http://nameservice predicates: - Path=/name/** filters: - StripPrefix=2 如上面例子中，如果请求的路径为/name/bar/foo，则路径会修改为/foo，即将路径的两个前缀去掉了。 21. Retry GatewayFilter FactoryRetry GatewayFilter Factory可以配置针对不同的响应做请求重试，可以配置如下参数： retries: 重试次数 statuses: 需要重试的状态码，需要根据枚举 org.springframework.http.HttpStatus来配置 methods: 需要重试的请求方法，需要根据枚举org.springframework.http.HttpMethod来配置 series: HTTP状态码系列，详情见枚举org.springframework.http.HttpStatus.Series application.yml. 12345678910111213spring: cloud: gateway: routes: - id: retry_test uri: http://localhost:8080/flakey predicates: - Host=*.retry.com filters: - name: Retry args: retries: 3 statuses: BAD_GATEWAY 上面例子，当下游服务返回502状态码时，gateway会重试3次。 22. RequestSize GatewayFilter FactoryRequestSize GatewayFilter Factory会限制客户端请求包的大小，通过参数RequestSize来配置最大上传大小，单位字节。 application.yml. 123456789101112spring: cloud: gateway: routes: - id: request_size_route uri: http://localhost:8080/upload predicates: - Path=/upload filters: - name: RequestSize args: maxSize: 5000000 如果请求大小超过5000kb限制，则会返回状态码413 Payload Too Large。 如果不设置这个filter，默认限制5M的请求大小。 23. Modify Request Body GatewayFilter Factory 官方说这个filter目前只是beta版本，API以后可能会修改。 Modify Request Body GatewayFilter Factory可以修改请求体内容，这个只能通过java来配置。 123456789101112131415161718192021222324252627@Beanpublic RouteLocator routes(RouteLocatorBuilder builder) &#123; return builder.routes() .route("rewrite_request_obj", r -&gt; r.host("*.rewriterequestobj.org") .filters(f -&gt; f.prefixPath("/httpbin") .modifyRequestBody(String.class, Hello.class, MediaType.APPLICATION_JSON_VALUE, (exchange, s) -&gt; return Mono.just(new Hello(s.toUpperCase())))).uri(uri)) .build();&#125;static class Hello &#123; String message; public Hello() &#123; &#125; public Hello(String message) &#123; this.message = message; &#125; public String getMessage() &#123; return message; &#125; public void setMessage(String message) &#123; this.message = message; &#125;&#125; 24. Modify Response Body GatewayFilter Factory 官方说这个filter目前只是beta版本，API以后可能会修改。 Modify Response Body GatewayFilter Factory用于修改响应返回的内容，同样只能通过java配置。 123456789@Beanpublic RouteLocator routes(RouteLocatorBuilder builder) &#123; return builder.routes() .route("rewrite_response_upper", r -&gt; r.host("*.rewriteresponseupper.org") .filters(f -&gt; f.prefixPath("/httpbin") .modifyResponseBody(String.class, String.class, (exchange, s) -&gt; Mono.just(s.toUpperCase()))).uri(uri) .build();&#125; 这一章接着上一章介绍了Spring Cloud Gateway官方的Gateway Filter使用场景，下一章讲Global Filters的使用。 如果想查看其他spring cloud gateway的案例和使用，可以点击查看]]></content>
      <categories>
        <category>spring cloud gateway系列教程</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>spring</tag>
        <tag>gateway</tag>
        <tag>spring cloud gateway</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[spring cloud gateway系列教程2——GatewayFilter_上篇]]></title>
    <url>%2F2019%2F01%2F01%2Fspring%20cloud%20gateway%E7%B3%BB%E5%88%97%E6%95%99%E7%A8%8B2%E2%80%94%E2%80%94GatewayFilter_%E4%B8%8A%E7%AF%87%2F</url>
    <content type="text"><![CDATA[spring cloud gateway系列教程目录 spring cloud gateway系列教程1—Route Predicate spring cloud gateway系列教程2——GatewayFilter_上篇 spring cloud gateway系列教程2——GatewayFilter_下篇 spring cloud gateway系列教程3—Global Filters spring cloud gateway系列教程4—其他配置 GatewayFilter FactoriesRoute filters可以通过一些方式修改HTTP请求的输入和输出，针对某些特殊的场景，Spring Cloud Gateway已经内置了很多不同功能的GatewayFilter Factories。 下面就来通过例子逐一讲解这些GatewayFilter Factories。 1. AddRequestHeader GatewayFilter FactoryAddRequestHeader GatewayFilter Factory通过配置name和value可以增加请求的header。application.yml： 12345678spring: cloud: gateway: routes: - id: add_request_header_route uri: http://www.google.com filters: - AddRequestHeader=X-Request-Foo, Bar 对匹配的请求，会额外添加X-Request-Foo:Bar的header。 2. AddRequestParameter GatewayFilter FactoryAddRequestParameter GatewayFilter Factory通过配置name和value可以增加请求的参数。application.yml： 12345678spring: cloud: gateway: routes: - id: add_request_parameter_route uri: http://www.google.com filters: - AddRequestParameter=foo, bar 对匹配的请求，会额外添加foo=bar的请求参数。 3. AddResponseHeader GatewayFilter FactoryAddResponseHeader GatewayFilter Factory通过配置name和value可以增加响应的header。application.yml： 12345678spring: cloud: gateway: routes: - id: add_request_header_route uri: http://www.google.com filters: - AddResponseHeader=X-Response-Foo, Bar 对匹配的请求，响应返回时会额外添加X-Response-Foo:Bar的header返回。 4. Hystrix GatewayFilter FactoryHystrix是Netflix实现的断路器模式工具包，The Hystrix GatewayFilter就是将断路器使用在gateway的路由上，目的是保护你的服务避免级联故障，以及在下游失败时可以降级返回。 项目里面引入spring-cloud-starter-netflix-hystrix依赖，并提供HystrixCommand的名字，即可生效Hystrix GatewayFilter。application.yml： 12345678spring: cloud: gateway: routes: - id: hystrix_route uri: http://www.google.com filters: - Hystrix=myCommandName 那么剩下的过滤器，就会包装在名为myCommandName的HystrixCommand中运行。 Hystrix过滤器也是通过配置可以参数fallbackUri，来支持路由熔断后的降级处理，降级后，请求会跳过fallbackUri配置的路径，目前只支持forward:的URI协议。application.yml： 12345678910111213spring: cloud: gateway: routes: - id: hystrix_route uri: lb://backing-service:8088 predicates: - Path=/consumingserviceendpoint filters: - name: Hystrix args: name: fallbackcmd fallbackUri: forward:/incaseoffailureusethis 当Hystrix降级后就会将请求转发到/incaseoffailureusethis。 整个流程其实是用fallbackUri将请求跳转到gateway内部的controller或者handler，然而也可以通过以下的方式将请求转发到外部的服务：application.yml： 1234567891011121314151617spring: cloud: gateway: routes: - id: ingredients uri: lb://ingredients predicates: - Path=//ingredients/** filters: - name: Hystrix args: name: fetchIngredients fallbackUri: forward:/fallback - id: ingredients-fallback uri: http://localhost:9994 predicates: - Path=/fallback 以上的例子，gateway降级后就会将请求转发到http://localhost:9994。 Hystrix Gateway filter在转发降级请求时，会将造成降级的异常设置在ServerWebExchangeUtils.HYSTRIX_EXECUTION_EXCEPTION_ATTR属性中，在处理降级时也可以用到。 比如下一节讲到的FallbackHeaders GatewayFilter Factory，就会通过上面的方式拿到异常信息，设置到降级转发请求的header上，来告知降级下游异常信息。 通过下面配置可以设置Hystrix的全局超时信息：application.yml： 1hystrix.command.fallbackcmd.execution.isolation.thread.timeoutInMilliseconds: 5000 5. FallbackHeaders GatewayFilter FactoryFallbackHeaders GatewayFilter Factory可以将Hystrix执行的异常信息添加到外部请求的fallbackUriheader上。application.yml： 123456789101112131415161718192021spring: cloud: gateway: routes: - id: ingredients uri: lb://ingredients predicates: - Path=//ingredients/** filters: - name: Hystrix args: name: fetchIngredients fallbackUri: forward:/fallback - id: ingredients-fallback uri: http://localhost:9994 predicates: - Path=/fallback filters: - name: FallbackHeaders args: executionExceptionTypeHeaderName: Test-Header 在这个例子中，当请求lb://ingredients降级后，FallbackHeadersfilter会将HystrixCommand的异常信息，通过Test-Header带给http://localhost:9994服务。 你也可以使用默认的header，也可以像上面一下配置修改header的名字： executionExceptionTypeHeaderName (&quot;Execution-Exception-Type&quot;) executionExceptionMessageHeaderName (&quot;Execution-Exception-Message&quot;) rootCauseExceptionTypeHeaderName (&quot;Root-Cause-Exception-Type&quot;) rootCauseExceptionMessageHeaderName (&quot;Root-Cause-Exception-Message&quot;) 6. PrefixPath GatewayFilter FactoryThe PrefixPath GatewayFilter Factor通过设置prefix参数来路径前缀。application.yml： 12345678spring: cloud: gateway: routes: - id: prefixpath_route uri: http://www.google.com filters: - PrefixPath=/mypath 如果一个请求是/hello，通过上面路由，就会将请求修改为/mypath/hello。 7. PreserveHostHeader GatewayFilter FactoryPreserveHostHeader GatewayFilter Factory会保留原始请求的host头信息，并原封不动的转发出去，而不是被gateway的http客户端重置。 application.yml: 12345678spring: cloud: gateway: routes: - id: preserve_host_route uri: http://www.google.com filters: - PreserveHostHeader 由于GatewayFilter Factory比较多，分开两篇来写，下一篇 如果想查看其他spring cloud gateway的案例和使用，可以点击查看]]></content>
      <categories>
        <category>spring cloud gateway系列教程</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>spring</tag>
        <tag>gateway</tag>
        <tag>spring cloud gateway</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[怎么写代码才舒服--合理使用异常]]></title>
    <url>%2F2018%2F12%2F28%2F%E6%80%8E%E4%B9%88%E5%86%99%E4%BB%A3%E7%A0%81%E6%89%8D%E8%88%92%E6%9C%8D-%E5%90%88%E7%90%86%E4%BD%BF%E7%94%A8%E5%BC%82%E5%B8%B8%2F</url>
    <content type="text"><![CDATA[今天讲点干货，来点思想上的碰撞，代码风格的讨论，我们来探讨一下怎么合理使用异常，来使代码流程简洁易懂，使写代码写得舒服。 异常类型先说一下java的异常类型有： 检查性异常(checked exceptions) 是必须在在方法的throws子句中声明的异常。它们扩展了异常，旨在成为一种“在你面前”的异常类型。JAVA希望你能够处理它们，因为它们以某种方式依赖于程序之外的外部因素。检查的异常表示在正常系统操作期间可能发生的预期问题。 当你尝试通过网络或文件系统使用外部系统时，通常会发生这些异常。 大多数情况下，对检查性异常的正确响应应该是稍后重试，或者提示用户修改其输入。非检查性异常(unchecked Exceptions) 是不需要在throws子句中声明的异常。 由于程序错误，JVM并不会强制你处理它们，因为它们大多数是在运行时生成的。 它们扩展了RuntimeException。 最常见的例子是NullPointerException [相当可怕..是不是？]。 未经检查的异常可能不应该重试，正确的操作通常应该是什么都不做，并让它从你的方法和执行堆栈中出来。 在高层次的执行中，应该记录这种类型的异常。错误(errors) 是严重的运行时系统问题，几乎肯定无法恢复。 例如OutOfMemoryError，LinkageError和StackOverflowError, 它们通常会让程序崩溃或程序的一部分。 只有良好的日志练习才能帮助你确定错误的确切原因。 补充完以上的概念后，我们再来探讨一下这三种异常怎么用，什么时候用，该用哪种。Error是系统异常就不说了，我们直接讲Exception和RuntimeException。 怎么用直接通过场景对比来剖析异常什么时候用比较好，该用哪种类型异常。 场景分析用户在系统中输入证件号查询用户信息，如果成功返回{&quot;code&quot;:200, &quot;result&quot;:{用户信息对象}}，验证不通过则返回{&quot;code&quot;:-100, &quot;error&quot;:&quot;系统验证错误&quot;}，那么这里的验证不通过，怎么处理才比较好。 方案一return的方式，定义通用结果对象CommonResult： 12345678910public class CommonResult &#123; private int code; private UseInfo result; private String error; //===== 忽略getset ====&#125; 如果只是单层方法里面的验证逻辑，可能代码是这样的： 12345678910111213141516public CommonResult handle(String phone) &#123; // 验证逻辑1 // 验证逻辑2 if ("123".equals(phone)) &#123; return new CommonResult(-100, null, "验证逻辑2不通过"); &#125; // 验证逻辑3 //用户信息填充 UseInfo userinfo = new UseInfo(); return new CommonResult(200, userinfo, null); &#125; 但是如果是两层嵌套方法，那么我就需要将嵌在里面的方法也返回这种通用的CommonResult对象，不然只通过return的方法无法处理里面方法的校验不通过情况，最后的结果是，所有方法都必须返回通用结果对象。 造成这种现象的本质原因是，我们将正确的结果返回和错误的结果返回混在一起了，就会造成极其混乱的结果返回逻辑。我觉得正确的、舒服的代码结构方式，应该是我只需要舒服、专注地处理正确的业务流程就好，不正确的流程都throw异常出去。(曾经听过一些老一辈的程序员觉得抛异常会影响性能，所以会采用上面那种处理方式，我觉得完全是多虑和没有必要的，先不讨论影响性能多少，以现在的机器配置，这点消耗是完全可以忽略的) 以上，我觉得就该用异常来处理了，那么第二个问题就是，我是该抛Exception还是RuntimeException呢？ 方案二抛出Exception的方案，先定义通用CommonException如下： 123456789101112public class CommonException extends Exception &#123; private int code; private String error; public CommonException(int code, String error) &#123; this.code = code; this.error = error; &#125; //==== 其他构造方法 ====&#125; 最后方法里面就变成如下： 12345678910111213141516public UserInfo handle2(String phone) throws CommonException &#123; // 验证逻辑1 // 验证逻辑2 if ("123".equals(phone)) &#123; throw new CommonException(-100, "验证逻辑2不通过"); &#125; // 验证逻辑3 //用户信息填充 UserInfo userinfo = new UserInfo(); return userinfo; &#125; 变成这种结构之后，则需要在controller或者全局过滤器里面，将结果和异常捕获，在封装成CommonResult序列化返回，整个流程暂时也是很舒服的。但是如果有多层嵌套的方法，则需要每个方法都抛出这种通用异常CommonException，但是这种异常并不是由调用方来处理的，而是最外层的controller或者全局过滤器统一处理的，这个是不合理也不舒服的地方。 造成这种现象的本质原因是，没有搞清楚Exception和RuntimeException的分别使用场景，我觉得Exception的设计考虑是，需要调用方处理这种异常情况，并且需要调用方针对不同的Exception做不同的处理，影响的是业务流转方向，不会中断业务流程。 RuntimeException的场景则是，某个校验不通过，整个流程其实都跑不下去的，需要中断整个流程。 方案三定义通用RuntimeException如下： 123456789101112public class CommonRuntimeException extends RuntimeException &#123; private int code; private String error; public CommonRuntimeException(int code, String error) &#123; this.code = code; this.error = error; &#125; //==== 其他构造方法 ====&#125; 方法逻辑如下： 12345678910111213141516public UserInfo handle3(String phone) &#123; // 验证逻辑1 // 验证逻辑2 if ("123".equals(phone)) &#123; throw new CommonRuntimeException(-100, "验证逻辑2不通过"); &#125; // 验证逻辑3 //用户信息填充 UserInfo userinfo = new UserInfo(); return userinfo; &#125; 这种的话，无论几层的方法嵌套，我都不需要显式的抛出异常，我只需要舒服地专注于写正常流程的代码即可，异常的、校验不通过的，并且流程跑不下去的情况，我只需抛CommonRuntimeException出来即可，其他就不需要管了。 补充个全局过滤器的实现： 123456789101112131415161718192021222324252627282930313233@Overridepublic void doFilter(ServletRequest request, ServletResponse response, FilterChain chain) throws IOException, ServletException &#123; HttpServletRequest hreq = (HttpServletRequest) request; HttpServletResponse hres = (HttpServletResponse) response; try &#123; before(hreq, hres); chain.doFilter(request, response); &#125; catch (Exception e) &#123; error(hreq, hres, e); &#125; finally &#123; after(hreq, hres); &#125; &#125; public void error(HttpServletRequest hreq, HttpServletResponse hres, Exception e) throws IOException &#123; if (e instanceof CommonRuntimeException) &#123; CommonRuntimeException ex = (CommonRuntimeException) e; ResponseUtil.fail(hreq, hres, ex.getCode(), ex.getError()); return; &#125; else if (e.getCause() instanceof CommonRuntimeException) &#123; //处理spring框架包了一层的情况 CommonRuntimeException ex = (CommonRuntimeException) e.getCause(); ResponseUtil.fail(hreq, hres, ex.getCode(), ex.getError()); return; &#125; &#125; //其他非通用异常的默认处理 ResponseUtil.response(hreq, hres, getCode(defErr), getError(defErr)); return; &#125; 至此，使用方案三的话，在处理业务逻辑时，我就完全不需要处理异常的流转逻辑了，专注于正常的业务流转逻辑开发。当然，如果有一些异常的场景，是影响到整个业务的流转方向的，那么自定义Exception并显示地提示调用方处理，还是很有必要的，所以并不能滥用CommonRuntimeException。 补充，每次抛CommonRuntimeException之前，必须打印上下文的日志，本文为了讲述清晰，代码例子都没添加日志。 btw，如果文章有帮助，请点赞转发。]]></content>
      <categories>
        <category>编程思想</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>exception</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[spring cloud gateway系列教程1—Route Predicate]]></title>
    <url>%2F2018%2F12%2F25%2Fspring%20cloud%20gateway%E7%B3%BB%E5%88%97%E6%95%99%E7%A8%8B1%E2%80%94%E2%80%94Route%20Predicate%2F</url>
    <content type="text"><![CDATA[spring cloud gateway系列教程目录 spring cloud gateway系列教程1—Route Predicate spring cloud gateway系列教程2——GatewayFilter_上篇 spring cloud gateway系列教程2——GatewayFilter_下篇 spring cloud gateway系列教程3—Global Filters spring cloud gateway系列教程4—其他配置 怎么引入spring cloud gatewaymaven上，只需引入如下依赖即可： 1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-gateway&lt;/artifactId&gt;&lt;/dependency&gt; 引入了依赖默认即开启gateway了，如果暂时不想使用这个功能，这可以配置spring.cloud.gateway.enabled=false即可。 Spring Cloud Gateway使用的是Spring Boot和Spring Webflux提供的Netty底层环境，不能和传统的Servlet容器一起使用，也不能打包成一个WAR包。 工作原理 当客户端发送请求到Spring Cloud Gateway，Gateway Handler Mapping会匹配Route映射分发到Gateway Web Handler。handler会将请求经过一系列的filter处理，代理请求前，会执行左右的”pre” filter逻辑，代理请求后，会执行所有”post” filter逻辑。 Route Predicate FactoriesSpring Cloud Gateway是使用Spring WebFlux的HandlerMapping作为匹配路由底层实现，本身已自带很多Route Predicate Factories，分别匹配不同的http请求属性，多个Route Predicate Factories也可以通过and进行逻辑合并匹配。 1. After Route Predicate FactoryAfter Route Predicate Factory使用的是时间作为匹配规则，只要当前时间大于设定时间，路由才会匹配请求。application.yml： 12345678spring: cloud: gateway: routes: - id: after_route uri: http://www.google.com predicates: - After=2018-12-25T14:33:47.789+08:00 这个路由规则会在东8区的2018-12-25 14:33:47后，将请求都转跳到google。 2. Before Route Predicate FactoryBefore Route Predicate Factory也是使用时间作为匹配规则，只要当前时间小于设定时间，路由才会匹配请求。application.yml： 12345678spring: cloud: gateway: routes: - id: before_route uri: http://www.google.com predicates: - Before=2018-12-25T14:33:47.789+08:00 这个路由规则会在东8区的2018-12-25 14:33:47前，将请求都转跳到google。 3. Between Route Predicate FactoryBetween Route Predicate Factory也是使用两个时间作为匹配规则，只要当前时间大于第一个设定时间，并小于第二个设定时间，路由才会匹配请求。 application.yml： 12345678spring: cloud: gateway: routes: - id: between_route uri: http://www.google.com predicates: - Between=2018-12-25T14:33:47.789+08:00, 2018-12-26T14:33:47.789+08:00 这个路由规则会在东8区的2018-12-25 14:33:47到2018-12-26 14:33:47之间，将请求都转跳到google。 4. Cookie Route Predicate FactoryCookie Route Predicate Factory使用的是cookie名字和正则表达式的value作为两个输入参数，请求的cookie需要匹配cookie名和符合其中value的正则。application.yml： 12345678spring: cloud: gateway: routes: - id: cookie_route uri: http://www.google.com predicates: - Cookie=cookiename, cookievalue 路由匹配请求存在cookie名为cookiename，cookie内容匹配cookievalue的，将请求转发到google。 5. Header Route Predicate FactoryHeader Route Predicate Factory，与Cookie Route Predicate Factory类似，也是两个参数，一个header的name，一个是正则匹配的value。application.yml： 12345678spring: cloud: gateway: routes: - id: header_route uri: http://www.google.com predicates: - Header=X-Request-Id, \d+ 路由匹配存在名为X-Request-Id，内容为数字的header的请求，将请求转发到google。 6. Host Route Predicate FactoryHost Route Predicate Factory使用的是host的列表作为参数，host使用Ant style匹配。application.yml： 12345678spring: cloud: gateway: routes: - id: host_route uri: http://www.google.com predicates: - Host=**.somehost.org,**.anotherhost.org 路由会匹配Host诸如：www.somehost.org 或 beta.somehost.org或www.anotherhost.org等请求。 7. Method Route Predicate FactoryMethod Route Predicate Factory是通过HTTP的method来匹配路由。application.yml： 12345678spring: cloud: gateway: routes: - id: method_route uri: http://www.google.com predicates: - Method=GET 路由会匹配到所有GET方法的请求。 8. Path Route Predicate FactoryPath Route Predicate Factory使用的是path列表作为参数，使用Spring的PathMatcher匹配path，可以设置可选变量。application.yml： 12345678spring: cloud: gateway: routes: - id: host_route uri: http://www.google.com predicates: - Path=/foo/&#123;segment&#125;,/bar/&#123;segment&#125; 上面路由可以匹配诸如：/foo/1 或 /foo/bar 或 /bar/baz等其中的segment变量可以通过下面方式获取： 123PathMatchInfo variables = exchange.getAttribute(URI_TEMPLATE_VARIABLES_ATTRIBUTE);Map&lt;String, String&gt; uriVariables = variables.getUriVariables();String segment = uriVariables.get("segment"); 在后续的GatewayFilter Factories就可以做对应的操作了。 9. Query Route Predicate FactoryQuery Route Predicate Factory可以通过一个或两个参数来匹配路由，一个是查询的name，一个是查询的正则value。application.yml： 12345678spring: cloud: gateway: routes: - id: query_route uri: http://www.google.com predicates: - Query=baz 路由会匹配所有包含baz查询参数的请求。application.yml： 12345678spring: cloud: gateway: routes: - id: query_route uri: http://www.google.com predicates: - Query=foo, ba. 路由会匹配所有包含foo，并且foo的内容为诸如：bar或baz等符合ba.正则规则的请求。 10. RemoteAddr Route Predicate FactoryRemoteAddr Route Predicate Factory通过无类别域间路由(IPv4 or IPv6)列表匹配路由。application.yml： 12345678spring: cloud: gateway: routes: - id: remoteaddr_route uri: http://www.google.com predicates: - RemoteAddr=192.168.1.1/24 上面路由就会匹配RemoteAddr诸如192.168.1.10等请求。 10.1 Modifying the way remote addresses are resolvedRemoteAddr Route Predicate Factory默认情况下，使用的是请求的remote address。但是如果Spring Cloud Gateway是部署在其他的代理后面的，如Nginx，则Spring Cloud Gateway获取请求的remote address是其他代理的ip，而不是真实客户端的ip。 考虑到这种情况，你可以自定义获取remote address的处理器RemoteAddressResolver。当然Spring Cloud Gateway也提供了基于X-Forwarded-For请求头的XForwardedRemoteAddressResolver。熟悉Http代理协议的，都知道X-Forwarded-For头信息做什么的，不熟悉的可以自己谷歌了解一下。 XForwardedRemoteAddressResolver提供了两个静态方法获取它的实例：XForwardedRemoteAddressResolver::trustAll得到的RemoteAddressResolver总是获取X-Forwarded-For的第一个ip地址作为remote address，这种方式就比较容易被伪装的请求欺骗，模拟请求很容易通过设置初始的X-Forwarded-For头信息，就可以欺骗到gateway。 XForwardedRemoteAddressResolver::maxTrustedIndex得到的RemoteAddressResolver则会在X-Forwarded-For信息里面，从右到左选择信任最多maxTrustedIndex个ip，因为X-Forwarded-For是越往右是越接近gateway的代理机器ip，所以是越往右的ip，信任度是越高的。那么如果前面只是挡了一层Nginx的话，如果只需要Nginx前面客户端的ip，则maxTrustedIndex取1，就可以比较安全地获取真实客户端ip。 使用java的配置：GatewayConfig.java： 123456789101112RemoteAddressResolver resolver = XForwardedRemoteAddressResolver .maxTrustedIndex(1);....route("direct-route", r -&gt; r.remoteAddr("10.1.1.1", "10.10.1.1/24") .uri("http://www.google.com").route("proxied-route", r -&gt; r.remoteAddr(resolver, "10.10.1.1", "10.10.1.1/24") .uri("http://www.google.com")) 这一章节讲的了几种Route Predicate Factory的使用及场景，下一章节讲GatewayFilter Factories的使用。 如果想查看其他spring cloud gateway的案例和使用，可以点击查看]]></content>
      <categories>
        <category>spring cloud gateway系列教程</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>spring</tag>
        <tag>gateway</tag>
        <tag>spring cloud gateway</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java如何经得起时间的考验]]></title>
    <url>%2F2018%2F12%2F16%2FJava%E5%A6%82%E4%BD%95%E7%BB%8F%E5%BE%97%E8%B5%B7%E6%97%B6%E9%97%B4%E7%9A%84%E8%80%83%E9%AA%8C%2F</url>
    <content type="text"><![CDATA[Java已经存在了二十多年，并且仍然是当今使用的顶级编程语言之一。什么能说明语言的成功，以及它如何改变以适应更现代的技术？ Java崛起的力量Java最初出现于1995年，从1991年的一项名为“Oak”的创新发展而来。对于希望发展分布式系统的工程师来说，这显然是正确的时机。当时一些比较流行的语言–C，C ++，甚至是Cobol，都是有着陡峭的学习曲线。而Java的允许多线程并发执行程序等特性，解决了多任务工作的困难问题。 Java很快成为任务关键型系统的第一人选语言。从那时起，即使不断有新的语言出现，但Java仍然根深蒂固，难以取代。事实上，正如这篇顶级编程语言文章所暗示的那样，Java自最初出现以来几乎已经成为最重要的计算语言之一 。 Sun Microsystems的James Gosling被认为是Java之父，他设计构想出了java的原型。他曾在Sun工作，直到Sun于2010年被Oracle收购。在权力的转移中，Java成为了Oracle的产品，但它并没有保持静态，持续创新使其成为最受欢迎的编程语言。此外，Java社区发布了很多用于编写，维护和调试代码的高级工具。 字节码和可移植性的吸引力Java实现通常使用两步编译过程。换句话说，源代码由Java编译器转换为字节码。然后由Java虚拟机（JVM）执行字节码。JVM今天使用一种称为即时（JIT）编译的技术来生成系统CPU可以执行的本机指令。这促进了Sun在Java早期所倡导的“一次编写，随处运行”（WORA）理念。 编译为何重要灵活性的字节码为可移植性提供了真正的好处。不用为每个平台编译应用程序，而是将相同的代码分发到每个系统，并且使用JVM管理它们。当然如果小型设备不能很好地处理所需的解释开销，就会出现问题。 此外，在jdk越来越大的情况下，对于需要快速响应的小型应用程序来说，它不需要太复杂的模块。因此，我们看到的涉及更少开销的分支，例如Avian（具有Java功能子集的轻量级虚拟机）和Excelsior JET（使用提前编译器增强的完整Java虚拟机实现）优化的本机可执行文件，牺牲了部分可移植性，对云和物联网（IoT）应用程序都受益。 转移方向Java世界中的其他创新包括GraalVM和SubstrateVM。 GraalVM是JVM的一个新的即时编译器，可以使Java与其他语言（例如，JavaScript，Python和Ruby）一起使用。GraalVM可以在OpenJDK的上下文中运行，通过新的即时编译技术使Java应用程序运行得更快。它包括一个名为Graal的新型高性能Java编译器，可以与HotSpot VM一起使用，或者与SubstrateVM一起提前设置。 SubstrateVM大大缩短了启动时间，使短期应用程序运行得更快。它是一个框架，允许将Java应用程序提前（AOT）编译为可执行映像或共享对象（ELF-64或64位Mach-O）。 开放与专有甲骨文去年宣布，JDK和OpenJDK之间的技术差异将会消失。今天两者之间的差异主要是表面的。 Java仍然是免费的。有关即将死亡的谣言几年前就出现了，但OpenJDK并没有失去任何动力。 Oracle和Red Hat扮演什么角色？Java用户最初担心的，当Oracle宣布它将不再为JDK版本提供免费二进制下载或在六个月后为OpenJDK编写bug补丁时，该怎么办。当Oracle退出时，Red Hat接管了，而且接力棒的传递似乎平稳可靠。 Java生命顽强的关键虽然甲骨文和红帽继续提供无差距的方法来支持OpenJDK，但造就Java的强大生命力的，最早的开发者和这两家公司只能占部分功劳。正如Red Hat的Mark Little所说，Java生命顽强的50％归功于开发者社区。 英文原文： https://www.networkworld.com/article/3325334/linux/how-java-has-stood-the-test-of-time.html]]></content>
      <categories>
        <category>行业资讯</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>译文</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[网关选择困难症]]></title>
    <url>%2F2018%2F12%2F16%2F%E7%BD%91%E5%85%B3%E9%80%89%E6%8B%A9%E5%9B%B0%E9%9A%BE%E7%97%87%2F</url>
    <content type="text"><![CDATA[近期因为新项目需要，我们的后端选用的技术栈是spring cloud微服务，需要对网关进行选型，所以有了一下这篇网关的性能测试分析，对你有帮助的，关注一下公众号：二当家的黑板报。 测试机器 使用的是4核4G内存的虚拟机，系统是Ubuntu18 wrk测试以下为10个线程，200并发，持续30秒，对不同网关的测试结果。（wrk使用的是http1.1版本） 直连 Nginx zuul（热身后），时不时有timeout的情况 gateway（热身后） linkerd（热身后） 测试结果说明 当没有网关，直接连接服务时，每秒可处理22433个请求，每个线程平均延时13.25ms 使用Nginx代理时，每秒可处理12507个请求，约为直连时的56%，平均延时75.43ms 使用热身后的zuul，每秒可处理6517个请求，约为直连时的29%，平均延时105.06ms；多次测试，偶尔会有几次存在3、4个timeout的请求。 使用热身后的gateway，每秒可处理9228个请求，约为直连时的41%，平均延时为21.83ms 使用热身后的linkerd，每秒可处理9999个请求，约为直连时的44%，平均延时为20.35ms ab测试，针对http1.0以下使用ab做200并发，50000个请求测试： 直连 Nginx zuul(热身后) gateway（热身后） linkerd(热身后) 测试结果说明针对网上有个帖子用ab测试，测试到gateway性能很差，官方已排查是ab用的是http1.0，而gateway的底层reactor-netty不支持http1.0，这个问题已修复。 https://cloud.tencent.com/developer/article/1083254 https://github.com/reactor/reactor-netty/issues/21 直连，50000个请求，总耗时2.748秒，每秒处理18195个请求，平均11ms一个请求 Nginx，总耗时5.021秒，每秒处理9958个请求，约为直连的55%，平均20ms一个请求 热身后的zuul，总耗时7.862秒，每秒处理6360个请求，约为直连的35%，平均31ms一个请求 热身后的gateway，总耗时6.762秒，每秒处理7394个请求，约为直连的41%，平均27ms一个请求 热身后的linkerd，总耗时6.972秒，每秒处理7171个请求，约为直连的39%，平均28ms一个请求 简单总结 根据测试情况，多种网关中Nginx的性能无疑是最好的，无论是淘宝的Tengine的功能增强，还是OpenResty的模块增强，都是对Nginx优化。不过从定位来说，我比较倾向于把它定位为传统的只是承载请求转发的工具，对于我们开发的技术栈没那么友好。另外，OpenResty有一个lua-resty-mysql模块，可做mysql的网关，我们没用过，不知道性能如何。 zuul和gateway，都是可以无缝对接spring cloud，结合服务发现，对服务层来说可以做到屏蔽机器资源（ip或内外网域名等），服务对机器无依赖性。它们的定位是融合在微服务的网关，承载着对微服务增强的功能。对于二次开发的易用性来说，两者对我们开发上手都比较容易。 zuul和gateway对比，根据测试情况，gateway的性能还是比zuul好很多的。另外，gateway的也有很多zuul没有的功能，比如支持http2、websocket，根据域名转发等。对于做服务转发，而不是数据库转发这种高性能要求，性能上和Nginx差距不大。 linkerd则是服务网格的概念了，linkerd的原理是系统代理请求，对服务是无侵入性的，有比较成熟的监控管理界面。gateway的官方测试是，gateway性能比linkerd好很多，但是在我的虚拟机上测试，两者差不多。linkerd结合docker和k8s使用，对机器资源的抽象就更上一层。linkerd更像是应用程序或者说微服务间的 TCP/IP，网络监控、限流、熔断对服务层来说是透明无感的。 以上，个人感觉是不同维度的东西，可以根据自己的功能需求、开发技术栈等，来选取对应的网关。]]></content>
      <categories>
        <category>技术测评</category>
      </categories>
      <tags>
        <tag>网关</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[你的httpclient真的使用正确吗]]></title>
    <url>%2F2018%2F12%2F15%2F%E4%BD%A0%E7%9A%84httpclient%E7%9C%9F%E7%9A%84%E4%BD%BF%E7%94%A8%E6%AD%A3%E7%A1%AE%E5%90%97%2F</url>
    <content type="text"><![CDATA[我用java开发类似scrapy的工具包时，在使用httpclient做网络请求，遇到了请求无限卡死的问题，今天将其解决方案拿出来，避免后人踩坑。 问题如下：1RequestConfig.custom().setSocketTimeout(SO_TIME_OUT).setConnectTimeout(CONNECTION_TIME_OUT).setConnectionRequestTimeout(CONNECTION_REQUEST_TIME_OUT) 在设置了常规的超时配置如socketTimeout、connectTimeout和connectionRequestTimeout，在大并发情况下，时不时会出现部分请求在java.net.SocketInputStream.socketRead0方法中一直卡死，dump出的信息如下： 1234567891011121314151617181920212223"pool-2-thread-87" #202 prio=5 os_prio=0 tid=0x00007f52603a8000 nid=0x6672 runnable [0x00007f51888c6000] java.lang.Thread.State: RUNNABLE at java.net.SocketInputStream.socketRead0(Native Method) at java.net.SocketInputStream.socketRead(SocketInputStream.java:116) at java.net.SocketInputStream.read(SocketInputStream.java:171) at java.net.SocketInputStream.read(SocketInputStream.java:141) at org.apache.http.impl.io.SessionInputBufferImpl.streamRead(SessionInputBufferImpl.java:137) at org.apache.http.impl.io.SessionInputBufferImpl.fillBuffer(SessionInputBufferImpl.java:153) at org.apache.http.impl.io.SessionInputBufferImpl.readLine(SessionInputBufferImpl.java:282) at org.apache.http.impl.conn.DefaultHttpResponseParser.parseHead(DefaultHttpResponseParser.java:138) at org.apache.http.impl.conn.DefaultHttpResponseParser.parseHead(DefaultHttpResponseParser.java:56) at org.apache.http.impl.io.AbstractMessageParser.parse(AbstractMessageParser.java:259) at org.apache.http.impl.DefaultBHttpClientConnection.receiveResponseHeader(DefaultBHttpClientConnection.java:163) at org.apache.http.protocol.HttpRequestExecutor.doReceiveResponse(HttpRequestExecutor.java:273) at org.apache.http.protocol.HttpRequestExecutor.execute(HttpRequestExecutor.java:125) at org.apache.http.impl.execchain.MainClientExec.createTunnelToTarget(MainClientExec.java:486) at org.apache.http.impl.execchain.MainClientExec.establishRoute(MainClientExec.java:411) at org.apache.http.impl.execchain.MainClientExec.execute(MainClientExec.java:237) at org.apache.http.impl.execchain.ProtocolExec.execute(ProtocolExec.java:185) at org.apache.http.impl.execchain.RetryExec.execute(RetryExec.java:89) at org.apache.http.impl.execchain.RedirectExec.execute(RedirectExec.java:110) at org.apache.http.impl.client.InternalHttpClient.doExecute(InternalHttpClient.java:185) at org.apache.http.impl.client.CloseableHttpClient.execute(CloseableHttpClient.java:83) 排查自己是否踩坑可以通过运行命令：jstack -l ${pid} | grep java.net.SocketInputStream.socketRead0如果多次dump信息都有同一个线程栈每次都出现，则可以判断你的服务也存在这种问题 解决方案 如果是https的，并且使用的是4.3.5版本的httpclient版本，升级httpclient版本即可，这个是httpclient的bug，详情见https://issues.apache.org/jira/browse/HTTPCLIENT-1589，这是第一种可能性的解决方案。 如果第一种不能解决，基本都是这种可能性了。我在使用httpclient的代理请求，经过无数次的debug，发现代理请求时在TUNNEL_TARGET步骤，里面的连接用的DefaultBHttpClientConnection绑定的socket，使用的是SocketConfig配置，而不是RequestConfig，所以在没有设置SocketConfig情况下，socket的ocketRead0方法是无限等待的，就会造成线程一直卡死。增加下面的配置即可： 1connectionManager.setSocketConfig(SocketConfig.custom().setSoTimeout(SO_TIME_OUT).build()); 另外，凡是用到socket的，可能都需要注意是否设置了socket的timeout，不然就会出现一直socketRead0的情况。]]></content>
      <categories>
        <category>问题总结</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>httpclient</tag>
      </tags>
  </entry>
</search>
